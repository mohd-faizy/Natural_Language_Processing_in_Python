{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Building tf-idf Document Vectors</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Feature Engineering for NLP in Python)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "1. [n-gram Modeling and Motivation](#section-1)\n",
        "2. [Term Frequency-Inverse Document Frequency (TF-IDF)](#section-2)\n",
        "3. [TF-IDF Implementation with Scikit-Learn](#section-3)\n",
        "4. [Cosine Similarity: Theory and Math](#section-4)\n",
        "5. [Cosine Similarity Implementation](#section-5)\n",
        "6. [Project: Building a Plot Line Based Recommender](#section-6)\n",
        "7. [Beyond n-grams: Word Embeddings](#section-7)\n",
        "8. [Word and Document Similarities with spaCy](#section-8)\n",
        "9. [Review and Conclusion](#section-9)\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. n-gram Modeling and Motivation</span><br>\n",
        "\n",
        "### Understanding n-gram Modeling\n",
        "In n-gram modeling, the weight of a dimension is dependent on the frequency of the word corresponding to that dimension.\n",
        "\n",
        "*   **Example**: If a document contains the word `human` in five places.\n",
        "*   **Result**: The dimension corresponding to `human` has a weight of **5**.\n",
        "\n",
        "### The Motivation for Better Vectors\n",
        "While simple frequency counts are useful, they have significant drawbacks when analyzing a corpus of documents.\n",
        "\n",
        "**The Problem:**\n",
        "*   Some words occur very commonly across **all** documents.\n",
        "*   Consider a corpus of documents about the universe.\n",
        "    *   One document has `jupiter` and `universe` occurring 20 times each.\n",
        "    *   `jupiter` rarely occurs in the other documents, but `universe` is common across the whole corpus.\n",
        "    *   Simple frequency weighting treats them equally (both weight 20).\n",
        "    *   **Goal**: We want to give more weight to `jupiter` on account of its **exclusivity** to that specific document.\n",
        "\n",
        "### Applications\n",
        "Advanced vectorization techniques like TF-IDF allow us to:\n",
        "1.  Automatically detect stopwords.\n",
        "2.  Improve Search algorithms.\n",
        "3.  Build Recommender systems.\n",
        "4.  Achieve better performance in predictive modeling.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. Term Frequency-Inverse Document Frequency (TF-IDF)</span><br>\n",
        "\n",
        "TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
        "\n",
        "### Key Concepts\n",
        "*   **Term Frequency (TF)**: Proportional to the frequency of the word in the specific document.\n",
        "*   **Inverse Document Frequency (IDF)**: Inverse function of the number of documents in which the word occurs.\n",
        "\n",
        "### Mathematical Formula\n",
        "The weight $w_{i,j}$ of term $i$ in document $j$ is calculated as:\n",
        "\n",
        "$$ w_{i,j} = tf_{i,j} \\cdot \\log \\left( \\frac{N}{df_i} \\right) $$\n",
        "\n",
        "Where:\n",
        "*   $w_{i,j}$: Weight of term $i$ in document $j$.\n",
        "*   $tf_{i,j}$: Term frequency of term $i$ in document $j$.\n",
        "*   $N$: Total number of documents in the corpus.\n",
        "*   $df_i$: Number of documents containing term $i$.\n",
        "\n",
        "### Calculation Example\n",
        "Imagine a corpus where:\n",
        "*   Total documents ($N$) = 20\n",
        "*   The word \"library\" appears in 8 documents ($df_{library} = 8$).\n",
        "*   In a specific document, \"library\" appears 5 times ($tf = 5$).\n",
        "\n",
        "$$ w_{library, document} = 5 \\cdot \\log \\left( \\frac{20}{8} \\right) \\approx 2 $$\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. TF-IDF Implementation with Scikit-Learn</span><br>\n",
        "\n",
        "We can implement TF-IDF easily using `TfidfVectorizer` from scikit-learn.\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> TfidfVectorizer converts a collection of raw documents to a matrix of TF-IDF features. It handles tokenization and weighting automatically. </div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample Corpus (Created for demonstration purposes)\n",
        "corpus = [\n",
        "    \"The sun is hot\",\n",
        "    \"The sun is bright\",\n",
        "    \"The moon is cold\",\n",
        "    \"The stars are far\"\n",
        "]\n",
        "\n",
        "# Create TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Print the matrix as an array\n",
        "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
        "print(\"\\nTF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Explanation:**\n",
        "1.  We initialize the `TfidfVectorizer`.\n",
        "2.  `fit_transform(corpus)` learns the vocabulary and inverse document frequencies, then returns the document-term matrix.\n",
        "3.  The output is a sparse matrix where non-zero values represent the TF-IDF weight of a word in a document.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Cosine Similarity: Theory and Math</span><br>\n",
        "\n",
        "To build recommender systems or search engines, we need a way to calculate how similar two documents (vectors) are. The **Cosine Similarity** is a standard metric used in NLP.\n",
        "\n",
        "### The Dot Product\n",
        "Consider two vectors $V = (v_1, v_2, ..., v_n)$ and $W = (w_1, w_2, ..., w_n)$.\n",
        "The dot product is:\n",
        "$$ V \\cdot W = (v_1 \\times w_1) + (v_2 \\times w_2) + ... + (v_n \\times w_n) $$\n",
        "\n",
        "**Example:**\n",
        "$$ A = (4, 7, 1) $$\n",
        "$$ B = (5, 2, 3) $$\n",
        "$$ A \\cdot B = (4 \\times 5) + (7 \\times 2) + (1 \\times 3) $$\n",
        "$$ A \\cdot B = 20 + 14 + 3 = 37 $$\n",
        "\n",
        "### Magnitude of a Vector\n",
        "The magnitude (length) of a vector is defined as:\n",
        "$$ ||V|| = \\sqrt{(v_1)^2 + (v_2)^2 + ... + (v_n)^2} $$\n",
        "\n",
        "**Example for Vector A:**\n",
        "$$ ||A|| = \\sqrt{(4)^2 + (7)^2 + (1)^2} $$\n",
        "$$ ||A|| = \\sqrt{16 + 49 + 1} = \\sqrt{66} \\approx 8.12 $$\n",
        "\n",
        "### The Cosine Score Formula\n",
        "The cosine similarity is the cosine of the angle $\\theta$ between two vectors.\n",
        "\n",
        "$$ \\text{sim}(A, B) = \\cos(\\theta) = \\frac{A \\cdot B}{||A|| \\cdot ||B||} $$\n",
        "\n",
        "**Calculating the Score for A and B:**\n",
        "$$ \\cos(A, B) = \\frac{37}{\\sqrt{66} \\times \\sqrt{38}} $$\n",
        "$$ \\cos(A, B) \\approx \\frac{37}{8.12 \\times 6.16} \\approx 0.7388 $$\n",
        "\n",
        "### Points to Remember\n",
        "*   Mathematically, the value is between -1 and 1.\n",
        "*   In NLP (where term counts are non-negative), the value is between **0 and 1**.\n",
        "*   It is **robust to document length** (unlike Euclidean distance).\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Cosine Similarity Implementation</span><br>\n",
        "\n",
        "Scikit-learn provides a utility to calculate this efficiently.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the cosine_similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Define two 3-dimensional vectors A and B\n",
        "A = [4, 7, 1]\n",
        "B = [5, 2, 3]\n",
        "\n",
        "# Compute the cosine score of A and B\n",
        "# Note: cosine_similarity expects 2D arrays (lists of vectors)\n",
        "score = cosine_similarity([A], [B])\n",
        "\n",
        "# Print the cosine score\n",
        "print(score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Output Interpretation**:\n",
        "The output `[[0.73881883]]` matches our manual calculation.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. Project: Building a Plot Line Based Recommender</span><br>\n",
        "\n",
        "We will build a system that recommends movies based on the similarity of their plot descriptions.\n",
        "\n",
        "### The Data\n",
        "We will use a small subset of movie data containing titles and overviews.\n",
        "\n",
        "| Title | Overview |\n",
        "| :--- | :--- |\n",
        "| **Shanghai Triad** | A provincial boy related to a Shanghai crime family is recruited by his uncle into cosmopolitan Shanghai in the 1930s to be a servant to a ganglord's mistress. |\n",
        "| **Cry, the Beloved Country** | A South-African preacher goes to search for his wayward son who has committed a crime in the big city. |\n",
        "| **The Godfather** | Spanning the years 1945 to 1955, a chronicle of the fictional Italian-American Corleone crime family. |\n",
        "\n",
        "### Step 1: Data Setup and Preprocessing\n",
        "First, let's create a DataFrame to simulate the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating a dummy dataset for the recommender\n",
        "data = {\n",
        "    'title': [\n",
        "        'The Godfather', \n",
        "        'The Godfather: Part II', \n",
        "        'The Godfather: Part III', \n",
        "        'Shanghai Triad', \n",
        "        'Cry, the Beloved Country', \n",
        "        'Goodfellas',\n",
        "        'The Lion King',\n",
        "        'The Lion King 2: Simba\\'s Pride'\n",
        "    ],\n",
        "    'overview': [\n",
        "        'Spanning the years 1945 to 1955, a chronicle of the fictional Italian-American Corleone crime family.',\n",
        "        'The early life and career of Vito Corleone in 1920s New York City is portrayed, while his son, Michael, expands and tightens his grip on the family crime syndicate.',\n",
        "        'In the midst of trying to legitimize his business dealings in New York City and Italy in 1979, aging Mafia Don Michael Corleone seeks to avow for his sins.',\n",
        "        'A provincial boy related to a Shanghai crime family is recruited by his uncle into cosmopolitan Shanghai in the 1930s.',\n",
        "        'A South-African preacher goes to search for his wayward son who has committed a crime in the big city.',\n",
        "        'The story of Henry Hill and his life in the mob, covering his relationship with his wife Karen Hill and his mob partners Jimmy Conway and Tommy DeVito.',\n",
        "        'Lion prince Simba and his father are targeted by his bitter uncle, who wants to ascend the throne himself.',\n",
        "        'Simba\\'s daughter is the key to a resolution of a bitter feud between Simba\\'s pride and the outcast pride led by the mate of Scar.'\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Step 2: Generating TF-IDF Vectors\n",
        "We convert the text overviews into numerical vectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create TfidfVectorizer object\n",
        "# We can use built-in stop words to remove common English words\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Generate matrix of tf-idf vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(df['overview'])\n",
        "\n",
        "print(\"Matrix Shape:\", tfidf_matrix.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Step 3: Generating Cosine Similarity Matrix\n",
        "We calculate the similarity of every movie against every other movie.\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> The magnitude of a TF-IDF vector is 1. Therefore, the dot product of two TF-IDF vectors is equal to their cosine similarity. We can use <code>linear_kernel</code> instead of <code>cosine_similarity</code> to significantly improve computation time. </div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# Generate cosine similarity matrix\n",
        "# linear_kernel calculates the dot product\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "print(cosine_sim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Step 4: The Recommender Function\n",
        "We need a function that:\n",
        "1.  Takes a movie title.\n",
        "2.  Finds the index of that movie.\n",
        "3.  Extracts pairwise similarity scores.\n",
        "4.  Sorts scores in descending order.\n",
        "5.  Returns the top similar titles (ignoring the movie itself).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construct a reverse map of indices and movie titles\n",
        "indices = pd.Series(df.index, index=df['title']).drop_duplicates()\n",
        "\n",
        "def get_recommendations(title, cosine_sim=cosine_sim):\n",
        "    # Get the index of the movie that matches the title\n",
        "    idx = indices[title]\n",
        "\n",
        "    # Get the pairwsie similarity scores of all movies with that movie\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "\n",
        "    # Sort the movies based on the similarity scores\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the scores of the 3 most similar movies (ignoring index 0 which is the movie itself)\n",
        "    sim_scores = sim_scores[1:4]\n",
        "\n",
        "    # Get the movie indices\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    # Return the top 3 most similar movies\n",
        "    return df['title'].iloc[movie_indices]\n",
        "\n",
        "# Test the recommender\n",
        "print(\"Recommendations for 'The Godfather':\")\n",
        "print(get_recommendations('The Godfather'))\n",
        "\n",
        "print(\"\\nRecommendations for 'The Lion King':\")\n",
        "print(get_recommendations('The Lion King'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 7. Beyond n-grams: Word Embeddings</span><br>\n",
        "\n",
        "### The Problem with BoW and TF-IDF\n",
        "Consider these sentences:\n",
        "1.  'I am happy'\n",
        "2.  'I am joyous'\n",
        "3.  'I am sad'\n",
        "\n",
        "In Bag-of-Words or TF-IDF, \"happy\" and \"joyous\" are just different strings. The model does not know they are synonyms.\n",
        "\n",
        "### Word Embeddings\n",
        "Word embeddings map words into an n-dimensional vector space.\n",
        "*   Produced using deep learning on huge amounts of data.\n",
        "*   Can discern how similar two words are.\n",
        "*   Used to detect synonyms and antonyms.\n",
        "*   **Captures complex relationships**:\n",
        "    *   King - Queen $\\rightarrow$ Man - Woman\n",
        "    *   France - Paris $\\rightarrow$ Russia - Moscow\n",
        "\n",
        "### Implementation using spaCy\n",
        "We use the `spaCy` library. (Note: You must download the model via `python -m spacy download en_core_web_lg`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load model and create Doc object\n",
        "# Note: 'en_core_web_lg' is required for vectors. 'sm' models do not contain vectors.\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_lg')\n",
        "except OSError:\n",
        "    print(\"Downloading model...\")\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_lg\")\n",
        "    nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "doc = nlp('I am happy')\n",
        "\n",
        "# Generate word vectors for each token\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, Vector Size: {token.vector.shape}\")\n",
        "    # Printing just the first 5 dimensions for brevity\n",
        "    print(token.vector[:5]) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 8. Word and Document Similarities with spaCy</span><br>\n",
        "\n",
        "### Word Similarities\n",
        "We can compare individual tokens to see how semantically close they are.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc = nlp(\"happy joyous sad\")\n",
        "\n",
        "# Iterate over tokens to compare them\n",
        "for token1 in doc:\n",
        "    for token2 in doc:\n",
        "        print(f\"{token1.text} - {token2.text}: {token1.similarity(token2):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Expected Behavior**:\n",
        "*   `happy` and `joyous` should have a high similarity score.\n",
        "*   `happy` and `sad` will have a lower score (though still related as they are both emotions).\n",
        "\n",
        "### Document Similarities\n",
        "We can also compare entire documents (sentences). spaCy averages the vectors of the words in the document.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate doc objects\n",
        "sent1 = nlp(\"I am happy\")\n",
        "sent2 = nlp(\"I am sad\")\n",
        "sent3 = nlp(\"I am joyous\")\n",
        "\n",
        "# Compute similarity between sent1 and sent2\n",
        "score_1_2 = sent1.similarity(sent2)\n",
        "print(f\"Similarity ('I am happy', 'I am sad'): {score_1_2}\")\n",
        "\n",
        "# Compute similarity between sent1 and sent3\n",
        "score_1_3 = sent1.similarity(sent3)\n",
        "print(f\"Similarity ('I am happy', 'I am joyous'): {score_1_3}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 9. Review and Conclusion</span><br>\n",
        "\n",
        "### Summary of Feature Engineering for NLP\n",
        "In this notebook, we have covered the essential techniques for converting text into numerical data that machine learning models can understand.\n",
        "\n",
        "1.  **Basic Features**: Characters, words, mentions.\n",
        "2.  **Preprocessing**: Tokenization, Lemmatization, Text cleaning.\n",
        "3.  **n-gram Modeling**: Capturing context by looking at adjacent words.\n",
        "4.  **TF-IDF**: Weighting words by their importance (exclusivity) to a document.\n",
        "5.  **Cosine Similarity**: Measuring the angle between document vectors to find similarities.\n",
        "6.  **Word Embeddings**: Using deep learning (spaCy) to capture semantic meaning and relationships.\n",
        "\n",
        "### Next Steps\n",
        "To further advance your NLP skills, consider exploring:\n",
        "*   **Advanced NLP with spaCy**: Custom pipelines and entity recognition.\n",
        "*   **Deep Learning**: Using libraries like TensorFlow or PyTorch for Transformers (BERT, GPT).\n",
        "\n",
        "**Congratulations on building your own TF-IDF vectorizer and Movie Recommender System!**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}