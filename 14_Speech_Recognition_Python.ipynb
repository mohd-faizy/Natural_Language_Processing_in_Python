{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "efe07828",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Speech Recognition Python Library</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(SPOKEN LANGUAGE PROCESSING IN PYTHON)</span></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Introduction & Why SpeechRecognition?](#section-1)\n",
        "2. [Getting Started: Installation](#section-2)\n",
        "3. [The Recognizer Class](#section-3)\n",
        "4. [Recognizing Speech (APIs)](#section-4)\n",
        "5. [Basic Transcription Example](#section-5)\n",
        "6. [Reading Audio Files](#section-6)\n",
        "7. [From AudioFile to AudioData](#section-7)\n",
        "8. [Transcribing AudioData](#section-8)\n",
        "9. [Duration and Offset](#section-9)\n",
        "10. [Handling Different Languages](#section-10)\n",
        "11. [Non-Speech Audio & Errors](#section-11)\n",
        "12. [Showing All Alternatives](#section-12)\n",
        "13. [Handling Multiple Speakers](#section-13)\n",
        "14. [Handling Noisy Audio](#section-14)\n",
        "15. [Conclusion](#section-15)\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. Introduction & Why SpeechRecognition?</span><br>\n",
        "\n",
        "Spoken language processing is a critical component of modern AI applications. While there are several libraries available for Python, the **SpeechRecognition** library stands out for its ease of use and compatibility.\n",
        "\n",
        "### Existing Python Libraries for Speech\n",
        "There are various tools available in the ecosystem. Below is a comparison of common libraries mentioned in the course material:\n",
        "\n",
        "| Library | Developer/Origin | Note |\n",
        "| :--- | :--- | :--- |\n",
        "| **CMU Sphinx** | Carnegie Mellon University | Offline, older toolkit. |\n",
        "| **Kaldi** | Kaldi Project | Powerful, research-focused. |\n",
        "| **SpeechRecognition** | Anthony Zhang (Uberi) | **Focus of this notebook.** Wrapper for many engines. |\n",
        "| **Wav2letter++** | Facebook AI Research | Fast, C++ based. |\n",
        "\n",
        "We choose `SpeechRecognition` because it provides a simple, unified interface to access multiple speech recognition APIs (both online and offline).\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. Getting Started: Installation</span><br>\n",
        "\n",
        "To begin using the library, it must be installed from PyPi. The library is compatible with both Python 2 and Python 3, though Python 3 is the standard for modern development.\n",
        "\n",
        "### Installation Command\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the library using pip (run this in your terminal or a Jupyter cell with !)\n",
        "# !pip install SpeechRecognition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> Ensure you have a working internet connection if you plan to use web-based APIs like Google Speech Recognition. </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. The Recognizer Class</span><br>\n",
        "\n",
        "The core component of the library is the `Recognizer` class. This class contains all the necessary functions to process audio and send it to recognition services.\n",
        "\n",
        "### Initializing the Recognizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the SpeechRecognition library\n",
        "import speech_recognition as sr\n",
        "\n",
        "# Create an instance of the Recognizer class\n",
        "recognizer = sr.Recognizer()\n",
        "\n",
        "# Set the energy threshold\n",
        "# This value determines how loud audio must be to be considered speech.\n",
        "# The default is usually 300, but it can be adjusted for sensitivity.\n",
        "recognizer.energy_threshold = 300\n",
        "\n",
        "print(f\"Recognizer created with energy threshold: {recognizer.energy_threshold}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Recognizing Speech (APIs)</span><br>\n",
        "\n",
        "The `Recognizer` class has built-in functions that interact with various external Speech APIs. This allows you to switch backends easily without rewriting your audio processing code.\n",
        "\n",
        "### Available Recognition Methods\n",
        "\n",
        "| Method | Service | Note |\n",
        "| :--- | :--- | :--- |\n",
        "| `recognize_bing()` | Microsoft Bing Speech | Requires API Key |\n",
        "| `recognize_google()` | Google Web Speech API | **Default / Free (limited)** |\n",
        "| `recognize_google_cloud()` | Google Cloud Speech | Requires Credentials |\n",
        "| `recognize_wit()` | Wit.ai | Requires API Key |\n",
        "\n",
        "**Input**: An `audio_file` (specifically, an `AudioData` instance).\n",
        "**Output**: Transcribed text (string).\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Basic Transcription Example</span><br>\n",
        "\n",
        "In this section, we focus on `recognize_google()`, which is convenient for testing because it doesn't always require an API key for low-volume usage.\n",
        "\n",
        "### Code Example\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "# Instantiate Recognizer class\n",
        "recognizer = sr.Recognizer()\n",
        "\n",
        "# NOTE: In a real scenario, 'audio_file' would be an AudioData object \n",
        "# obtained from recording a file (see next sections).\n",
        "# For this example, we assume the data is ready.\n",
        "\n",
        "# Transcribe speech using Google Web API\n",
        "# recognizer.recognize_google(audio_data=audio_file, language=\"en-US\")\n",
        "\n",
        "# Expected Output Example:\n",
        "# \"Learning speech recognition on DataCamp is awesome!\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. Reading Audio Files</span><br>\n",
        "\n",
        "To process pre-recorded audio, we use the `AudioFile` class. This class acts as a file handle for `.wav`, `.aiff`, or `.flac` files.\n",
        "\n",
        "### The AudioFile Class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "# Setup recognizer instance\n",
        "recognizer = sr.Recognizer()\n",
        "\n",
        "# Define the path to the audio file\n",
        "# Note: Ensure \"clean-support-call.wav\" exists in your directory\n",
        "audio_filename = \"clean-support-call.wav\"\n",
        "\n",
        "# Read in audio file using the AudioFile class\n",
        "# This creates a file object, but does not yet load the data into memory\n",
        "clean_support_call = sr.AudioFile(audio_filename)\n",
        "\n",
        "# Check type of clean_support_call\n",
        "print(type(clean_support_call))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Output:**\n",
        "`<class 'speech_recognition.AudioFile'>`\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 7. From AudioFile to AudioData</span><br>\n",
        "\n",
        "You cannot pass the `AudioFile` object directly to the recognizer. You must first \"record\" the data from the file into an `AudioData` object.\n",
        "\n",
        "### The `record()` Method\n",
        "\n",
        "If you try to pass the file directly, you will get an error:\n",
        "> `AssertionError: audio_data must be audio data`\n",
        "\n",
        "Here is the correct way to convert the file to data:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert from AudioFile to AudioData using a context manager\n",
        "with clean_support_call as source:\n",
        "    # Record the audio from the source file\n",
        "    clean_support_call_audio = recognizer.record(source)\n",
        "\n",
        "# Check the type of the resulting object\n",
        "print(type(clean_support_call_audio))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Output:**\n",
        "`<class 'speech_recognition.AudioData'>`\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 8. Transcribing AudioData</span><br>\n",
        "\n",
        "Now that we have the `AudioData` object, we can pass it to the Google API for transcription.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transcribe clean support call\n",
        "text = recognizer.recognize_google(audio_data=clean_support_call_audio)\n",
        "\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Output:**\n",
        "`hello I'd like to get some help setting up my account please`\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 9. Duration and Offset</span><br>\n",
        "\n",
        "The `record()` function allows you to read specific parts of an audio file using `duration` and `offset` parameters. By default, both are set to `None`, meaning the whole file is read.\n",
        "\n",
        "### Using Duration and Offset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Leave duration and offset as default (reads whole file)\n",
        "with clean_support_call as source:\n",
        "    clean_support_call_audio = recognizer.record(source, duration=None, offset=None)\n",
        "\n",
        "# Example 2: Get first 2 seconds of clean support call\n",
        "with clean_support_call as source:\n",
        "    # Record only the first 2.0 seconds\n",
        "    clean_support_call_audio_short = recognizer.record(source, duration=2.0)\n",
        "\n",
        "# Transcribe the short segment\n",
        "text_short = recognizer.recognize_google(clean_support_call_audio_short)\n",
        "print(text_short)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Output:**\n",
        "`hello I'd like to get`\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 10. Handling Different Languages</span><br>\n",
        "\n",
        "The `recognize_google()` function accepts a `language` parameter. If you pass the wrong language code, the API will attempt to transcribe the sounds into the default language (English), often resulting in phonetic nonsense.\n",
        "\n",
        "### Example: Japanese Audio\n",
        "\n",
        "**Scenario 1: Wrong Language (English)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a recognizer class\n",
        "recognizer = sr.Recognizer()\n",
        "\n",
        "# Assume 'japanese_good_morning' is an AudioData object containing Japanese speech\n",
        "# Pass the Japanese audio to recognize_google with English setting\n",
        "# text = recognizer.recognize_google(japanese_good_morning, language=\"en-US\")\n",
        "\n",
        "# Print the text\n",
        "# print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Output (Phonetic English):**\n",
        "`Ohio gozaimasu` (Sounds like \"Ohayo Gozaimasu\" but transcribed as English words).\n",
        "\n",
        "**Scenario 2: Correct Language (Japanese)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pass the Japanese audio to recognize_google with Japanese setting (\"ja\")\n",
        "# text = recognizer.recognize_google(japanese_good_morning, language=\"ja\")\n",
        "\n",
        "# Print the text\n",
        "# print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Output:**\n",
        "`?????????` (Note: The slide displays question marks, likely indicating character encoding issues in the display environment or font support, but the API returns Japanese characters).\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 11. Non-Speech Audio & Errors</span><br>\n",
        "\n",
        "Speech recognition APIs are designed for human speech. If you feed them non-speech audio (like animal sounds), they may fail to produce a transcription.\n",
        "\n",
        "### Example: Leopard Roar\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the leopard roar audio file\n",
        "leopard_roar = sr.AudioFile(\"leopard_roar.wav\")\n",
        "\n",
        "# Convert the AudioFile to AudioData\n",
        "with leopard_roar as source:\n",
        "    leopard_roar_audio = recognizer.record(source)\n",
        "\n",
        "# Attempt to recognize the AudioData\n",
        "try:\n",
        "    recognizer.recognize_google(leopard_roar_audio)\n",
        "except sr.UnknownValueError:\n",
        "    print(\"UnknownValueError: The API could not understand the audio.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> The <code>UnknownValueError</code> is raised when the speech recognition engine cannot match the audio to any known words. </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 12. Showing All Alternatives</span><br>\n",
        "\n",
        "Sometimes the API is unsure about the transcription. You can use `show_all=True` to see the raw response from the API, which may include alternative transcriptions and confidence scores.\n",
        "\n",
        "### Using `show_all=True`\n",
        "\n",
        "**Example 1: Non-speech (Leopard)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recognize the AudioData with show_all turned on\n",
        "result = recognizer.recognize_google(leopard_roar_audio, show_all=True)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output:** `[]` (Empty list, meaning no speech detected).\n",
        "\n",
        "**Example 2: Ambiguous Speech (Japanese)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recognizing Japanese audio with show_all=True\n",
        "# text = recognizer.recognize_google(japanese_good_morning, language=\"en-US\", show_all=True)\n",
        "# print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Output (Dictionary with alternatives):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{\n",
        "    'alternative': [\n",
        "        {'transcript': 'Ohio gozaimasu', 'confidence': 0.89041114},\n",
        "        {'transcript': 'all hail gozaimasu'},\n",
        "        {'transcript': 'ohayo gozaimasu'},\n",
        "        {'transcript': 'olho gozaimasu'},\n",
        "        {'transcript': 'all Hale gozaimasu'}\n",
        "    ],\n",
        "    'final': True\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 13. Handling Multiple Speakers</span><br>\n",
        "\n",
        "A major limitation of the standard `SpeechRecognition` library is that it does not perform **diarization** (distinguishing between different speakers). It returns all text as a single block.\n",
        "\n",
        "### The Limitation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import an audio file with multiple speakers\n",
        "multiple_speakers = sr.AudioFile(\"multiple-speakers.wav\")\n",
        "\n",
        "# Convert AudioFile to AudioData\n",
        "with multiple_speakers as source:\n",
        "    multiple_speakers_audio = recognizer.record(source)\n",
        "\n",
        "# Recognize the AudioData\n",
        "text = recognizer.recognize_google(multiple_speakers_audio)\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Output:**\n",
        "`one of the limitations of the speech recognition library is that it doesn't recognise different speakers and voices it will just return it all as one block of text`\n",
        "\n",
        "### The Workaround: Split Files\n",
        "To handle multiple speakers, you often need to pre-process the audio into separate files (e.g., `s0.wav`, `s1.wav`, `s2.wav`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import audio files separately\n",
        "speakers = [sr.AudioFile(\"s0.wav\"), sr.AudioFile(\"s1.wav\"), sr.AudioFile(\"s2.wav\")]\n",
        "\n",
        "# Transcribe each speaker individually\n",
        "for i, speaker in enumerate(speakers):\n",
        "    with speaker as source:\n",
        "        speaker_audio = recognizer.record(source)\n",
        "        print(f\"Text from speaker {i}: {recognizer.recognize_google(speaker_audio)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Output:**\n",
        "*   Text from speaker 0: one of the limitations of the speech recognition library\n",
        "*   Text from speaker 1: is that it doesn't recognise different speakers and voices\n",
        "*   Text from speaker 2: it will just return it all as one block a text\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 14. Handling Noisy Audio</span><br>\n",
        "\n",
        "Background noise can severely impact transcription accuracy. The `Recognizer` class includes a utility to adjust for ambient noise levels.\n",
        "\n",
        "### Using `adjust_for_ambient_noise`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import audio file with background noise\n",
        "noisy_support_call = sr.AudioFile(\"noisy_support_call.wav\")\n",
        "\n",
        "with noisy_support_call as source:\n",
        "    # Adjust for ambient noise and record\n",
        "    # The recognizer listens for 0.5 seconds to calibrate the energy threshold\n",
        "    recognizer.adjust_for_ambient_noise(source, duration=0.5)\n",
        "    \n",
        "    # Now record the actual audio\n",
        "    noisy_support_call_audio = recognizer.record(source)\n",
        "\n",
        "# Recognize the audio\n",
        "text = recognizer.recognize_google(noisy_support_call_audio)\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Output:**\n",
        "`hello ID like to get some help setting up my calories`\n",
        "\n",
        "*(Note: Even with adjustment, noise can lead to errors. Here \"account please\" might have been misheard as \"calories\" or similar due to noise).*\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 15. Conclusion</span><br>\n",
        "\n",
        "In this notebook, we explored the capabilities of the **SpeechRecognition** library in Python.\n",
        "\n",
        "**Key Takeaways:**\n",
        "1.  **Recognizer Class**: The central hub for managing audio and APIs.\n",
        "2.  **AudioFile & AudioData**: You must convert file handles (`AudioFile`) into raw data (`AudioData`) using `record()` before transcription.\n",
        "3.  **Flexibility**: The library supports multiple APIs (Google, Bing, Wit) and languages.\n",
        "4.  **Limitations**: It does not natively handle speaker diarization (multiple speakers) and requires clean audio or noise adjustment for best results.\n",
        "5.  **Debugging**: Use `show_all=True` to inspect raw API responses and confidence scores.\n",
        "\n",
        "**Next Steps:**\n",
        "*   Try recording your own voice using a microphone (using `sr.Microphone()` instead of `sr.AudioFile()`).\n",
        "*   Experiment with different `energy_threshold` settings for noisy environments.\n",
        "*   Integrate this into a larger NLP pipeline to analyze the sentiment of the transcribed text.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
