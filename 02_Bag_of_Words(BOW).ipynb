{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "53fda7b5",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Word counts with bag-of-words</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Introduction to Natural Language Processing in Python)</span></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Bag-of-words Concept](#section-1)\n",
        "2. [Bag-of-words in Python](#section-2)\n",
        "3. [Simple Text Preprocessing](#section-3)\n",
        "4. [Text Preprocessing with Python](#section-4)\n",
        "5. [Introduction to Gensim](#section-5)\n",
        "6. [Creating a Gensim Corpus](#section-6)\n",
        "7. [Tf-idf with Gensim](#section-7)\n",
        "8. [Conclusion](#section-8)\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. Bag-of-words Concept</span><br>\n",
        "\n",
        "### What is Bag-of-words?\n",
        "The **Bag-of-words (BoW)** model is a fundamental method in Natural Language Processing (NLP) for finding topics within a text. It simplifies text data by disregarding grammar and word order, focusing entirely on **word frequency**.\n",
        "\n",
        "**Key Principles:**\n",
        "*   **Tokenization:** You must first break the text into individual units called tokens (words).\n",
        "*   **Counting:** You count the occurrence of every token.\n",
        "*   **Frequency = Importance:** The underlying assumption is that the more frequent a word is, the more significant it is to the meaning of the text.\n",
        "\n",
        "### Example\n",
        "Consider the following text:\n",
        "> \"The cat is in the box. The cat likes the box. The box is over the cat.\"\n",
        "\n",
        "If we strip punctuation and count the words, we get the following distribution:\n",
        "\n",
        "| Word | Count |\n",
        "| :--- | :--- |\n",
        "| \"The\" / \"the\" | 3 |\n",
        "| \"box\" | 3 |\n",
        "| \"cat\" | 3 |\n",
        "| \"is\" | 2 |\n",
        "| \"in\" | 1 |\n",
        "| \"likes\" | 1 |\n",
        "| \"over\" | 1 |\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> In a raw Bag-of-words model, the sentence structure is lost. \"The cat eats the fish\" and \"The fish eats the cat\" would look identical mathematically if they have the same word counts. </div>\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. Bag-of-words in Python</span><br>\n",
        "\n",
        "To implement this in Python, we typically use the `nltk` library for tokenization and the `collections` library to handle the counting.\n",
        "\n",
        "### Original Code (From Document)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'The': 3,\n",
              "         'cat': 3,\n",
              "         'the': 3,\n",
              "         'box': 3,\n",
              "         '.': 3,\n",
              "         'is': 2,\n",
              "         'in': 1,\n",
              "         'likes': 1,\n",
              "         'over': 1})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Note: The input string contains newlines in the original document\n",
        "Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box. \n",
        "The box is over the cat.\"\"\"))\n",
        "\n",
        "# To find the most common words\n",
        "# counter.most_common(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Enhanced Executable Code\n",
        "Below is a complete, runnable example. We ensure the necessary NLTK data is downloaded and print the results clearly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full Counter Object:\n",
            "Counter({'The': 3, 'cat': 3, 'the': 3, 'box': 3, '.': 3, 'is': 2, 'in': 1, 'likes': 1, 'over': 1})\n",
            "\n",
            "Top 2 most common words:\n",
            "[('The', 3), ('cat', 3)]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure the tokenizer model is downloaded\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Input text\n",
        "text = \"\"\"The cat is in the box. The cat likes the box. \n",
        "The box is over the cat.\"\"\"\n",
        "\n",
        "# 1. Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# 2. Create the Bag-of-words counter\n",
        "counter = Counter(tokens)\n",
        "\n",
        "# Display the full counter\n",
        "print(\"Full Counter Object:\")\n",
        "print(counter)\n",
        "\n",
        "# 3. Retrieve the top 2 most common words\n",
        "print(\"\\nTop 2 most common words:\")\n",
        "print(counter.most_common(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Explanation:**\n",
        "1.  `word_tokenize`: Splits the string into a list of words and punctuation.\n",
        "2.  `Counter`: Creates a dictionary-like object where keys are words and values are their frequencies.\n",
        "3.  `.most_common(n)`: A method of `Counter` that returns the top `n` frequent elements.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. Simple Text Preprocessing</span><br>\n",
        "\n",
        "### Why Preprocess?\n",
        "Raw text is often messy. Preprocessing helps create better input data for machine learning models or statistical analysis.\n",
        "\n",
        "**Common Preprocessing Steps:**\n",
        "1.  **Tokenization:** Creating the bag of words.\n",
        "2.  **Lowercasing:** Ensuring \"Cat\" and \"cat\" are treated as the same word.\n",
        "3.  **Lemmatization/Stemming:** Shortening words to their root stems (e.g., \"walking\" $\\rightarrow$ \"walk\").\n",
        "4.  **Filtering:** Removing stop words (common words like \"the\", \"is\", \"and\"), punctuation, or unwanted tokens.\n",
        "\n",
        "### Preprocessing Example\n",
        "*   **Input Text:** \"Cats, dogs and birds are common pets. So are fish.\"\n",
        "*   **Target Output Tokens:** `cat`, `dog`, `bird`, `common`, `pet`, `fish`.\n",
        "\n",
        "Notice how punctuation is removed, plurals are handled (in advanced stemming), and common words like \"are\" and \"and\" are removed.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Text Preprocessing with Python</span><br>\n",
        "\n",
        "We can combine list comprehensions with NLTK's stopword list to clean our text efficiently.\n",
        "\n",
        "### Original Code (From Document)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('cat', 3), ('box', 3)]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"\"\"The cat is in the box. The cat likes the box. \n",
        "The box is over the cat.\"\"\"\n",
        "\n",
        "# Tokenize and lowercase, keep only alphabetic tokens\n",
        "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
        "\n",
        "# Remove stopwords\n",
        "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
        "\n",
        "# Count results\n",
        "Counter(no_stops).most_common(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Enhanced Executable Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Tokens (subset): ['the', 'cat', 'is', 'in', 'the']...\n",
            "Cleaned Tokens: ['cat', 'box', 'cat', 'likes', 'box', 'box', 'cat']\n",
            "\n",
            "Most Common Cleaned Words:\n",
            "[('cat', 3), ('box', 3)]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "text = \"\"\"The cat is in the box. The cat likes the box. \n",
        "The box is over the cat.\"\"\"\n",
        "\n",
        "# 1. Tokenize and Lowercase\n",
        "# We also check w.isalpha() to remove punctuation like '.'\n",
        "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
        "\n",
        "# 2. Remove Stopwords\n",
        "# We load the english stopwords list\n",
        "english_stops = stopwords.words('english')\n",
        "no_stops = [t for t in tokens if t not in english_stops]\n",
        "\n",
        "# 3. Analyze results\n",
        "print(f\"Original Tokens (subset): {tokens[:5]}...\")\n",
        "print(f\"Cleaned Tokens: {no_stops}\")\n",
        "\n",
        "# 4. Most Common\n",
        "print(\"\\nMost Common Cleaned Words:\")\n",
        "print(Counter(no_stops).most_common(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> Notice that \"is\", \"in\", \"the\", and \"over\" were removed because they are in the standard English stopword list. This leaves us with the content-heavy words: \"cat\", \"box\", and \"likes\". </div>\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Introduction to Gensim</span><br>\n",
        "\n",
        "### What is Gensim?\n",
        "**Gensim** is a popular open-source NLP library designed for topic modeling and document similarity analysis. It uses top academic models to perform complex tasks such as:\n",
        "*   Building document or word vectors.\n",
        "*   Performing topic identification.\n",
        "*   Document comparison.\n",
        "\n",
        "### Word Vectors\n",
        "Gensim is famous for handling Word Vectors (embeddings). A word vector represents a word as a multi-dimensional coordinate. This allows for mathematical operations on words, such as the famous analogy:\n",
        "$$ \\text{King} - \\text{Man} + \\text{Woman} = \\text{Queen} $$\n",
        "\n",
        "### Building a Dictionary\n",
        "The first step in using Gensim for Bag-of-words is creating a **Dictionary**. This maps every unique word in the corpus to a unique integer ID.\n",
        "\n",
        "### Enhanced Executable Code: Creating a Dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token ID Mapping (first 10):\n",
            "Word: '.' -> ID: 0\n",
            "Word: 'a' -> ID: 1\n",
            "Word: 'about' -> ID: 2\n",
            "Word: 'aliens' -> ID: 3\n",
            "Word: 'and' -> ID: 4\n",
            "Word: 'movie' -> ID: 5\n",
            "Word: 'spaceship' -> ID: 6\n",
            "Word: 'the' -> ID: 7\n",
            "Word: 'was' -> ID: 8\n",
            "Word: '!' -> ID: 9\n"
          ]
        }
      ],
      "source": [
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# A corpus of movie reviews/descriptions\n",
        "my_documents = [\n",
        "    'The movie was about a spaceship and aliens.',\n",
        "    'I really liked the movie!',\n",
        "    'Awesome action scenes, but boring characters.',\n",
        "    'The movie was awful! I hate alien films.',\n",
        "    'Space is cool! I liked the movie.',\n",
        "    'More space films, please!',\n",
        "]\n",
        "\n",
        "# Tokenize all documents\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
        "\n",
        "# Create the Gensim Dictionary\n",
        "dictionary = Dictionary(tokenized_docs)\n",
        "\n",
        "# View the token to ID mapping\n",
        "print(\"Token ID Mapping (first 10):\")\n",
        "# We iterate to show just a few, as dictionaries can be large\n",
        "for k, v in list(dictionary.token2id.items())[:10]:\n",
        "    print(f\"Word: '{k}' -> ID: {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. Creating a Gensim Corpus</span><br>\n",
        "\n",
        "Once we have a dictionary, we can convert our documents into a **Corpus**. In Gensim, a corpus is a list of bag-of-words vectors.\n",
        "\n",
        "Instead of storing the strings, Gensim stores a list of tuples: `(word_id, word_frequency)`.\n",
        "\n",
        "### Original Code (From Document)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "# corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Enhanced Executable Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gensim Corpus (Bag-of-words representation):\n",
            "Doc 0: [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n",
            "Doc 1: [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)]\n",
            "Doc 2: [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]\n",
            "Doc 3: [(0, 1), (5, 1), (7, 1), (8, 1), (9, 1), (10, 1), (20, 1), (21, 1), (22, 1), (23, 1)]\n",
            "Doc 4: [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)]\n",
            "Doc 5: [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]\n"
          ]
        }
      ],
      "source": [
        "# Create the corpus using the dictionary from the previous section\n",
        "# doc2bow converts a collection of words to its bag-of-words representation\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "print(\"Gensim Corpus (Bag-of-words representation):\")\n",
        "for i, doc_vector in enumerate(corpus):\n",
        "    print(f\"Doc {i}: {doc_vector}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Understanding the Output:**\n",
        "If the output for a document is `[(0, 1), (1, 1)]`:\n",
        "*   The word with ID `0` appears `1` time.\n",
        "*   The word with ID `1` appears `1` time.\n",
        "\n",
        "This format is highly memory efficient compared to storing raw text. Gensim models can be easily saved, updated, and reused.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 7. Tf-idf with Gensim</span><br>\n",
        "\n",
        "### What is Tf-idf?\n",
        "**Tf-idf** stands for **Term Frequency - Inverse Document Frequency**. It is a numerical statistic intended to reflect how important a word is to a document in a collection (corpus).\n",
        "\n",
        "*   **Problem with raw counts:** Common words like \"the\" or \"movie\" (in a movie corpus) appear frequently but carry little specific meaning.\n",
        "*   **Solution:** Tf-idf down-weights words that appear in *many* documents and up-weights words that appear frequently in *one* document but rarely elsewhere.\n",
        "\n",
        "### The Formula\n",
        "The weight $w_{i,j}$ for token $i$ in document $j$ is calculated as:\n",
        "\n",
        "$$ w_{i,j} = tf_{i,j} * \\log(\\frac{N}{df_i}) $$\n",
        "\n",
        "Where:\n",
        "*   $tf_{i,j}$ = number of occurrences of token $i$ in document $j$.\n",
        "*   $df_i$ = number of documents that contain token $i$.\n",
        "*   $N$ = total number of documents.\n",
        "\n",
        "### Implementing Tf-idf in Gensim\n",
        "Gensim allows you to transform a simple Bag-of-words corpus into a Tf-idf corpus.\n",
        "\n",
        "### Enhanced Executable Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original BoW Vector for Doc 1: [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)]\n",
            "Tf-idf Weighted Vector for Doc 1: [(5, np.float64(0.1746298276735174)), (7, np.float64(0.1746298276735174)), (9, np.float64(0.1746298276735174)), (10, np.float64(0.29853166221463673)), (11, np.float64(0.47316148988815415)), (12, np.float64(0.7716931521027908))]\n",
            "\n",
            "Interpretation:\n",
            "Word: 'movie' \t Weight: 0.1746\n",
            "Word: 'the' \t Weight: 0.1746\n",
            "Word: '!' \t Weight: 0.1746\n",
            "Word: 'i' \t Weight: 0.2985\n",
            "Word: 'liked' \t Weight: 0.4732\n",
            "Word: 'really' \t Weight: 0.7717\n"
          ]
        }
      ],
      "source": [
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "\n",
        "# 1. Initialize the Tf-idf model using the existing corpus\n",
        "tfidf = TfidfModel(corpus)\n",
        "\n",
        "# 2. Apply the model to a specific document (e.g., the second document at index 1)\n",
        "# doc_vector is the BoW representation of \"I really liked the movie!\"\n",
        "doc_vector = corpus[1]\n",
        "tfidf_weights = tfidf[doc_vector]\n",
        "\n",
        "print(f\"Original BoW Vector for Doc 1: {doc_vector}\")\n",
        "print(f\"Tf-idf Weighted Vector for Doc 1: {tfidf_weights}\")\n",
        "\n",
        "# Let's see what words these IDs correspond to\n",
        "print(\"\\nInterpretation:\")\n",
        "for word_id, weight in tfidf_weights:\n",
        "    print(f\"Word: '{dictionary[word_id]}' \\t Weight: {weight:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Observation:**\n",
        "You will notice that common words (if they appear in many documents) have lower weights, while unique words specific to this sentence have higher weights.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 8. Conclusion</span><br>\n",
        "\n",
        "In this notebook, we explored the foundational techniques for processing text data in Python:\n",
        "\n",
        "1.  **Bag-of-words (BoW):** We learned that BoW converts text into word counts, discarding grammar but keeping frequency information.\n",
        "2.  **Preprocessing:** We utilized `nltk` to clean data by tokenizing, lowercasing, and removing stopwords to improve data quality.\n",
        "3.  **Gensim Dictionary & Corpus:** We moved beyond simple lists to using Gensim's efficient `Dictionary` (mapping words to IDs) and `Corpus` (sparse vector representations).\n",
        "4.  **Tf-idf:** We applied the Tf-idf transformation to down-weight common words and highlight significant, document-specific terms.\n",
        "\n",
        "**Next Steps:**\n",
        "These techniques form the basis for more advanced NLP tasks such as:\n",
        "*   **Topic Modeling:** Using Latent Dirichlet Allocation (LDA) on the Gensim corpus.\n",
        "*   **Document Similarity:** Calculating Cosine Similarity between Tf-idf vectors to find similar documents.\n",
        "*   **Sentiment Analysis:** Using the weighted word vectors as features for machine learning classifiers.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
