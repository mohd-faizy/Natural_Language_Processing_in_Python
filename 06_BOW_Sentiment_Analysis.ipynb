{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Bag-of-Words Sentiment Analysis in Python</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Text Processing, N-Grams, and Feature Engineering)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction to Bag-of-Words (BOW)](#section-1)\n",
        "2. [The Bag-of-Words Model Example](#section-2)\n",
        "3. [Implementing BOW with Scikit-Learn](#section-3)\n",
        "4. [Getting Granular with N-Grams](#section-4)\n",
        "5. [Specifying Vocabulary Size](#section-5)\n",
        "6. [Feature Engineering from Text](#section-6)\n",
        "7. [Language Detection](#section-7)\n",
        "8. [Conclusion](#section-8)\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. Introduction to Bag-of-Words (BOW)</span><br>\n",
        "\n",
        "### What is a Bag-of-Words?\n",
        "\n",
        "The **Bag-of-Words (BOW)** model is a fundamental concept in Natural Language Processing (NLP) used to transform text into numerical features that machine learning algorithms can understand.\n",
        "\n",
        "**Key Characteristics:**\n",
        "*   **Occurrence Description**: It describes the occurrence of words within a document or a collection of documents (known as a **corpus**).\n",
        "*   **Vocabulary Building**: It builds a vocabulary of all unique words found in the corpus and creates a measure of their presence (usually a count).\n",
        "*   **Loss of Structure**: As the name implies, it treats text as a \"bag\" of words, disregarding grammar rules and word order.\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> The core idea is that similar documents will have similar word counts. If two documents contain the word \"excellent\" multiple times, they are likely to have a similar positive sentiment. </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. The Bag-of-Words Model Example</span><br>\n",
        "\n",
        "### Amazon Product Reviews Data\n",
        "\n",
        "To understand BOW, let's look at a sample dataset of Amazon product reviews. The dataset typically contains a sentiment score and the text review.\n",
        "\n",
        "| | score | review |\n",
        "|:---|:---:|:---|\n",
        "| **0** | 1 | Stuning even for the non-gamer: This sound tr... |\n",
        "| **1** | 1 | The best soundtrack ever to anything.: I'm re... |\n",
        "| **2** | 1 | Amazing!: This soundtrack is my favorite musi... |\n",
        "| **3** | 1 | Excellent Soundtrack: I truly like this sound... |\n",
        "| **4** | 1 | Remember, Pull Your Jaw Off The Floor After H... |\n",
        "| **5** | 1 | an absolute masterpiece: I am quite sure any ... |\n",
        "| **6** | 0 | Buyer beware: This is a self-published book, ... |\n",
        "| **7** | 1 | Glorious story: I loved Whisper of the wicked... |\n",
        "| **8** | 1 | A FIVE STAR BOOK: I just finished reading Whi... |\n",
        "| **9** | 1 | Whispers of the Wicked Saints: This was a eas... |\n",
        "\n",
        "### How BOW Works Conceptually\n",
        "\n",
        "Consider the following positive review:\n",
        "> *\"This is the best book ever. I loved the book and highly recommend it!!!\"*\n",
        "\n",
        "A Bag-of-Words model converts this sentence into a frequency dictionary (or vector). It ignores punctuation and case (usually) and counts the tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frequency Dictionary: {'This': 1, 'is': 1, 'the': 2, 'best': 1, 'book': 2, 'ever': 1, 'I': 1, 'loved': 1, 'and': 1, 'highly': 1, 'recommend': 1, 'it': 1}\n"
          ]
        }
      ],
      "source": [
        "# Conceptual representation of the review above\n",
        "bow_representation = {\n",
        "    'This': 1,\n",
        "    'is': 1,\n",
        "    'the': 2,\n",
        "    'best': 1,\n",
        "    'book': 2,\n",
        "    'ever': 1,\n",
        "    'I': 1,\n",
        "    'loved': 1,\n",
        "    'and': 1,\n",
        "    'highly': 1,\n",
        "    'recommend': 1,\n",
        "    'it': 1\n",
        "}\n",
        "\n",
        "print(\"Frequency Dictionary:\", bow_representation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**The Resulting Matrix:**\n",
        "When applied to a whole corpus, the output looks like a matrix where:\n",
        "*   **Rows** represent individual documents (reviews).\n",
        "*   **Columns** represent every unique word in the vocabulary.\n",
        "*   **Values** represent the count of that word in that document.\n",
        "\n",
        "| | ... | wrong | wrote | year | years | yes | yet | you | young | your | yourself |\n",
        "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
        "| **0** | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |\n",
        "| **1** | ... | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
        "| **2** | ... | 0 | 0 | 0 | 1 | 0 | 0 | 2 | 0 | 0 | 0 |\n",
        "| **3** | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
        "| **4** | ... | 0 | 1 | 0 | 0 | 0 | 0 | 3 | 0 | 1 | 0 |\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. Implementing BOW with Scikit-Learn</span><br>\n",
        "\n",
        "We use the `CountVectorizer` from the `sklearn.feature_extraction.text` module to automate this process.\n",
        "\n",
        "### Step 1: Setup and Data Creation\n",
        "First, let's create a small DataFrame to mimic the Amazon reviews dataset so the code is executable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   score                                             review\n",
            "0      1  Stunning even for the non-gamer: This sound tr...\n",
            "1      1              The best soundtrack ever to anything.\n",
            "2      1     Amazing!: This soundtrack is my favorite music\n",
            "3      1  Excellent Soundtrack: I truly like this soundt...\n",
            "4      0  Buyer beware: This is a self-published book an...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating a dummy dataset to mimic the slides\n",
        "data = {\n",
        "    'score': [1, 1, 1, 1, 0],\n",
        "    'review': [\n",
        "        \"Stunning even for the non-gamer: This sound track is beautiful\",\n",
        "        \"The best soundtrack ever to anything.\",\n",
        "        \"Amazing!: This soundtrack is my favorite music\",\n",
        "        \"Excellent Soundtrack: I truly like this soundtrack\",\n",
        "        \"Buyer beware: This is a self-published book and it is not good.\"\n",
        "    ]\n",
        "}\n",
        "reviews = pd.DataFrame(data)\n",
        "print(reviews.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Step 2: Using CountVectorizer\n",
        "We instantiate the vectorizer, fit it to our data, and transform the data into a sparse matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X: (5, 32)\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 40 stored elements and shape (5, 32)>\n",
            "  Coords\tValues\n",
            "  (0, 3)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 12)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 15)\t1\n",
            "  (0, 20)\t1\n",
            "  (0, 24)\t1\n",
            "  (0, 26)\t1\n",
            "  (0, 27)\t1\n",
            "  (0, 28)\t1\n",
            "  (0, 30)\t1\n",
            "  (1, 2)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 9)\t1\n",
            "  (1, 25)\t1\n",
            "  (1, 27)\t1\n",
            "  (1, 29)\t1\n",
            "  (2, 0)\t1\n",
            "  (2, 11)\t1\n",
            "  (2, 15)\t1\n",
            "  (2, 18)\t1\n",
            "  (2, 19)\t1\n",
            "  (2, 25)\t1\n",
            "  (2, 28)\t1\n",
            "  (3, 10)\t1\n",
            "  (3, 17)\t1\n",
            "  (3, 25)\t2\n",
            "  (3, 28)\t1\n",
            "  (3, 31)\t1\n",
            "  (4, 1)\t1\n",
            "  (4, 5)\t1\n",
            "  (4, 6)\t1\n",
            "  (4, 7)\t1\n",
            "  (4, 14)\t1\n",
            "  (4, 15)\t2\n",
            "  (4, 16)\t1\n",
            "  (4, 21)\t1\n",
            "  (4, 22)\t1\n",
            "  (4, 23)\t1\n",
            "  (4, 28)\t1\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Instantiate the vectorizer\n",
        "# max_features=1000 limits the vocabulary to the top 1000 most frequent words\n",
        "vect = CountVectorizer(max_features=1000)\n",
        "\n",
        "# Fit the vectorizer to the data (learn the vocabulary)\n",
        "vect.fit(reviews.review)\n",
        "\n",
        "# Transform the data (create the matrix)\n",
        "X = vect.transform(reviews.review)\n",
        "\n",
        "# Output the shape and type of the matrix\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "print(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Step 3: Transforming the Output to a DataFrame\n",
        "The output `X` is a sparse matrix (which saves memory by only storing non-zero values). To visualize it or use it in standard pandas workflows, we can convert it to a dense array and then a DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   amazing  and  anything  beautiful  best  beware  book  buyer  even  ever\n",
            "0        0    0         0          1     0       0     0      0     1     0\n",
            "1        0    0         1          0     1       0     0      0     0     1\n",
            "2        1    0         0          0     0       0     0      0     0     0\n",
            "3        0    0         0          0     0       0     0      0     0     0\n",
            "4        0    1         0          0     0       1     1      1     0     0\n"
          ]
        }
      ],
      "source": [
        "# Transform to an array\n",
        "my_array = X.toarray()\n",
        "\n",
        "# Transform back to a dataframe, assign column names\n",
        "# get_feature_names_out() is the modern sklearn equivalent of get_feature_names()\n",
        "X_df = pd.DataFrame(my_array, columns=vect.get_feature_names_out())\n",
        "\n",
        "# Display the first few rows and columns\n",
        "print(X_df.iloc[:, :10].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Getting Granular with N-Grams</span><br>\n",
        "\n",
        "### Why Context Matters\n",
        "Standard Bag-of-Words considers words in isolation (Unigrams). However, context often changes the meaning entirely, especially with negation.\n",
        "\n",
        "*   **Example 1**: \"I am *happy*, **not** *sad*.\"\n",
        "*   **Example 2**: \"I am *sad*, **not** *happy*.\"\n",
        "\n",
        "If we only count \"happy\" and \"sad\", both sentences look identical. We need to capture the sequence.\n",
        "\n",
        "### N-Gram Types\n",
        "*   **Unigrams**: Single tokens (e.g., \"The\", \"weather\").\n",
        "*   **Bigrams**: Pairs of tokens (e.g., \"The weather\", \"weather today\").\n",
        "*   **Trigrams**: Triples of tokens (e.g., \"The weather today\").\n",
        "\n",
        "**Example Sentence**: \"The weather today is wonderful.\"\n",
        "\n",
        "*   **Unigrams**: `{The, weather, today, is, wonderful}`\n",
        "*   **Bigrams**: `{The weather, weather today, today is, is wonderful}`\n",
        "*   **Trigrams**: `{The weather today, weather today is, today is wonderful}`\n",
        "\n",
        "### Implementing N-Grams\n",
        "We can capture these sequences using the `ngram_range` argument in `CountVectorizer`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary with Bigrams (First 10):\n",
            "['amazing' 'amazing this' 'and' 'and it' 'anything' 'beautiful' 'best'\n",
            " 'best soundtrack' 'beware' 'beware this']\n"
          ]
        }
      ],
      "source": [
        "# Example: Capturing Unigrams and Bigrams\n",
        "# ngram_range=(min_n, max_n)\n",
        "\n",
        "# Only unigrams (Default)\n",
        "vect_uni = CountVectorizer(ngram_range=(1, 1))\n",
        "\n",
        "# Uni- and Bigrams\n",
        "vect_bi = CountVectorizer(ngram_range=(1, 2))\n",
        "\n",
        "# Fit and transform to see the difference\n",
        "vect_bi.fit(reviews.review)\n",
        "print(\"Vocabulary with Bigrams (First 10):\")\n",
        "print(vect_bi.get_feature_names_out()[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> <b>What is the best n?</b> Longer sequences result in more features and higher precision for models, but they drastically increase the risk of <b>overfitting</b> because specific long phrases may only appear in the training set. </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Specifying Vocabulary Size</span><br>\n",
        "\n",
        "When working with large corpora, the vocabulary can become massive. We can control the size using three key parameters in `CountVectorizer`.\n",
        "\n",
        "1.  **`max_features`**:\n",
        "    *   If specified (int), it includes only the top most frequent words.\n",
        "    *   If `None`, all words are included.\n",
        "2.  **`max_df`** (Maximum Document Frequency):\n",
        "    *   Ignores terms that appear in *too many* documents (likely stop words like \"the\", \"is\").\n",
        "    *   **Float (0.0 - 1.0)**: Represents a proportion (e.g., 0.95 means ignore words appearing in >95% of docs).\n",
        "    *   **Integer**: Absolute count.\n",
        "3.  **`min_df`** (Minimum Document Frequency):\n",
        "    *   Ignores terms that appear in *too few* documents (likely typos or extremely rare words).\n",
        "    *   **Float**: Proportion.\n",
        "    *   **Integer**: Absolute count.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example configuration\n",
        "vect_limited = CountVectorizer(\n",
        "    max_features=500,  # Keep top 500 words\n",
        "    max_df=0.95,       # Ignore words appearing in > 95% of reviews\n",
        "    min_df=2           # Ignore words appearing in < 2 reviews\n",
        ")\n",
        "\n",
        "# Note: With our tiny dummy dataset, min_df=2 might filter out almost everything, \n",
        "# but this is the syntax for real-world data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. Feature Engineering from Text</span><br>\n",
        "\n",
        "### Goal: Enriching the Dataset\n",
        "Beyond just word counts, we can extract meta-features from the text to help our model. Common features include:\n",
        "*   Length of the review.\n",
        "*   Number of sentences.\n",
        "*   Parts of speech involved.\n",
        "*   Count of punctuation marks (e.g., many \"!\" might indicate strong emotion).\n",
        "\n",
        "### Tokenizing Strings\n",
        "We use `nltk` (Natural Language Toolkit) to split text into tokens accurately.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Happy', 'families', 'are', 'all', 'alike', ',', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# Ensure you have the tokenizer data downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "# Example string\n",
        "anna_k = 'Happy families are all alike, every unhappy family is unhappy in its own way.'\n",
        "\n",
        "# Tokenize\n",
        "tokens = word_tokenize(anna_k)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Creating a \"Number of Tokens\" Feature\n",
        "We can apply this logic to the entire `reviews` column using list comprehension.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of word_tokens: <class 'list'>\n",
            "Type of first element: <class 'list'>\n",
            "   score                                             review  n_tokens\n",
            "0      1  Stunning even for the non-gamer: This sound tr...        11\n",
            "1      1              The best soundtrack ever to anything.         7\n",
            "2      1     Amazing!: This soundtrack is my favorite music         9\n",
            "3      1  Excellent Soundtrack: I truly like this soundt...         8\n",
            "4      0  Buyer beware: This is a self-published book an...        14\n"
          ]
        }
      ],
      "source": [
        "# 1. Create a list of tokens for every review\n",
        "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
        "\n",
        "print(f\"Type of word_tokens: {type(word_tokens)}\")\n",
        "print(f\"Type of first element: {type(word_tokens[0])}\")\n",
        "\n",
        "# 2. Calculate the length of each token list\n",
        "len_tokens = []\n",
        "for i in range(len(word_tokens)):\n",
        "    len_tokens.append(len(word_tokens[i]))\n",
        "\n",
        "# 3. Assign this new list as a column in the DataFrame\n",
        "reviews['n_tokens'] = len_tokens\n",
        "\n",
        "# View the result\n",
        "print(reviews[['score', 'review', 'n_tokens']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 7. Language Detection</span><br>\n",
        "\n",
        "### The Problem\n",
        "In a global dataset, reviews might be in different languages. We want to detect the language of each string and capture the most likely language in a new column. We use the `langdetect` library.\n",
        "\n",
        "### Detecting Language of a Single String\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[es:0.9999941038205692]\n"
          ]
        }
      ],
      "source": [
        "from langdetect import detect_langs\n",
        "\n",
        "# Example Spanish sentence\n",
        "foreign = 'Este libro ha sido uno de los mejores libros que he leido.'\n",
        "\n",
        "# Detect language\n",
        "# Returns a list of probabilities, e.g., [es:0.999...]\n",
        "result = detect_langs(foreign)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Building a Feature for the Language\n",
        "We iterate through the DataFrame rows and apply detection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw detection results (first 5): [[en:0.9999957297420001], [en:0.9999972643875168], [en:0.999994856018333], [en:0.9999970741645459], [en:0.999996468162091]]\n"
          ]
        }
      ],
      "source": [
        "languages = []\n",
        "\n",
        "# Loop through the reviews\n",
        "# Note: In a real scenario, ensure 'langdetect' is installed via pip\n",
        "# We will simulate the loop based on our dummy dataframe\n",
        "for row in range(len(reviews)):\n",
        "    # We use a try-except block because short text or numbers can cause errors\n",
        "    try:\n",
        "        # iloc[row, 1] refers to the 'review' column in our specific dataframe structure\n",
        "        # Adjust index based on your actual dataframe columns\n",
        "        lang_res = detect_langs(reviews.iloc[row, 1])\n",
        "        languages.append(lang_res)\n",
        "    except:\n",
        "        languages.append(\"unknown\")\n",
        "\n",
        "print(\"Raw detection results (first 5):\", languages[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Cleaning the Output\n",
        "The output of `detect_langs` is a list of objects. To get a clean string like 'es' or 'en', we need to parse it. The slides suggest converting to string and splitting, though accessing the object attributes directly is also possible. Here we follow the slide's logic:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              review language\n",
            "0  Stunning even for the non-gamer: This sound tr...       en\n",
            "1              The best soundtrack ever to anything.       en\n",
            "2     Amazing!: This soundtrack is my favorite music       en\n",
            "3  Excellent Soundtrack: I truly like this soundt...       en\n",
            "4  Buyer beware: This is a self-published book an...       en\n"
          ]
        }
      ],
      "source": [
        "# Logic demonstrated in slides:\n",
        "# 1. Convert list to string -> \"[es:0.999]\"\n",
        "# 2. Split on colon -> [\"['es\", \"0.999]\"]\n",
        "# 3. Take first element -> \"['es\"\n",
        "# 4. Slice to remove bracket -> \"es\"\n",
        "\n",
        "# Applying this logic to the list\n",
        "clean_languages = []\n",
        "for lang in languages:\n",
        "    if lang != \"unknown\":\n",
        "        # Convert the first detection result to string and parse\n",
        "        # Note: lang[0] gets the most probable language object\n",
        "        code = str(lang[0]).split(':')[0]\n",
        "        clean_languages.append(code)\n",
        "    else:\n",
        "        clean_languages.append('unknown')\n",
        "\n",
        "# Assign to DataFrame\n",
        "reviews['language'] = clean_languages\n",
        "\n",
        "print(reviews[['review', 'language']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 8. Conclusion</span><br>\n",
        "\n",
        "In this notebook, we covered the essential techniques for preparing text data for sentiment analysis:\n",
        "\n",
        "1.  **Bag-of-Words (BOW)**: We learned how to convert unstructured text into a numeric matrix of token counts using `CountVectorizer`.\n",
        "2.  **N-Grams**: We explored how to capture context (like \"not happy\") by including Bigrams and Trigrams, rather than just single words.\n",
        "3.  **Vocabulary Management**: We discussed using `max_features`, `max_df`, and `min_df` to optimize the size of our feature set and reduce noise.\n",
        "4.  **Feature Engineering**: We extracted meta-features like review length (`n_tokens`) to enrich the dataset.\n",
        "5.  **Language Detection**: We used `langdetect` to identify the language of reviews, which is crucial for filtering or specific processing in multi-lingual datasets.\n",
        "\n",
        "**Next Steps:**\n",
        "With these numeric features (the BOW matrix and the extra columns), you are now ready to train machine learning models (like Logistic Regression or Naive Bayes) to predict the sentiment score of new reviews.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
