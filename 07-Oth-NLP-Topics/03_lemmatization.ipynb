{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<strong>\n",
        "    <h1 align='center'><strong>Lemmatization</strong></h1>\n",
        "</strong>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WFtXMYkVYWjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming:**\n",
        "\n",
        "- **Stemming** is a text normalization technique that aims to reduce words to their root or stem form by removing `suffixes` or `prefixes`.\n",
        "\n",
        "- It operates through a set of heuristic rules and does not consider the meaning of words. As a result, stemming can sometimes produce root forms that are not actual words.\n",
        "\n",
        "- For example:\n",
        "  - `\"jumping\" → jump\"`\n",
        "  - `\"flies\" → \"fli\"`\n",
        "\n",
        "**Lemmatization:**\n",
        "\n",
        "- **Lemmatization** is a text normalization technique that aims to reduce words to their base or root form by **considering the word's meaning and context**.\n",
        "\n",
        "- It converts words to their `lemma` or `dictionary form`, which is a valid word.\n",
        "\n",
        "- **Lemmatization** typically involves looking up a word in a `lexical resource` (like a dictionary) to find its base form.\n",
        "\n",
        "- For example:\n",
        "  - `\"jumping\" → \"jump\"`\n",
        "  - `\"flies\" → \"fly\"`\n",
        "\n",
        "\n",
        "In summary, while both **stemming** and **lemmatization** are used for text normalization in NLP.\n",
        "\n",
        "- **lemmatization** is a more sophisticated technique that considers the meaning and context of words, resulting in more accurate base forms.\n",
        "\n",
        "- Stemming, on the other hand, uses heuristic rules and may produce root forms that are not actual words.\n",
        "\n",
        "The choice between `stemming` and `lemmatization` depends on the specific requirements of the NLP task and the level of linguistic accuracy needed.\n",
        "\n",
        "Lemmatization is often preferred when working with tasks that require a deeper understanding of language, such as information retrieval or machine translation, while stemming may be suitable for simpler tasks like text classification or search engine indexing."
      ],
      "metadata": {
        "id": "NakpiEkMo5uR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "VbzwIb3xOirB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download a specific NLTK dataset, e.g., the 'punkt' tokenizer models.\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Download the NLTK stopwords dataset, which contains common stopwords for various languages.\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Download the NLTK averaged perceptron tagger, which is used for part-of-speech tagging.\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "# Download the WordNet lexical database, which is used for various NLP tasks like synonym and antonym lookup.\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# Download the NLTK names dataset, which contains a list of common first names and last names.\n",
        "# nltk.download('names', quiet=True)\n",
        "\n",
        "# Download the NLTK movie_reviews dataset, which contains movie reviews categorized as positive and negative.\n",
        "# nltk.download('movie_reviews', quiet=True)\n",
        "\n",
        "# Download the NLTK reuters dataset, which is a collection of news documents categorized into topics.\n",
        "# nltk.download('reuters', quiet=True)\n",
        "\n",
        "# Download the NLTK brown corpus, which is a collection of text from various genres of written American English.\n",
        "# nltk.download('brown', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtShdAU6N3QZ",
        "outputId": "8b6bcf79-5524-4dc5-b9a5-9cc9d948cdf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"The cats were chasing the mice, but they couldn't catch them.\"\n",
        "\n",
        "# Initialize WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenize the sentence\n",
        "words = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Lemmatize each word in the sentence\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "# Join the lemmatized words back into a sentence\n",
        "lemmatized_sentence = \" \".join(lemmatized_words)\n",
        "\n",
        "# Print the original and lemmatized sentences\n",
        "print(\"Original Sentence:\", sentence)\n",
        "print(\"Lemmatized Sentence:\", lemmatized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UBtbVtiOwfS",
        "outputId": "76eabd08-2906-446b-b442-e08879e1d363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: The cats were chasing the mice, but they couldn't catch them.\n",
            "Lemmatized Sentence: The cat were chasing the mouse , but they could n't catch them .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "my_list = [\"apple\", \"banana\", \"cherry\"]\n",
        "separator = \", \"\n",
        "result = separator.join(my_list)\n",
        "print(result)\n",
        "\n",
        "Output:\n",
        "-----------------------\n",
        "apple, banana, cherry\n",
        "-----------------------\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "rVaVDtuN13HR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason `\"chasing\"` is not changed to `\"chase\"` in the lemmatized sentence is because the `WordNetLemmatizer` **in NLTK, by default, does not perform lemmatization on verbs** if they are in their `present participle (-ing)` form.\n",
        "\n",
        "Instead, it preserves them as they are. **This is because lemmatization aims to reduce words to their base or dictionary form, and for verbs, the base form is typically the infinitive form**.\n",
        "\n",
        "If you want to lemmatize verbs into their base form, you can specify the part of speech (POS) tag when calling the lemmatize method.\n",
        "\n",
        "In this case, you can tag the words in the sentence with their corresponding POS before lemmatization.\n",
        "\n",
        "**Here's how you can modify your code to achieve this:**"
      ],
      "metadata": {
        "id": "SSLU9DL9aYZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"The cats were chasing the mice, but they couldn't catch them.\"\n",
        "\n",
        "# Initialize WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenize the sentence\n",
        "words = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Tag the words with their POS (Part of Speech)\n",
        "tagged_words = nltk.pos_tag(words)\n",
        "\n",
        "# Define a function to map POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return 'a'  # Adjective\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return 'v'  # Verb\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return 'n'  # Noun\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return 'r'  # Adverb\n",
        "    else:\n",
        "        return 'n'  # Default to noun if POS tag is not found\n",
        "\n",
        "# Lemmatize each word in the sentence based on its POS\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(pos_tag)) for word, pos_tag in tagged_words]\n",
        "\n",
        "# Join the lemmatized words back into a sentence\n",
        "lemmatized_sentence = \" \".join(lemmatized_words)\n",
        "\n",
        "# Print the original and lemmatized sentences\n",
        "print(\"Original Sentence:\", sentence)\n",
        "print(\"Lemmatized Sentence:\", lemmatized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO2W6VhUPLGn",
        "outputId": "fa5ad77f-4250-4a53-825b-7081886ff433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: The cats were chasing the mice, but they couldn't catch them.\n",
            "Lemmatized Sentence: The cat be chase the mouse , but they could n't catch them .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLPIibB3NowW",
        "outputId": "e6a6d68b-bab2-46e3-de61-459646b12b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1: quick brown fox jumping lazy dog .\n",
            "Document 2: Sheep grazing peacefully meadow .\n",
            "Document 3: studying NLTK natural language processing .\n",
            "Document 4: Lemmatization help reduce word base form .\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Sample text corpus\n",
        "corpus = [\n",
        "    \"The quick brown foxes are jumping over the lazy dogs.\",\n",
        "    \"Sheep are grazing peacefully in the meadow.\",\n",
        "    \"I have been studying NLTK for natural language processing.\",\n",
        "    \"Lemmatization helps reduce words to their base form.\"\n",
        "]\n",
        "\n",
        "# Initialize WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenize the corpus and perform lemmatization\n",
        "lemmatized_corpus = []\n",
        "\n",
        "for document in corpus:\n",
        "    words = nltk.word_tokenize(document)\n",
        "    # Remove stopwords and perform lemmatization\n",
        "    filtered_words = [lemmatizer.lemmatize(word) for word in words if word.lower() not in stopwords.words('english')]\n",
        "    lemmatized_corpus.append(\" \".join(filtered_words))\n",
        "\n",
        "# Print the lemmatized corpus\n",
        "for i, document in enumerate(lemmatized_corpus):\n",
        "    print(f\"Document {i + 1}: {document}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "paragraph = \"\"\"Thank you to all of you in this room. I have to congratulate\n",
        "               the other incredible nominees this year. The Revenant was\n",
        "               the product of the tireless efforts of an unbelievable cast\n",
        "               and crew. First off, to my brother in this endeavor, Mr. Tom\n",
        "               Hardy. Tom, your talent on screen can only be surpassed by\n",
        "               your friendship off screen … thank you for creating a t\n",
        "               ranscendent cinematic experience. Thank you to everybody at\n",
        "               Fox and New Regency … my entire team. I have to thank\n",
        "               everyone from the very onset of my career … To my parents;\n",
        "               none of this would be possible without you. And to my\n",
        "               friends, I love you dearly; you know who you are. And lastly,\n",
        "               I just want to say this: Making The Revenant was about\n",
        "               man's relationship to the natural world. A world that we\n",
        "               collectively felt in 2023 as the hottest year in recorded\n",
        "               history. Our production needed to move to the southern\n",
        "               tip of this planet just to be able to find snow. Climate\n",
        "               change is real, it is happening right now. It is the most\n",
        "               urgent threat facing our entire species, and we need to work\n",
        "               collectively together and stop procrastinating. We need to\n",
        "               support leaders around the world who do not speak for the\n",
        "               big polluters, but who speak for all of humanity, for the\n",
        "               indigenous people of the world, for the billions and\n",
        "               billions of underprivileged people out there who would be\n",
        "               most affected by this. For our children’s children, and\n",
        "               for those people out there whose voices have been drowned\n",
        "               out by the politics of greed. I thank you all for this\n",
        "               amazing award tonight. Let us not take this planet for\n",
        "               granted. I do not take tonight for granted. Thank you so very much.\"\"\"\n",
        "\n",
        "\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatization\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)\n",
        "\n",
        "pprint(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGN_1G-NOSLO",
        "outputId": "9de25a18-b887-4ac0-8c77-aa48beb82c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Thank room .',\n",
            " 'I congratulate incredible nominee year .',\n",
            " 'The Revenant product tireless effort unbelievable cast crew .',\n",
            " 'First , brother endeavor , Mr. Tom Hardy .',\n",
            " 'Tom , talent screen surpassed friendship screen … thank creating ranscendent '\n",
            " 'cinematic experience .',\n",
            " 'Thank everybody Fox New Regency … entire team .',\n",
            " 'I thank everyone onset career … To parent ; none would possible without .',\n",
            " 'And friend , I love dearly ; know .',\n",
            " \"And lastly , I want say : Making The Revenant man 's relationship natural \"\n",
            " 'world .',\n",
            " 'A world collectively felt 2023 hottest year recorded history .',\n",
            " 'Our production needed move southern tip planet able find snow .',\n",
            " 'Climate change real , happening right .',\n",
            " 'It urgent threat facing entire specie , need work collectively together stop '\n",
            " 'procrastinating .',\n",
            " 'We need support leader around world speak big polluter , speak humanity , '\n",
            " 'indigenous people world , billion billion underprivileged people would '\n",
            " 'affected .',\n",
            " 'For child ’ child , people whose voice drowned politics greed .',\n",
            " 'I thank amazing award tonight .',\n",
            " 'Let u take planet granted .',\n",
            " 'I take tonight granted .',\n",
            " 'Thank much .']\n"
          ]
        }
      ]
    }
  ]
}