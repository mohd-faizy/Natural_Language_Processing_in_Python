{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<strong>\n",
        "    <h1 align='center'><strong>GemSim</strong></h1>\n",
        "</strong>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "cZMhqOJhJpn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Difference between `nltk.word_tokenize` and `gensim.utils.simple_preprocess`**?\n"
      ],
      "metadata": {
        "id": "IsbJUKOVLUun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "text = \"How's it going, folks?\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MalICLKjLUFF",
        "outputId": "b3aa4b3c-ead9-4c46-cdb1-272f9ff70f71"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['How', \"'s\", 'it', 'going', ',', 'folks', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "text = \"How's it going, folks?\"\n",
        "tokens = simple_preprocess(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba4RAkisMH1z",
        "outputId": "3ddb72ad-89cb-44db-dbfc-e1bd5efcdc4e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['how', 'it', 'going', 'folks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **When to use which?**\n",
        "\n",
        "- if we need more fine-grained and **linguistically sophisticated** tokenization, especially when dealing with `contractions`, `punctuation`, and other `complex` cases, you should use `nltk.word_tokenize`.\n",
        "\n",
        "- On the other hand, if you prefer a simple and efficient tokenization approach that splits text into words based on whitespace and removes non-alphanumeric characters, you can use gensim.utils.simple_preprocess. The choice between them depends on the specific requirements of your NLP task."
      ],
      "metadata": {
        "id": "JbuMv27xMUHI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdl5Nw1_JmQs",
        "outputId": "cb543f2f-98e8-48a2-b120-0dedf00b89bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['example', 'sentence'],\n",
            " ['another', 'sentence', 'punctuation'],\n",
            " ['remove', 'stop', 'words', 'text', 'extra', 'spaces', 'words'],\n",
            " ['hows', 'going', 'folks', 'cant', 'believe', 'already', 'september'],\n",
            " ['text', 'contains', 'numbers', 'like', 'symbols', 'testing'],\n",
            " ['nlp', 'fantastic', 'isnt'],\n",
            " ['lets', 'test', 'function', 'corpus']]\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from pprint import pprint\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()                                                               # Lowercase the text\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])         # Remove punctuation\n",
        "    tokens = gensim.utils.simple_preprocess(text, deacc=False, min_len=2, max_len=15) # Tokenization\n",
        "    stop_words = set(stopwords.words('english'))                                      # Remove stop words\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "corpus = [\n",
        "    \"This is an EXAMPLE sentence.\",\n",
        "    \"Another sentence with SOME punctuation!\",\n",
        "    \"Remove STOP words from this text. And it has extra spaces   between  words!\",\n",
        "    \"How's it going, folks? I can't believe it's already September 2023!\",\n",
        "    \"This text contains numbers like 123 and symbols #@$%! Testing...\",\n",
        "    \"NLP is fantastic! Isn't it?\",\n",
        "    \"Let's test our function on this corpus.\"\n",
        "]\n",
        "\n",
        "# Preprocess the corpus\n",
        "preprocessed_corpus = [preprocess_text(doc) for doc in corpus]\n",
        "\n",
        "# Example output for the first document:\n",
        "pprint(preprocessed_corpus)"
      ]
    }
  ]
}