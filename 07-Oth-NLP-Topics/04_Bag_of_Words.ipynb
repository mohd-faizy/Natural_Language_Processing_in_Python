{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<strong>\n",
        "    <h1 align='center'><strong>Bag of Words</strong></h1>\n",
        "</strong>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "tEAmzhOV0UCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing necessary libraries**"
      ],
      "metadata": {
        "id": "dn2c32qdZ9qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "404NXQCY5Sb3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download a specific NLTK dataset, e.g., the 'punkt' tokenizer models.\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Download the NLTK stopwords dataset, which contains common stopwords for various languages.\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Download the WordNet lexical database, which is used for various NLP tasks like synonym and antonym lookup.\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# Download the NLTK averaged perceptron tagger, which is used for part-of-speech tagging.\n",
        "# nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "\n",
        "# Download the NLTK names dataset, which contains a list of common first names and last names.\n",
        "# nltk.download('names', quiet=True)\n",
        "\n",
        "# Download the NLTK movie_reviews dataset, which contains movie reviews categorized as positive and negative.\n",
        "# nltk.download('movie_reviews', quiet=True)\n",
        "\n",
        "# Download the NLTK reuters dataset, which is a collection of news documents categorized into topics.\n",
        "# nltk.download('reuters', quiet=True)\n",
        "\n",
        "# Download the NLTK brown corpus, which is a collection of text from various genres of written American English.\n",
        "# nltk.download('brown', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kyvhhDZ4bkg",
        "outputId": "2d372706-e9e4-4fa7-a284-72cbc0a9d58d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"The cat in the hat.\",\n",
        "    \"The dog chased the cat.\"\n",
        "]\n",
        "\n",
        "\n",
        "# Create a CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the documents to create the bag of words representation\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the bag of words representation to a dense matrix and print it\n",
        "print(\"Bag of Words Matrix:\")\n",
        "print(X.toarray())\n",
        "\n",
        "# Get the vocabulary\n",
        "vocabulary = sorted(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the vocabulary\n",
        "print(\"Vocabulary:\")\n",
        "print(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s37AWwxIXDg9",
        "outputId": "f1a86d68-12da-471d-e08b-5b2e3a0860d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Matrix:\n",
            "[[1 0 0 1 1 2]\n",
            " [1 1 1 0 0 2]]\n",
            "Vocabulary:\n",
            "['cat', 'chased', 'dog', 'hat', 'in', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words in Natural Language Processing (NLP)\n",
        "\n",
        "In **Natural Language Processing (NLP)**, the **\"bag of words\"** (BoW) is a simple and widely used technique for **text data preprocessing** and **feature extraction**. It represents text data as a collection of individual words or tokens, without considering the order or structure of the words in the text. Here's how it works:\n",
        "\n",
        "### Tokenization\n",
        "The first step in creating a bag of words representation is to tokenize the text. Tokenization involves splitting the text into individual words or tokens. For example, the sentence \"The quick brown fox\" would be tokenized into the tokens: [\"The\", \"quick\", \"brown\", \"fox\"].\n",
        "\n",
        "### Vocabulary Creation\n",
        "Next, a vocabulary or dictionary is created. This vocabulary contains all unique words or tokens from the entire corpus of text data. For example, if you have a collection of documents, the vocabulary would contain all the unique words from those documents.\n",
        "\n",
        "### Vectorization\n",
        "Once the vocabulary is established, each document or text sample is represented as a vector. The vector is typically of fixed length, and each element of the vector corresponds to a word in the vocabulary. The value of each element in the vector represents the frequency of the corresponding word in the document. There are variations, such as using binary values (`1` if the word is present, `0` if not) or term frequency-inverse document frequency (TF-IDF) values instead of raw word frequencies.\n",
        "\n",
        "#### Example:\n",
        "Let's say you have two sentences:\n",
        "\n",
        "- Sentence 1: `\"The cat in the hat.\"`\n",
        "- Sentence 2: `\"The dog chased the cat.\"`\n",
        "\n",
        "\n",
        "\n",
        "Using the bag of words representation with word frequency counting, the vector representations of these sentences would be:\n",
        "\n",
        "# Bag of Words Matrix\n",
        "\n",
        "| Document | cat | chased | dog | hat | in | the |\n",
        "|---|---|---|---|---|---|---|\n",
        "| The cat in the hat. | 1 | 0 | 0 | 1 | 1 | 2 |\n",
        "| The dog chased the cat. | 1 | 1 | 1 | 0 | 0 | 2 |\n",
        "\n",
        "\n",
        "\n",
        "- `cat` appears once in the first document and once in the second document.\n",
        "- `chased` appears zero times in the first document and once in the second - document.\n",
        "- `dog` appears zero times in the first document and once in the second document.\n",
        "- `hat` appears once in the first document and zero times in the second document.\n",
        "- `in` appears once in the first document and zero times in the second document.\n",
        "- `the` appears twice in the first document and twice in the second document.\n"
      ],
      "metadata": {
        "id": "sCp2Vv7nUO_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explaining the bags of words Matrix:**\n",
        "\n",
        "- Each `row` in the matrix represents one of the input documents.\n",
        "Each column represents a unique word (or token) from the entire corpus of documents.\n",
        "\n",
        "- The values in the matrix indicate the frequency of each word's occurrence in each document."
      ],
      "metadata": {
        "id": "VL73dxxeYNKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The bag of words model has certain limitations:**\n",
        "\n",
        "- It does not capture **word order** within a document.\n",
        "- It does not consider the **semantic meaning** of words or the context in which they are used.\n",
        "\n",
        "To address these limitations, more advanced techniques have been developed:\n",
        "\n",
        "- **Word Embeddings**: Techniques like `Word2Vec` and `GloVe` generate `word embeddings`, which are **dense vector representations** of words that **capture semantic meaning and relationships between words**.\n",
        "\n",
        "- **Deep Learning Models**: Models like `Recurrent Neural Networks (RNNs)` and `Transformer-based models (e.g., BERT)` are **capable of capturing complex patterns** and **context in text data**.\n",
        "\n",
        "- `RNNs` are especially good at considering sequential information, while **Transformers** excel at capturing `long-range dependencies`.\n",
        "\n",
        "These advanced techniques provide richer and more meaningful representations of text data, making them suitable for a wide range of NLP tasks, including text classification, sentiment analysis, and machine translation.\n"
      ],
      "metadata": {
        "id": "93AcqKtAXLH1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "211c_OhM0OaK",
        "outputId": "40bcf1d9-2509-4179-9b5a-9df93212e507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in the text corpus: 8\n",
            "['natural language processing nlp field artificial intelligence focus interaction computer human natural language', 'ultimate goal nlp enable computer understand interpret generate human language way meaningful useful', 'nlp technique used wide range application including machine translation speech recognition sentiment analysis chatbots information retrieval', 'involves various task tokenization part speech tagging named entity recognition syntactic parsing', 'nltk popular python library nlp providing tool resource task like text processing text classification language modeling', 'offer wide range function datasets help get started nlp project', 'sample text demonstrate basic nlp task using nltk tokenization part speech tagging', 'let get started']\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries for text cleaning and processing\n",
        "import re                                  # Regular expressions library for text pattern matching and replacement\n",
        "import nltk                                # Natural Language Toolkit library for various NLP tasks\n",
        "from nltk.corpus import stopwords          # NLTK corpus containing common stop words\n",
        "from nltk.stem.porter import PorterStemmer # Stemming algorithm for word reduction\n",
        "from nltk.stem import WordNetLemmatizer    # Lemmatization tool for word normalization\n",
        "\n",
        "# Creating the stemming and lemmatization objects\n",
        "ps = PorterStemmer()\n",
        "wordnet = WordNetLemmatizer()\n",
        "\n",
        "# we will convert this whole text-corpus into the bags of words\n",
        "text = \"\"\"\n",
        "                Natural Language Processing (NLP) is a field of artificial intelligence that focuses\n",
        "                on the interaction between computers and humans through natural language. The ultimate\n",
        "                goal of NLP is to enable computers to understand, interpret, and generate human language\n",
        "                in a way that is both meaningful and useful. NLP techniques are used in a wide range of\n",
        "                applications, including machine translation, speech recognition, sentiment analysis,\n",
        "                chatbots, and information retrieval. It involves various tasks such as tokenization,\n",
        "                part-of-speech tagging, named entity recognition, and syntactic parsing. NLTK is a popular\n",
        "                Python library for NLP, providing tools and resources for tasks like text processing, text\n",
        "                classification, and language modeling. It offers a wide range of functions and datasets\n",
        "                to help you get started with NLP projects. In this sample text, we'll demonstrate some\n",
        "                basic NLP tasks using NLTK, such as tokenization and part-of-speech tagging.\n",
        "                Let's get started!\n",
        "             \"\"\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(\"Number of sentences in the text corpus:\", len(sentences))\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    cleaned_sentence = re.sub('[^a-zA-Z]', ' ', sentences[i]) # Removes all characters from sentences[i] that are not alphabetic characters and replaces them with spaces.\n",
        "    cleaned_sentence = cleaned_sentence.lower()\n",
        "    cleaned_sentence = cleaned_sentence.split()\n",
        "    cleaned_sentence = [wordnet.lemmatize(word) for word in cleaned_sentence if not word in set(stopwords.words('english'))]\n",
        "    cleaned_sentence = ' '.join(cleaned_sentence)\n",
        "    corpus.append(cleaned_sentence)\n",
        "\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words (BoW) model using the scikit-learn library in Python.**"
      ],
      "metadata": {
        "id": "GRIsXwynNIN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Bag of Words model\n",
        "# CountVectorizer is used for text preprocessing and creating the Bag of Words model.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# This parameter controls the maximum number of words (features) to include in the BoW model.\n",
        "# It means that only the top 1500 most frequent words in your corpus will be considered as features in the BoW model.\n",
        "vectorizer = CountVectorizer(max_features=1500)\n",
        "\n",
        "# Converting the corpus into the the Bag of Words matrix then to a dense Numpy array\n",
        "X = vectorizer.fit_transform(corpus).toarray()\n",
        "\n",
        "print(\"Bag of Words Model:\")\n",
        "pprint(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIyFSDRqM4aF",
        "outputId": "f7265128-10cd-4c13-db12-ae990250bfb3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Model:\n",
            "array([[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
            "        1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
            "        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
            "        1, 0],\n",
            "       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
            "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1],\n",
            "       [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0]])\n"
          ]
        }
      ]
    }
  ]
}