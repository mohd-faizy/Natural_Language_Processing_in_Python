{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<strong>\n",
        "    <h1 align='center'><strong>Bag of Words</strong></h1>\n",
        "</strong>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "tEAmzhOV0UCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing necessary libraries**"
      ],
      "metadata": {
        "id": "dn2c32qdZ9qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "404NXQCY5Sb3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download a specific NLTK dataset, e.g., the 'punkt' tokenizer models.\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Download the NLTK stopwords dataset, which contains common stopwords for various languages.\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Download the WordNet lexical database, which is used for various NLP tasks like synonym and antonym lookup.\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# Download the NLTK averaged perceptron tagger, which is used for part-of-speech tagging.\n",
        "# nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "\n",
        "# Download the NLTK names dataset, which contains a list of common first names and last names.\n",
        "# nltk.download('names', quiet=True)\n",
        "\n",
        "# Download the NLTK movie_reviews dataset, which contains movie reviews categorized as positive and negative.\n",
        "# nltk.download('movie_reviews', quiet=True)\n",
        "\n",
        "# Download the NLTK reuters dataset, which is a collection of news documents categorized into topics.\n",
        "# nltk.download('reuters', quiet=True)\n",
        "\n",
        "# Download the NLTK brown corpus, which is a collection of text from various genres of written American English.\n",
        "# nltk.download('brown', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kyvhhDZ4bkg",
        "outputId": "612a6626-c1bc-473b-e8bc-fd6e182433f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"The cat in the hat.\",\n",
        "    \"The dog chased the cat.\"\n",
        "]\n",
        "\n",
        "\n",
        "# Create a CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the documents to create the bag of words representation\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the bag of words representation to a dense matrix and print it\n",
        "print(\"Bag of Words Matrix:\")\n",
        "print(X.toarray())\n",
        "\n",
        "# Get the vocabulary\n",
        "vocabulary = sorted(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the vocabulary\n",
        "print(\"Vocabulary:\")\n",
        "print(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s37AWwxIXDg9",
        "outputId": "f26defc8-17bc-4913-c86c-3eb0437aeb8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Matrix:\n",
            "[[1 0 0 1 1 2]\n",
            " [1 1 1 0 0 2]]\n",
            "Vocabulary:\n",
            "['cat', 'chased', 'dog', 'hat', 'in', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words in Natural Language Processing (NLP)\n",
        "\n",
        "In **Natural Language Processing (NLP)**, the **\"bag of words\"** (BoW) is a simple and widely used technique for **text data preprocessing** and **feature extraction**. It represents text data as a collection of individual words or tokens, without considering the order or structure of the words in the text. Here's how it works:\n",
        "\n",
        "### Tokenization\n",
        "The first step in creating a bag of words representation is to tokenize the text. Tokenization involves splitting the text into individual words or tokens. For example, the sentence \"The quick brown fox\" would be tokenized into the tokens: [\"The\", \"quick\", \"brown\", \"fox\"].\n",
        "\n",
        "### Vocabulary Creation\n",
        "Next, a vocabulary or dictionary is created. This vocabulary contains all unique words or tokens from the entire corpus of text data. For example, if you have a collection of documents, the vocabulary would contain all the unique words from those documents.\n",
        "\n",
        "### Vectorization\n",
        "Once the vocabulary is established, each document or text sample is represented as a vector. The vector is typically of fixed length, and each element of the vector corresponds to a word in the vocabulary. The value of each element in the vector represents the frequency of the corresponding word in the document. There are variations, such as using binary values (`1` if the word is present, `0` if not) or term frequency-inverse document frequency (TF-IDF) values instead of raw word frequencies.\n",
        "\n",
        "#### Example:\n",
        "Let's say you have two sentences:\n",
        "\n",
        "- Sentence 1: `\"The cat in the hat.\"`\n",
        "- Sentence 2: `\"The dog chased the cat.\"`\n",
        "\n",
        "\n",
        "\n",
        "Using the bag of words representation with word frequency counting, the vector representations of these sentences would be:\n",
        "\n",
        "# Bag of Words Matrix\n",
        "\n",
        "| Document | cat | chased | dog | hat | in | the |\n",
        "|---|---|---|---|---|---|---|\n",
        "| The cat in the hat. | 1 | 0 | 0 | 1 | 1 | 2 |\n",
        "| The dog chased the cat. | 1 | 1 | 1 | 0 | 0 | 2 |\n",
        "\n",
        "\n",
        "\n",
        "- `cat` appears once in the first document and once in the second document.\n",
        "- `chased` appears zero times in the first document and once in the second - document.\n",
        "- `dog` appears zero times in the first document and once in the second document.\n",
        "- `hat` appears once in the first document and zero times in the second document.\n",
        "- `in` appears once in the first document and zero times in the second document.\n",
        "- `the` appears twice in the first document and twice in the second document.\n"
      ],
      "metadata": {
        "id": "sCp2Vv7nUO_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explaining the bags of words Matrix:**\n",
        "\n",
        "- Each `row` in the matrix represents one of the input documents.\n",
        "Each column represents a unique word (or token) from the entire corpus of documents.\n",
        "\n",
        "- The values in the matrix indicate the frequency of each word's occurrence in each document."
      ],
      "metadata": {
        "id": "VL73dxxeYNKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The bag of words model has certain limitations:**\n",
        "\n",
        "- It does not capture **word order** within a document.\n",
        "- It does not consider the **semantic meaning** of words or the context in which they are used.\n",
        "\n",
        "To address these limitations, more advanced techniques have been developed:\n",
        "\n",
        "- **Word Embeddings**: Techniques like `Word2Vec` and `GloVe` generate `word embeddings`, which are **dense vector representations** of words that **capture semantic meaning and relationships between words**.\n",
        "\n",
        "- **Deep Learning Models**: Models like `Recurrent Neural Networks (RNNs)` and `Transformer-based models (e.g., BERT)` are **capable of capturing complex patterns** and **context in text data**.\n",
        "\n",
        "- `RNNs` are especially good at considering sequential information, while **Transformers** excel at capturing `long-range dependencies`.\n",
        "\n",
        "These advanced techniques provide richer and more meaningful representations of text data, making them suitable for a wide range of NLP tasks, including text classification, sentiment analysis, and machine translation.\n"
      ],
      "metadata": {
        "id": "93AcqKtAXLH1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "211c_OhM0OaK",
        "outputId": "a34775dc-617a-4964-d8e1-2df1cd0e5af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in the text corpus: 8\n",
            "['natural language processing nlp field artificial intelligence focus interaction computer human natural language', 'ultimate goal nlp enable computer understand interpret generate human language way meaningful useful', 'nlp technique used wide range application including machine translation speech recognition sentiment analysis chatbots information retrieval', 'involves various task tokenization part speech tagging named entity recognition syntactic parsing', 'nltk popular python library nlp providing tool resource task like text processing text classification language modeling', 'offer wide range function datasets help get started nlp project', 'sample text demonstrate basic nlp task using nltk tokenization part speech tagging', 'let get started']\n",
            "\n",
            "\n",
            "Bag of Words Model:\n",
            "array([[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
            "        1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
            "        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
            "        1, 0],\n",
            "       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
            "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1],\n",
            "       [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0]])\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries for text cleaning and processing\n",
        "import re                                  # Regular expressions library for text pattern matching and replacement\n",
        "import nltk                                # Natural Language Toolkit library for various NLP tasks\n",
        "from nltk.corpus import stopwords          # NLTK corpus containing common stop words\n",
        "from nltk.stem.porter import PorterStemmer # Stemming algorithm for word reduction\n",
        "from nltk.stem import WordNetLemmatizer    # Lemmatization tool for word normalization\n",
        "\n",
        "# Creating the stemming and lemmatization objects\n",
        "ps = PorterStemmer()\n",
        "wordnet = WordNetLemmatizer()\n",
        "\n",
        "# we will convert this whole text-corpus into the bags of words\n",
        "text = \"\"\"\n",
        "                Natural Language Processing (NLP) is a field of artificial intelligence that focuses\n",
        "                on the interaction between computers and humans through natural language. The ultimate\n",
        "                goal of NLP is to enable computers to understand, interpret, and generate human language\n",
        "                in a way that is both meaningful and useful. NLP techniques are used in a wide range of\n",
        "                applications, including machine translation, speech recognition, sentiment analysis,\n",
        "                chatbots, and information retrieval. It involves various tasks such as tokenization,\n",
        "                part-of-speech tagging, named entity recognition, and syntactic parsing. NLTK is a popular\n",
        "                Python library for NLP, providing tools and resources for tasks like text processing, text\n",
        "                classification, and language modeling. It offers a wide range of functions and datasets\n",
        "                to help you get started with NLP projects. In this sample text, we'll demonstrate some\n",
        "                basic NLP tasks using NLTK, such as tokenization and part-of-speech tagging.\n",
        "                Let's get started!\n",
        "             \"\"\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(\"Number of sentences in the text corpus:\", len(sentences))\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    cleaned_sentence = re.sub('[^a-zA-Z]', ' ', sentences[i]) # Removes all characters from sentences[i] that are not alphabetic characters and replaces them with spaces.\n",
        "    cleaned_sentence = cleaned_sentence.lower()\n",
        "    cleaned_sentence = cleaned_sentence.split()\n",
        "    cleaned_sentence = [wordnet.lemmatize(word) for word in cleaned_sentence if not word in set(stopwords.words('english'))]\n",
        "    cleaned_sentence = ' '.join(cleaned_sentence)\n",
        "    corpus.append(cleaned_sentence)\n",
        "\n",
        "print(corpus)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Creating the Bag of Words model\n",
        "# CountVectorizer is used for text preprocessing and creating the Bag of Words model.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# This parameter controls the maximum number of words (features) to include in the BoW model.\n",
        "# It means that only the top 1500 most frequent words in your corpus will be considered as features in the BoW model.\n",
        "vectorizer = CountVectorizer(max_features=1500)\n",
        "\n",
        "# Converting the corpus into the the Bag of Words matrix then to a dense Numpy array\n",
        "X = vectorizer.fit_transform(corpus).toarray()\n",
        "\n",
        "print(\"Bag of Words Model:\")\n",
        "pprint(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the CountVectorizer class from scikit-learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define a list of text documents (corpus)\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "# Create a CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Transform the text data into a document-term matrix (DTM)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get the feature (word) names from the CountVectorizer\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the DTM as an array\n",
        "pprint(X.toarray())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Create a new CountVectorizer object with specific settings\n",
        "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "\n",
        "# Transform the text data into a new DTM with bigram (two-word) features\n",
        "X2 = vectorizer2.fit_transform(corpus)\n",
        "\n",
        "# Get the feature (bigram) names from the new CountVectorizer\n",
        "feature_names2 = vectorizer2.get_feature_names_out()\n",
        "\n",
        "# Print the new DTM as an array\n",
        "pprint(X2.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ9dfFWmPOjm",
        "outputId": "ac703965-8479-46a6-ec1a-638ab09280c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
            "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
            "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
            "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])\n",
            "\n",
            "\n",
            "array([[0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
            "       [0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
            "       [1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0],\n",
            "       [0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of scikit-learn's `CountVectorizer`, `max_features`, `analyzer`, and `ngram_range` these are parameters that allow you to customize **how text data is converted into a numerical format suitable for machine learning models**.\n",
        "\n",
        "1. **`max_features`**:\n",
        "   - `max_features` is an integer parameter that limits the maximum number of features (words or n-grams) to be extracted from the text data.\n",
        "   - It's often used to reduce the dimensionality of the feature space when dealing with high-dimensional text data, which can help improve model training speed and potentially reduce overfitting.\n",
        "   - You can set it to a specific number, and the vectorizer will select the top `max_features` most frequent words or n-grams based on their document frequency.\n",
        "   - For example, if you set `max_features=1000`, the vectorizer will only consider the top 1000 most frequent words or n-grams.\n",
        "\n",
        "2. **`analyzer`**:\n",
        "   - `analyzer` determines whether the vectorizer should treat the input text as words or as character n-grams.\n",
        "   - It can take one of the following values:\n",
        "     - `'word'`: The vectorizer treats each word as a feature. This is the default setting and is appropriate for most natural language processing tasks.\n",
        "     - `'char'`: The vectorizer treats each character as a feature. This can be useful for tasks where character-level information is important, such as text classification with short text snippets.\n",
        "     - You can also define your custom analyzer function if needed.\n",
        "\n",
        "3. **`ngram_range`**:\n",
        "\n",
        "An **`n-gram`** is like a puzzle piece made of words. It's a way to break down a sentence of text into smaller chunks for analysis. The \"n\" in n-gram represents the number of words in each chunk.\n",
        "\n",
        "For example:\n",
        "- A **`uni-gram`** (or 1-gram) looks at one word at a time, like \"`cat`\" or \"`dog`.\"\n",
        "- A **`bi-gram`** (or 2-gram) looks at two words at a time, like \"`red car`\" or \"`big house`.\"\n",
        "- A **`tri-gram`** (or 3-gram) looks at three words at a time, like \"`jumped over it`\" or \"`apple pie recipe`.\"\n",
        "\n",
        "N-grams help us understand the context of words in a sentence and are commonly used in natural language processing and text analysis to find patterns, like predicting the next word in a sentence or identifying commonly used phrases.\n",
        "\n",
        "   - `ngram_range` is a tuple specifying the range of n-grams to consider during vectorization.\n",
        "   - An n-gram is a contiguous sequence of n items from a given sample of text. In the context of `CountVectorizer`, these items can be words or characters, depending on the `analyzer` setting.\n",
        "   - The `ngram_range` parameter takes a tuple of two integers: `(min_n, max_n)`. It specifies the minimum and maximum n-gram sizes to include.\n",
        "   - For example, if you set `ngram_range=(1, 2)`, the vectorizer will include both single words and word pairs (bigrams) as features.\n",
        "   - If you set `ngram_range=(1, 1)`, only single words will be included (no n-grams).\n",
        "\n",
        "Here's an example of how you might use these parameters in scikit-learn's `CountVectorizer`:\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize the vectorizer with specified parameters\n",
        "vectorizer = CountVectorizer(max_features=1000, analyzer='word', ngram_range=(1, 2))\n",
        "\n",
        "# Fit and transform the text data\n",
        "X = vectorizer.fit_transform(text_data)\n"
      ],
      "metadata": {
        "id": "0wdhSSMlWpH9"
      }
    }
  ]
}