{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<strong>\n",
        "    <h1 align='center'><strong>Tokenization</strong></h1>\n",
        "</strong>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Jo-w_SFd34l8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "404NXQCY5Sb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aaaimport nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download a specific NLTK dataset, e.g., the 'punkt' tokenizer models.\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Download the NLTK stopwords dataset, which contains common stopwords for various languages.\n",
        "# nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Download the NLTK averaged perceptron tagger, which is used for part-of-speech tagging.\n",
        "# nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "# Download the WordNet lexical database, which is used for various NLP tasks like synonym and antonym lookup.\n",
        "# nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# Download the NLTK names dataset, which contains a list of common first names and last names.\n",
        "# nltk.download('names', quiet=True)\n",
        "\n",
        "# Download the NLTK movie_reviews dataset, which contains movie reviews categorized as positive and negative.\n",
        "# nltk.download('movie_reviews', quiet=True)\n",
        "\n",
        "# Download the NLTK reuters dataset, which is a collection of news documents categorized into topics.\n",
        "# nltk.download('reuters', quiet=True)\n",
        "\n",
        "# Download the NLTK brown corpus, which is a collection of text from various genres of written American English.\n",
        "# nltk.download('brown', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kyvhhDZ4bkg",
        "outputId": "f90bd1be-e667-48d1-96d0-ecc394f218a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"Hello, world! This is an Sample Text, to test some fucntions in #NLP\""
      ],
      "metadata": {
        "id": "PD4zttCa8PGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Tokenize using sent_tokenize (splits text into sentences)\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "for sentence in sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AO3iGd08I8u",
        "outputId": "d207c330-f3e3-4674-a036-2c4b14e710bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, world!\n",
            "This is an Sample Text, to test some fucntions in #NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kuDtsyo3efJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6afb68-9ef4-41fa-ece7-7ae01aa58c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '!', 'This', 'is', 'an', 'Sample', 'Text', ',', 'to', 'test', 'some', 'fucntions', 'in', '#', 'NLP']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize using word_tokenize (splits text into words)\n",
        "tokens_word_tokenize = word_tokenize(text)\n",
        "\n",
        "print(tokens_word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize using wordpunct_tokenize (splits text into words and punctuation)\n",
        "tokens_wordpunct_tokenize = nltk.wordpunct_tokenize(text\n",
        "                                                    )\n",
        "print(tokens_wordpunct_tokenize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ6amtbs8MrO",
        "outputId": "9e428d5d-9afe-4498-9a22-f125236689ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '!', 'This', 'is', 'an', 'Sample', 'Text', ',', 'to', 'test', 'some', 'fucntions', 'in', '#', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Both `word_tokenize` and `wordpunct_tokenize` tokenize text into words and treat punctuation marks as separate tokens. The primary difference between them is in the level of detail at which they tokenize punctuation. If you need fine-grained control over punctuation tokens, you can use wordpunct_tokenize. However, for many natural language processing tasks, either function can be used interchangeably based on your specific requirements."
      ],
      "metadata": {
        "id": "bbi_fGeo-HDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Tokenize using regexp_tokenize (splits text based on a regular expression)\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "pattern = r'\\w+'\n",
        "tokens_regexp_tokenize = regexp_tokenize(text, pattern)\n",
        "\n",
        "print(tokens_regexp_tokenize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9sMV1KS8Ksx",
        "outputId": "c1277203-db47-46ed-b56d-5603a1b4d07c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'world', 'This', 'is', 'an', 'Sample', 'Text', 'to', 'test', 'some', 'fucntions', 'in', 'NLP']\n"
          ]
        }
      ]
    }
  ]
}