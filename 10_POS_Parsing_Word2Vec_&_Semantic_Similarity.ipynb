{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Linguistic Features: NLP with spaCy</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(POS Tagging, Dependency Parsing, Word Vectors, and Semantic Similarity)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "1. [POS Tagging](#section-1)\n",
        "2. [Word-Sense Disambiguation](#section-2)\n",
        "3. [Dependency Parsing](#section-3)\n",
        "4. [Introduction to Word Vectors](#section-4)\n",
        "5. [Word Vectors in spaCy](#section-5)\n",
        "6. [Visualizing Word Vectors](#section-6)\n",
        "7. [Measuring Semantic Similarity](#section-7)\n",
        "8. [Conclusion](#section-8)\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. POS Tagging</span><br>\n",
        "\n",
        "### Understanding Part-of-Speech (POS)\n",
        "POS tagging is the process of assigning grammatical categories (such as nouns, verbs, adjectives) to words in a text. Crucially, **POS tags depend on the context**, meaning the surrounding words and their tags influence how a specific word is categorized.\n",
        "\n",
        "In spaCy, we can access these tags using the `.pos_` attribute of a token.\n",
        "\n",
        "#### Code Example: Contextual Tagging\n",
        "In the example below, the word \"fish\" appears twice. Once as a verb (\"will fish\") and once as a noun (\"a fish\").\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('My', 'PRON', 'pronoun'), ('cat', 'NOUN', 'noun'), ('will', 'AUX', 'auxiliary'), ('fish', 'VERB', 'verb'), ('for', 'ADP', 'adposition'), ('a', 'DET', 'determiner'), ('fish', 'NOUN', 'noun'), ('tomorrrow', 'NOUN', 'noun'), ('in', 'ADP', 'adposition'), ('a', 'DET', 'determiner'), ('fishy', 'ADJ', 'adjective'), ('way', 'NOUN', 'noun'), ('.', 'PUNCT', 'punctuation')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# A sentence where 'fish' is used as a verb and a noun\n",
        "text = \"My cat will fish for a fish tomorrrow in a fishy way.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate over tokens and print text, POS tag, and explanation\n",
        "print([(token.text, token.pos_, spacy.explain(token.pos_)) for token in doc])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Expected Output Analysis:**\n",
        "- First 'fish': Tagged as **VERB**.\n",
        "- Second 'fish': Tagged as **NOUN**.\n",
        "- 'fishy': Tagged as **ADJ** (Adjective).\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> Use <code>spacy.explain(tag)</code> to get a human-readable description of any tag (e.g., \"ADP\" becomes \"adposition\"). </div>\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. Word-Sense Disambiguation</span><br>\n",
        "\n",
        "### Importance of POS\n",
        "POS tagging improves accuracy for many NLP tasks, such as translation systems. For example, translating \"fish\" requires knowing if it is an action (verb) or an object (noun).\n",
        "\n",
        "*   **Verb** -> *pescarÃ©* (Spanish for \"I will fish\")\n",
        "*   **Noun** -> *pescado* (Spanish for \"fish\" as food)\n",
        "\n",
        "### Word-Sense Disambiguation (WSD)\n",
        "WSD is the problem of deciding in which **sense** a word is used in a sentence.\n",
        "\n",
        "| Word | POS tag | Description |\n",
        "| :--- | :--- | :--- |\n",
        "| Play | VERB | engage in activity for enjoyment and recreation |\n",
        "| Play | NOUN | a dramatic work for the stage or to be broadcast |\n",
        "\n",
        "#### Code Example: Disambiguating \"Fish\"\n",
        "We can filter tokens based on their text and observe their assigned POS tags to understand their usage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verb Context:\n",
            "[('fish', 'VERB')]\n",
            "\n",
            "Noun Context:\n",
            "[('fish', 'NOUN')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Two sentences using 'fish' differently\n",
        "verb_text = \"I will fish tomorrow.\"\n",
        "noun_text = \"I ate fish.\"\n",
        "\n",
        "# Extract 'fish' from the verb sentence\n",
        "print(\"Verb Context:\")\n",
        "print([(token.text, token.pos_) for token in nlp(verb_text) if \"fish\" in token.text])\n",
        "\n",
        "print(\"\\nNoun Context:\")\n",
        "# Extract 'fish' from the noun sentence\n",
        "print([(token.text, token.pos_) for token in nlp(noun_text) if \"fish\" in token.text])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. Dependency Parsing</span><br>\n",
        "\n",
        "### Exploring Sentence Syntax\n",
        "Dependency parsing explores the syntax of a sentence by identifying links (dependencies) between tokens. The result is often represented as a tree structure.\n",
        "\n",
        "A **Dependency Label** describes the type of syntactic relation between two tokens.\n",
        "\n",
        "#### Common Dependency Labels\n",
        "\n",
        "| Dependency label | Description |\n",
        "| :--- | :--- |\n",
        "| **nsubj** | Nominal subject |\n",
        "| **root** | Root (main verb/action) |\n",
        "| **det** | Determiner |\n",
        "| **dobj** | Direct object |\n",
        "| **aux** | Auxiliary |\n",
        "\n",
        "### Using `displaCy` for Visualization\n",
        "spaCy provides a built-in visualizer called `displaCy` to draw dependency trees.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"We understand the differences.\")\n",
        "\n",
        "# In a Jupyter environment, use render. In a script, use serve.\n",
        "# displacy.render(doc, style=\"dep\", jupyter=True) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Accessing Dependency Labels\n",
        "You can access the dependency label using the `.dep_` attribute of a token.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('We', 'nsubj', 'nominal subject'), ('understand', 'ROOT', 'root'), ('the', 'det', 'determiner'), ('differences', 'dobj', 'direct object'), ('.', 'punct', 'punctuation')]\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"We understand the differences.\")\n",
        "\n",
        "# Print token text, dependency label, and explanation\n",
        "print([(token.text, token.dep_, spacy.explain(token.dep_)) for token in doc])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Output Breakdown:**\n",
        "1.  **We**: `nsubj` (nominal subject)\n",
        "2.  **understand**: `ROOT` (root)\n",
        "3.  **the**: `det` (determiner)\n",
        "4.  **differences**: `dobj` (direct object)\n",
        "5.  **.**: `punct` (punctuation)\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Introduction to Word Vectors</span><br>\n",
        "\n",
        "### What are Word Vectors (Embeddings)?\n",
        "Word vectors are numerical representations of words. Unlike older methods, they capture semantic meaning.\n",
        "\n",
        "#### The \"Bag of Words\" Problem\n",
        "Older methods (like Bag of Words) count frequencies but do not understand meaning or context.\n",
        "\n",
        "| Sentences | I | got | covid | coronavirus |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| I got covid | 1 | 2 | 3 | 0 |\n",
        "| I got coronavirus | 1 | 2 | 0 | 4 |\n",
        "\n",
        "In the table above, \"covid\" and \"coronavirus\" are treated as completely different entities (columns 3 and 4), even though they mean the same thing in this context.\n",
        "\n",
        "#### The Vector Solution\n",
        "Word vectors use a pre-defined number of dimensions to represent words. They consider word frequencies and the presence of other words in **similar contexts**. This allows the model to understand that \"cat\" and \"kitten\" are related because they appear in similar sentence structures.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Word Vectors in spaCy</span><br>\n",
        "\n",
        "### Loading Vectors\n",
        "To use word vectors in spaCy, you typically need a medium (`md`) or large (`lg`) model. The small model (`sm`) does not contain static word vectors.\n",
        "\n",
        "*   **Model**: `en_core_web_md`\n",
        "*   **Specs**: 300-dimensional vectors for 20,000 words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n",
            "{'width': 300, 'vectors': 20000, 'keys': 684830, 'name': 'en_vectors', 'mode': 'default'}\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Ensure you have downloaded the model: python -m spacy download en_core_web_md\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_md\")\n",
        "    print(\"Model loaded successfully.\")\n",
        "    print(nlp.meta[\"vectors\"])\n",
        "except OSError:\n",
        "    print(\"Please download the model: python -m spacy download en_core_web_md\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Accessing Vector Data\n",
        "1.  **`nlp.vocab.strings`**: Access word IDs.\n",
        "2.  **`nlp.vocab.vectors`**: Access the actual vector arrays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID for 'like': 18194338103975822726\n",
            "Vector shape: (300,)\n",
            "First 5 dimensions: [-0.61052   0.11656  -0.50648  -0.32216  -0.099742]\n"
          ]
        }
      ],
      "source": [
        "# Get the ID for the word \"like\"\n",
        "like_id = nlp.vocab.strings[\"like\"]\n",
        "print(f\"ID for 'like': {like_id}\")\n",
        "\n",
        "# Get the vector using the ID\n",
        "vector_data = nlp.vocab.vectors[like_id]\n",
        "print(f\"Vector shape: {vector_data.shape}\")\n",
        "print(f\"First 5 dimensions: {vector_data[:5]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Finding Similar Words\n",
        "spaCy can find semantically similar terms to a given term by comparing their vectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'[E058] Could not retrieve vector for key 2127825066894192516.'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m word = \u001b[33m\"\u001b[39m\u001b[33mcovid\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Find the 5 most similar words to \"covid\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Note: This uses spaCy's internal vector table\u001b[39;00m\n\u001b[32m      7\u001b[39m most_similar_words = nlp.vocab.vectors.most_similar(\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     np.asarray([\u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m]), n=\u001b[32m5\u001b[39m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Decode the IDs back to strings\u001b[39;00m\n\u001b[32m     12\u001b[39m words = [nlp.vocab.strings[w] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m most_similar_words[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]]\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\001_Github_Repo_all\\Natural_Language_Processing_in_Python\\.venv\\Lib\\site-packages\\spacy\\vectors.pyx:274\u001b[39m, in \u001b[36mspacy.vectors.Vectors.__getitem__\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mKeyError\u001b[39m: '[E058] Could not retrieve vector for key 2127825066894192516.'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "word = \"covid\"\n",
        "\n",
        "# Find the 5 most similar words to \"covid\"\n",
        "# Note: This uses spaCy's internal vector table\n",
        "most_similar_words = nlp.vocab.vectors.most_similar(\n",
        "    np.asarray([nlp.vocab.vectors[nlp.vocab.strings[word]]]), n=5\n",
        ")\n",
        "\n",
        "# Decode the IDs back to strings\n",
        "words = [nlp.vocab.strings[w] for w in most_similar_words[0][0]]\n",
        "print(f\"Words similar to '{word}': {words}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. Visualizing Word Vectors</span><br>\n",
        "\n",
        "### Dimensionality Reduction\n",
        "Since word vectors are 300-dimensional, we cannot visualize them directly. We use **Principal Component Analysis (PCA)** to project these vectors into a 2-dimensional space.\n",
        "\n",
        "This visualization helps us see how words are grouped. For example, fruits should cluster together, and animals should cluster together.\n",
        "\n",
        "#### Code Example: PCA Visualization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "# Load model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# List of words to visualize\n",
        "words = [\"wonderful\", \"horrible\", \"apple\", \"banana\", \"orange\", \"watermelon\", \"dog\", \"cat\"]\n",
        "\n",
        "# Extract vectors and stack them vertically\n",
        "word_vectors = np.vstack([nlp.vocab.vectors[nlp.vocab.strings[w]] for w in words])\n",
        "\n",
        "# Extract two principal components using PCA\n",
        "pca = PCA(n_components=2)\n",
        "word_vectors_transformed = pca.fit_transform(word_vectors)\n",
        "\n",
        "# Visualize the scatter plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(word_vectors_transformed[:, 0], word_vectors_transformed[:, 1])\n",
        "\n",
        "# Add labels to the points\n",
        "for word, coord in zip(words, word_vectors_transformed):\n",
        "    x, y = coord\n",
        "    plt.text(x, y, word, size=12)\n",
        "\n",
        "plt.title(\"Word Vector Visualization (PCA)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Analogies\n",
        "Word embeddings can capture semantic relationships, allowing for vector arithmetic (analogies).\n",
        "*   **Formula**: `Queen - Woman + Man = King`\n",
        "*   This implies the vector direction from Woman to Queen is similar to Man to King.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 7. Measuring Semantic Similarity</span><br>\n",
        "\n",
        "### The Similarity Score\n",
        "Semantic similarity is the process of analyzing texts to identify how similar they are.\n",
        "*   **Metric**: Cosine Similarity.\n",
        "*   **Range**: 0 to 1 (0 = completely different, 1 = identical).\n",
        "\n",
        "spaCy allows you to calculate similarity between **Tokens**, **Spans**, and **Documents**.\n",
        "\n",
        "#### 1. Token Similarity\n",
        "Comparing individual words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc1 = nlp(\"We eat pizza\")\n",
        "doc2 = nlp(\"We like to eat pasta\")\n",
        "\n",
        "token1 = doc1[2] # pizza\n",
        "token2 = doc2[4] # pasta\n",
        "\n",
        "print(f\"Similarity between {token1} and {token2} = \", round(token1.similarity(token2), 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### 2. Span Similarity\n",
        "Comparing phrases or slices of a document.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "span1 = doc1[1:] # \"eat pizza\"\n",
        "span2 = doc2[1:] # \"like to eat pasta\"\n",
        "\n",
        "print(f\"Similarity between '{span1}' and '{span2}' = \", round(span1.similarity(span2), 3))\n",
        "\n",
        "# Comparing \"eat pizza\" vs \"eat pasta\"\n",
        "print(f\"Similarity between '{doc1[1:]}' and '{doc2[3:]}' = \", round(doc1[1:].similarity(doc2[3:]), 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### 3. Doc Similarity\n",
        "Comparing full sentences or documents. Doc vectors default to an average of the word vectors contained within them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc1 = nlp(\"I like to play basketball\")\n",
        "doc2 = nlp(\"I love to play basketball\")\n",
        "\n",
        "print(\"Similarity score :\", round(doc1.similarity(doc2), 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### 4. Sentence Similarity (Finding Relevant Content)\n",
        "We can use similarity scores to find the most relevant sentence in a text given a keyword.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A document with multiple sentences\n",
        "sentences = nlp(\"What is the cheapest flight from Boston to Seattle? \"\n",
        "                \"Which airline serves Denver, Pittsburgh and Atlanta? \"\n",
        "                \"What kinds of planes are used by American Airlines?\")\n",
        "\n",
        "keyword = nlp(\"price\")\n",
        "\n",
        "# Iterate through sentences and check similarity to \"price\"\n",
        "for i, sentence in enumerate(sentences.sents):\n",
        "    print(f\"Similarity score with sentence {i+1}: \", round(sentence.similarity(keyword), 5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Analysis**: The first sentence (\"cheapest flight\") should have the highest similarity score to \"price\" because \"cheapest\" and \"price\" are semantically related in the vector space.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 8. Conclusion</span><br>\n",
        "\n",
        "In this notebook, we explored the powerful linguistic features provided by spaCy:\n",
        "\n",
        "1.  **POS Tagging**: We learned how to identify grammatical roles (Noun, Verb, etc.) and how context changes these tags (e.g., \"fish\" as a verb vs. noun). This is crucial for Word-Sense Disambiguation.\n",
        "2.  **Dependency Parsing**: We visualized the syntactic structure of sentences using `displaCy` and accessed dependency labels (`nsubj`, `dobj`) to understand relationships between words.\n",
        "3.  **Word Vectors**: We moved beyond simple frequency counts (Bag of Words) to 300-dimensional embeddings that capture semantic meaning.\n",
        "4.  **Semantic Similarity**: Using word vectors, we calculated how similar tokens, spans, and documents are to one another using Cosine Similarity. This allows for advanced applications like search relevance and recommendation systems.\n",
        "\n",
        "**Next Steps:**\n",
        "*   Experiment with the `en_core_web_lg` model for potentially higher accuracy.\n",
        "*   Apply dependency parsing to extract specific information (e.g., \"Who did what?\").\n",
        "*   Use semantic similarity to build a simple FAQ bot that matches user queries to existing questions.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
