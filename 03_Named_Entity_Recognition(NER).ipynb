{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Named Entity Recognition</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Introduction to Natural Language Processing in Python)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [What is Named Entity Recognition?](#section-1)\n",
        "2. [NLTK and the Stanford CoreNLP Library](#section-2)\n",
        "3. [Introduction to SpaCy](#section-3)\n",
        "4. [Multilingual NER with Polyglot](#section-4)\n",
        "5. [Conclusion](#section-5)\n",
        "\n",
        "***\n",
        "\n",
        "<a id=\"section-1\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. What is Named Entity Recognition?</span><br>\n",
        "\n",
        "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP). Its primary goal is to identify and classify key information (entities) within unstructured text into predefined categories.\n",
        "\n",
        "### Key Concepts\n",
        "NER answers the questions: **Who? What? When? Where?**\n",
        "\n",
        "It involves identifying categories such as:\n",
        "*   **People**: Names of individuals (e.g., \"Ruth Reichl\", \"Einstein\").\n",
        "*   **Places**: Locations, cities, countries (e.g., \"New York\", \"Germany\").\n",
        "*   **Organizations**: Companies, institutions (e.g., \"MOMA\", \"Google\").\n",
        "*   **Dates & States**: Temporal expressions and geopolitical states.\n",
        "*   **Works of Art**: Books, movies, paintings.\n",
        "*   ...and many other categories!\n",
        "\n",
        "### Usage\n",
        "NER is versatile and can be deployed in various contexts:\n",
        "1.  **Alongside Topic Identification**: To understand not just *what* a text is about, but *who* is involved.\n",
        "2.  **On its own**: To extract structured data from unstructured documents (e.g., extracting dates and locations from news articles).\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> In visual examples (like those from Europeana Newspapers), NER systems often highlight text with color-coded tags such as <code>LOCATION</code>, <code>TIME</code>, <code>PERSON</code>, <code>ORGANIZATION</code>, <code>MONEY</code>, and <code>DATE</code>. </div>\n",
        "\n",
        "***\n",
        "\n",
        "<a id=\"section-2\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. NLTK and the Stanford CoreNLP Library</span><br>\n",
        "\n",
        "The Natural Language Toolkit (NLTK) is a classic library in Python for NLP. It integrates with the **Stanford CoreNLP** library to provide robust entity recognition capabilities.\n",
        "\n",
        "### The Stanford CoreNLP Library\n",
        "*   **Integration**: It is integrated into Python via `nltk`.\n",
        "*   **Architecture**: It is Java-based.\n",
        "*   **Capabilities**:\n",
        "    *   Named Entity Recognition (NER).\n",
        "    *   Coreference resolution.\n",
        "    *   Dependency tree parsing.\n",
        "\n",
        "### Using NLTK for NER\n",
        "To perform NER in NLTK, a standard pipeline is usually followed:\n",
        "1.  **Tokenize**: Split the sentence into words.\n",
        "2.  **POS Tag**: Assign Part-of-Speech tags (e.g., Noun, Verb) to each token.\n",
        "3.  **Chunk**: Use `ne_chunk` to identify named entities based on the tags.\n",
        "\n",
        "#### Step 1: Tokenization and POS Tagging\n",
        "\n",
        "**Original Code (from PDF):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('In', 'IN'), ('New', 'NNP'), ('York', 'NNP')]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "sentence = '''In New York, I like to ride the Metro to\n",
        "visit MOMA and some restaurants rated\n",
        "well by Ruth Reichl.'''\n",
        "\n",
        "tokenized_sent = nltk.word_tokenize(sentence)\n",
        "tagged_sent = nltk.pos_tag(tokenized_sent)\n",
        "tagged_sent[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Enhanced Executable Code:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('In', 'IN'), ('New', 'NNP'), ('York', 'NNP')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\mohdf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\mohdf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\mohdf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\mohdf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Ensure necessary NLTK models are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Define the sentence\n",
        "sentence = '''In New York, I like to ride the Metro to\n",
        "visit MOMA and some restaurants rated\n",
        "well by Ruth Reichl.'''\n",
        "\n",
        "# 1. Tokenize the sentence\n",
        "tokenized_sent = nltk.word_tokenize(sentence)\n",
        "\n",
        "# 2. Part-of-Speech (POS) Tagging\n",
        "tagged_sent = nltk.pos_tag(tokenized_sent)\n",
        "\n",
        "# Display the first 3 tagged tokens\n",
        "# Expected Output: [('In', 'IN'), ('New', 'NNP'), ('York', 'NNP')]\n",
        "print(tagged_sent[:3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Step 2: Named Entity Chunking\n",
        "Once the text is tagged, we use `nltk.ne_chunk` to classify the entities.\n",
        "\n",
        "**Original Code (from PDF):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  In/IN\n",
            "  (GPE New/NNP York/NNP)\n",
            "  ,/,\n",
            "  I/PRP\n",
            "  like/VBP\n",
            "  to/TO\n",
            "  ride/VB\n",
            "  the/DT\n",
            "  (ORGANIZATION Metro/NNP)\n",
            "  to/TO\n",
            "  visit/VB\n",
            "  (ORGANIZATION MOMA/NNP)\n",
            "  and/CC\n",
            "  some/DT\n",
            "  restaurants/NNS\n",
            "  rated/VBN\n",
            "  well/RB\n",
            "  by/IN\n",
            "  (PERSON Ruth/NNP Reichl/NNP)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "print(nltk.ne_chunk(tagged_sent))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Enhanced Executable Code:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  In/IN\n",
            "  (GPE New/NNP York/NNP)\n",
            "  ,/,\n",
            "  I/PRP\n",
            "  like/VBP\n",
            "  to/TO\n",
            "  ride/VB\n",
            "  the/DT\n",
            "  (ORGANIZATION Metro/NNP)\n",
            "  to/TO\n",
            "  visit/VB\n",
            "  (ORGANIZATION MOMA/NNP)\n",
            "  and/CC\n",
            "  some/DT\n",
            "  restaurants/NNS\n",
            "  rated/VBN\n",
            "  well/RB\n",
            "  by/IN\n",
            "  (PERSON Ruth/NNP Reichl/NNP)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "# 3. Perform Named Entity Chunks\n",
        "ne_tree = nltk.ne_chunk(tagged_sent)\n",
        "\n",
        "# Print the tree structure\n",
        "print(ne_tree)\n",
        "\n",
        "# Explanation of Output Tags:\n",
        "# GPE: Geo-Political Entity (e.g., New York)\n",
        "# ORGANIZATION: Organizations (e.g., Metro, MOMA)\n",
        "# PERSON: People (e.g., Ruth Reichl)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Expected Output Analysis:**\n",
        "The output is a tree structure where entities are grouped.\n",
        "*   `(GPE New/NNP York/NNP)` indicates \"New York\" is a Geo-Political Entity.\n",
        "*   `(ORGANIZATION Metro/NNP)` indicates \"Metro\" is an Organization.\n",
        "*   `(PERSON Ruth/NNP Reichl/NNP)` indicates a Person.\n",
        "\n",
        "***\n",
        "\n",
        "<a id=\"section-3\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. Introduction to SpaCy</span><br>\n",
        "\n",
        "SpaCy is a modern, industrial-strength NLP library. While similar to libraries like `gensim` in that it handles NLP tasks, its implementation and philosophy differ significantly.\n",
        "\n",
        "### What is SpaCy?\n",
        "*   **Pipeline Focus**: SpaCy focuses on creating NLP pipelines to generate models and corpora efficiently.\n",
        "*   **Open-source**: It includes extra libraries and tools.\n",
        "*   **Visualization**: Includes **Displacy**, a built-in entity recognition visualizer.\n",
        "\n",
        "### Why use SpaCy for NER?\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> SpaCy is particularly effective for informal language corpora, such as Tweets and chat messages. </div>\n",
        "\n",
        "1.  **Easy pipeline creation**: Streamlined API for processing text.\n",
        "2.  **Different entity types**: Offers a different set of entity labels compared to NLTK.\n",
        "3.  **Informal Language**: Robust performance on social media text.\n",
        "4.  **Quickly growing**: Active community and frequent updates.\n",
        "\n",
        "### Using SpaCy for NER\n",
        "\n",
        "**Original Code (from PDF):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "b7a3c440",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Berlin GPE\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp(\"\"\"Berlin is the capital of Germany;\n",
        "and the residence of Chancellor Angela Merkel.\"\"\")\n",
        "doc.ents\n",
        "print(doc.ents[0], doc.ents[0].label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Enhanced Executable Code:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entities found: (Berlin, Germany, Angela Merkel)\n",
            "First Entity: Berlin\n",
            "Label: GPE\n",
            "\n",
            "--- All Entities ---\n",
            "Berlin: GPE\n",
            "Germany: GPE\n",
            "Angela Merkel: PERSON\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# NOTE: You must download the model first in your terminal:\n",
        "# python -m spacy download en_core_web_sm\n",
        "\n",
        "# Load the pre-trained English model\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "except OSError:\n",
        "    print(\"Model not found. Please run: python -m spacy download en_core_web_sm\")\n",
        "    # Fallback for demonstration if model isn't installed in this specific env\n",
        "    # In a real notebook, the user must install the model.\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"\"\"Berlin is the capital of Germany; \n",
        "and the residence of Chancellor Angela Merkel.\"\"\")\n",
        "\n",
        "# Accessing Entities\n",
        "# doc.ents returns a tuple of named entities found in the document\n",
        "print(\"Entities found:\", doc.ents)\n",
        "\n",
        "# Inspecting the first entity\n",
        "# Expected: Berlin GPE\n",
        "if doc.ents:\n",
        "    print(f\"First Entity: {doc.ents[0]}\")\n",
        "    print(f\"Label: {doc.ents[0].label_}\")\n",
        "\n",
        "# Iterating through all entities to see their labels\n",
        "print(\"\\n--- All Entities ---\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text}: {ent.label_}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Output Explanation:**\n",
        "*   **Berlin**: `GPE` (Geo-Political Entity)\n",
        "*   **Germany**: `GPE`\n",
        "*   **Angela Merkel**: `PERSON`\n",
        "\n",
        "***\n",
        "\n",
        "<a id=\"section-4\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Multilingual NER with Polyglot</span><br>\n",
        "\n",
        "Polyglot is an NLP library that relies heavily on word vectors and is designed for multilingual applications.\n",
        "\n",
        "### What is Polyglot?\n",
        "*   **Word Vectors**: Uses word embeddings to understand context and meaning.\n",
        "*   **Language Support**: It has vectors for **more than 130 languages**.\n",
        "*   **Why use it?**: It is the go-to library when working with non-English text or when you need to support many different languages simultaneously.\n",
        "\n",
        "### Spanish NER Example\n",
        "The following example demonstrates extracting entities from a Spanish text about Carles Puigdemont and Manuela Carmena.\n",
        "\n",
        "**Original Code (from PDF):**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "617d310b",
      "metadata": {},
      "source": [
        "```python\n",
        "from polyglot.text import Text\n",
        "\n",
        "text = \"\"\"El presidente de la Generalitat de CataluÃ±a,\n",
        "Carles Puigdemont, ha afirmado hoy a la alcaldesa\n",
        "de Madrid, Manuela Carmena, que en su etapa de\n",
        "alcalde de Girona (de julio de 2011 a enero de 2016)\n",
        "hizo una gran promociÃ³n de Madrid.\"\"\"\n",
        "\n",
        "ptext = Text(text)\n",
        "ptext.entities\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "1720734d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entities found:\n",
            "Generalitat de CataluÃ±a: LOC\n",
            "Carles Puigdemont: PER\n",
            "Madrid: LOC\n",
            "Manuela Carmena: PER\n",
            "Girona: LOC\n",
            "Madrid: LOC\n"
          ]
        }
      ],
      "source": [
        "# Alternative to Polyglot: Using spaCy for Spanish NER\n",
        "# polyglot is not compatible with Python 3.13/Windows\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the Spanish model\n",
        "nlp_es = spacy.load('es_core_news_sm')\n",
        "\n",
        "text = \"\"\"El presidente de la Generalitat de CataluÃ±a,\n",
        "Carles Puigdemont, ha afirmado hoy a la alcaldesa\n",
        "de Madrid, Manuela Carmena, que en su etapa de\n",
        "alcalde de Girona (de julio de 2011 a enero de 2016)\n",
        "hizo una gran promociÃ³n de Madrid.\"\"\"\n",
        "\n",
        "doc = nlp_es(text)\n",
        "\n",
        "print(\"Entities found:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text}: {ent.label_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Enhanced Executable Code:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Polyglot is not installed or dependencies (libicu) are missing.\n",
            "This code block requires a specific environment setup.\n"
          ]
        }
      ],
      "source": [
        "# NOTE: Polyglot requires 'libicu' and specific model downloads.\n",
        "# Installation: pip install polyglot pyicu pycld2 Morfessor\n",
        "# Model download: polyglot download embeddings2.es ner2.es\n",
        "\n",
        "try:\n",
        "    from polyglot.text import Text\n",
        "    \n",
        "    # Spanish text input\n",
        "    text = \"\"\"El presidente de la Generalitat de CataluÃ±a,\n",
        "    Carles Puigdemont, ha afirmado hoy a la alcaldesa\n",
        "    de Madrid, Manuela Carmena, que en su etapa de\n",
        "    alcalde de Girona (de julio de 2011 a enero de 2016)\n",
        "    hizo una gran promociÃ³n de Madrid.\"\"\"\n",
        "\n",
        "    # Create Polyglot Text object\n",
        "    ptext = Text(text)\n",
        "\n",
        "    # Extract entities\n",
        "    print(\"Entities found:\")\n",
        "    for entity in ptext.entities:\n",
        "        print(entity)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Polyglot is not installed or dependencies (libicu) are missing.\")\n",
        "    print(\"This code block requires a specific environment setup.\")\n",
        "\n",
        "# Expected Output Structure based on PDF:\n",
        "# I-ORG(['Generalitat', 'de'])\n",
        "# I-LOC(['Generalitat', 'de', 'CataluÃ±a'])\n",
        "# I-PER(['Carles', 'Puigdemont'])\n",
        "# I-LOC(['Madrid'])\n",
        "# I-PER(['Manuela', 'Carmena'])\n",
        "# I-LOC(['Girona'])\n",
        "# I-LOC(['Madrid'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<a id=\"section-5\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Conclusion</span><br>\n",
        "\n",
        "This notebook covered the essentials of Named Entity Recognition (NER) in Python using three distinct libraries, each with its own strengths:\n",
        "\n",
        "1.  **NLTK**:\n",
        "    *   **Best for**: Education, understanding the underlying mechanics of NLP (tokenization -> tagging -> chunking).\n",
        "    *   **Pros**: Highly granular control, integrates with Stanford CoreNLP.\n",
        "    *   **Cons**: Can be slower and more verbose than modern alternatives.\n",
        "\n",
        "2.  **SpaCy**:\n",
        "    *   **Best for**: Production environments, building efficient pipelines.\n",
        "    *   **Pros**: Fast, easy to use, excellent visualization (Displacy), supports informal language.\n",
        "    *   **Cons**: Less transparent than NLTK regarding internal model decisions.\n",
        "\n",
        "3.  **Polyglot**:\n",
        "    *   **Best for**: Multilingual applications.\n",
        "    *   **Pros**: Massive language support (130+), relies on word vectors.\n",
        "    *   **Cons**: Installation can be complex due to system dependencies (`libicu`).\n",
        "\n",
        "### Next Steps\n",
        "*   **Practice**: Try running the SpaCy code on your own social media data (e.g., a tweet history).\n",
        "*   **Explore**: Use the `Displacy` visualizer to see how the model parses complex sentences.\n",
        "*   **Expand**: Attempt to train a custom NER model if the pre-trained models do not recognize specific entities relevant to your domain (e.g., medical drugs or specific product names).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
