{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Feature Engineering for NLP in Python</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Tokenization, Lemmatization, Cleaning, POS Tagging, and NER)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Tokenization and Lemmatization](#section-1)\n",
        "2. [Text Cleaning](#section-2)\n",
        "3. [Part-of-Speech (POS) Tagging](#section-3)\n",
        "4. [Named Entity Recognition (NER)](#section-4)\n",
        "5. [Conclusion](#section-5)\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. Tokenization and Lemmatization</span><br>\n",
        "\n",
        "### 1.1 Introduction to Text Data\n",
        "Natural Language Processing (NLP) involves processing and analyzing large amounts of natural language data. Before we can feed text into machine learning models, we must perform **Feature Engineering**.\n",
        "\n",
        "Common sources of text data include:\n",
        "*   News articles\n",
        "*   Tweets\n",
        "*   Comments\n",
        "*   Reviews\n",
        "\n",
        "### 1.2 Making Text Machine Friendly\n",
        "Raw text is often messy and inconsistent. To a machine, \"Dogs\" and \"dog\" are completely different strings, even though they represent the same concept. Similarly, \"won't\" and \"will not\" mean the same thing but look different.\n",
        "\n",
        "**Common inconsistencies:**\n",
        "*   **Case sensitivity:** `Dogs`, `dog`\n",
        "*   **Word forms:** `reduction`, `REDUCING`, `Reduce`\n",
        "*   **Contractions:** `don't` vs `do not`, `won't` vs `will not`\n",
        "\n",
        "### 1.3 Text Preprocessing Techniques\n",
        "To standardize text, we apply several preprocessing techniques:\n",
        "1.  Converting words into lowercase.\n",
        "2.  Removing leading and trailing whitespaces.\n",
        "3.  Removing punctuation.\n",
        "4.  Removing stopwords.\n",
        "5.  Expanding contractions.\n",
        "6.  Removing special characters (numbers, emojis, etc.).\n",
        "\n",
        "### 1.4 Tokenization\n",
        "Tokenization is the process of splitting a string into its constituent parts, called **tokens**. These tokens can be words, punctuation marks, or numbers.\n",
        "\n",
        "**Example 1:**\n",
        "*   Input: `\"I have a dog. His name is Hachi.\"`\n",
        "*   Tokens: `[\"I\", \"have\", \"a\", \"dog\", \".\", \"His\", \"name\", \"is\", \"Hachi\", \".\"]`\n",
        "\n",
        "**Example 2:**\n",
        "*   Input: `\"Don't do this.\"`\n",
        "*   Tokens: `[\"Do\", \"n't\", \"do\", \"this\", \".\"]`\n",
        "\n",
        "#### Tokenization using spaCy\n",
        "We will use the `spaCy` library for these tasks. First, we load the English model (`en_core_web_sm`), create a document object, and iterate over it to extract tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install spaCy and download the model if not already installed\n",
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Initialize string\n",
        "string = \"Hello! I don't know what I'm doing here.\"\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(string)\n",
        "\n",
        "# Generate list of tokens\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 1.5 Lemmatization\n",
        "Lemmatization is the process of converting a word into its **base form** (lemma). This helps in reducing the vocabulary size and grouping similar words together.\n",
        "\n",
        "**Examples:**\n",
        "*   `reducing`, `reduces`, `reduced`, `reduction` $\\rightarrow$ `reduce`\n",
        "*   `am`, `are`, `is` $\\rightarrow$ `be`\n",
        "*   `n't` $\\rightarrow$ `not`\n",
        "*   `'ve` $\\rightarrow$ `have`\n",
        "\n",
        "#### Lemmatization using spaCy\n",
        "In spaCy, the lemma of a token is accessed using the `token.lemma_` attribute. Note that spaCy handles pronouns specifically (often denoted as `-PRON-` in older versions, though newer versions may return the pronoun itself).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Initialize string\n",
        "string = \"Hello! I don't know what I'm doing here.\"\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(string)\n",
        "\n",
        "# Generate list of lemmas\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(lemmas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. Text Cleaning</span><br>\n",
        "\n",
        "### 2.1 Text Cleaning Techniques\n",
        "Text cleaning goes beyond simple tokenization. It involves removing noise that might confuse a machine learning model.\n",
        "\n",
        "**Key Techniques:**\n",
        "*   Removing unnecessary whitespaces and escape sequences (e.g., `\\n`, `\\t`).\n",
        "*   Removing punctuation.\n",
        "*   Removing special characters (numbers, emojis).\n",
        "*   Removing stopwords.\n",
        "\n",
        "### 2.2 The `isalpha()` Method\n",
        "Python's string method `.isalpha()` is useful for identifying if a string consists only of alphabetical characters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examples of isalpha()\n",
        "print(f\"'Dog'.isalpha(): {'Dog'.isalpha()}\")       # True\n",
        "print(f\"'3dogs'.isalpha(): {'3dogs'.isalpha()}\")   # False\n",
        "print(f\"'12347'.isalpha(): {'12347'.isalpha()}\")   # False\n",
        "print(f\"'!'.isalpha(): {'!'.isalpha()}\")           # False\n",
        "print(f\"'?'.isalpha(): {'?'.isalpha()}\")           # False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip: A Word of Caution</b> <br>\n",
        "Be careful when strictly removing non-alphabetic characters.\n",
        "<ul>\n",
        "    <li><b>Abbreviations:</b> U.S.A, U.K (dots make them non-alpha).</li>\n",
        "    <li><b>Proper Nouns:</b> word2vec, xto10x (contain numbers).</li>\n",
        "</ul>\n",
        "For nuanced cases, you should write custom functions using <b>regex</b>.\n",
        "</div>\n",
        "\n",
        "### 2.3 Removing Non-Alphabetic Characters\n",
        "Let's clean a messy string containing punctuation, escape characters, and numbers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "string = \"\"\"\n",
        "OMG!!!! This is like    the best thing ever \\t\\n.\n",
        "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
        "\"\"\"\n",
        "\n",
        "# Load model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(string)\n",
        "\n",
        "# Generate list of lemmas\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Remove tokens that are not alphabetic\n",
        "# Note: We keep '-PRON-' because older spaCy models use this for pronouns. \n",
        "# Even if your model returns the actual pronoun, isalpha() usually handles it well.\n",
        "a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() or lemma == '-PRON-']\n",
        "\n",
        "# Print string after text cleaning\n",
        "print(' '.join(a_lemmas))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 2.4 Stopwords\n",
        "Stopwords are words that occur extremely commonly in a language but often carry little specific meaning for classification tasks (e.g., articles, be-verbs, pronouns).\n",
        "*   Examples: *the, is, at, which, on*.\n",
        "\n",
        "#### Removing Stopwords using spaCy\n",
        "spaCy provides a built-in list of stopwords.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Get list of stopwords\n",
        "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "string = \"\"\"\n",
        "OMG!!!! This is like    the best thing ever \\t\\n.\n",
        "Wow, such an amazing song! I'm hooked. Top 5 definitely. ?\n",
        "\"\"\"\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(string)\n",
        "\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Remove stopwords and non-alphabetic tokens\n",
        "# We filter out lemmas that are NOT in the stopwords list\n",
        "a_lemmas = [lemma for lemma in lemmas \n",
        "            if lemma.isalpha() and lemma.lower() not in stopwords]\n",
        "\n",
        "# Print string after text cleaning\n",
        "print(' '.join(a_lemmas))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 2.5 Other Preprocessing Techniques\n",
        "Depending on your data source, you might need:\n",
        "*   **Removing HTML/XML tags**: Essential for web scraping.\n",
        "*   **Replacing accented characters**: Converting `Ã©` to `e`.\n",
        "*   **Correcting spelling errors**: Using libraries like `TextBlob` or `pyspellchecker`.\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> Always use only those text preprocessing techniques that are relevant to your specific application. Over-cleaning can lead to loss of information. </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. Part-of-Speech (POS) Tagging</span><br>\n",
        "\n",
        "### 3.1 What is POS Tagging?\n",
        "POS Tagging is the process of assigning every word in a text its corresponding part of speech (noun, verb, adjective, etc.).\n",
        "\n",
        "**Example:**\n",
        "Input: `\"Jane is an amazing guitarist.\"`\n",
        "\n",
        "*   **Jane** $\\rightarrow$ proper noun\n",
        "*   **is** $\\rightarrow$ verb\n",
        "*   **an** $\\rightarrow$ determiner\n",
        "*   **amazing** $\\rightarrow$ adjective\n",
        "*   **guitarist** $\\rightarrow$ noun\n",
        "\n",
        "### 3.2 Applications\n",
        "*   **Word-sense disambiguation**:\n",
        "    *   \"The **bear** is a majestic animal\" (Noun)\n",
        "    *   \"Please **bear** with me\" (Verb)\n",
        "*   **Sentiment analysis**: Adjectives often carry sentiment.\n",
        "*   **Question answering**.\n",
        "*   **Fake news and opinion spam detection**.\n",
        "\n",
        "### 3.3 POS Tagging using spaCy\n",
        "spaCy makes POS tagging easy. The tag is available via the `token.pos_` attribute.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Initialize string\n",
        "string = \"Jane is an amazing guitarist\"\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(string)\n",
        "\n",
        "# Generate list of tokens and pos tags\n",
        "pos = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "print(pos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 3.4 POS Annotations in spaCy\n",
        "Below is a table of common POS tags used in spaCy (based on the Universal Dependencies scheme).\n",
        "\n",
        "| POS | Description | Examples |\n",
        "| :--- | :--- | :--- |\n",
        "| **ADJ** | adjective | big, old, green, incomprehensible, first |\n",
        "| **ADP** | adposition | in, to, during |\n",
        "| **ADV** | adverb | very, tomorrow, down, where, there |\n",
        "| **AUX** | auxiliary | is, has (done), will (do), should (do) |\n",
        "| **CONJ** | conjunction | and, or, but |\n",
        "| **CCONJ** | coordinating conjunction | and, or, but |\n",
        "| **DET** | determiner | a, an, the |\n",
        "| **PROPN** | proper noun | Jane, London, Google |\n",
        "| **NOUN** | noun | guitarist, dog, table |\n",
        "| **VERB** | verb | run, eat, play |\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Named Entity Recognition (NER)</span><br>\n",
        "\n",
        "### 4.1 What is NER?\n",
        "Named Entity Recognition (NER) involves identifying and classifying named entities in text into predefined categories such as persons, organizations, countries, dates, etc.\n",
        "\n",
        "**Example:**\n",
        "Input: `\"John Doe is a software engineer working at Google. He lives in France.\"`\n",
        "\n",
        "*   **John Doe** $\\rightarrow$ PERSON\n",
        "*   **Google** $\\rightarrow$ ORGANIZATION\n",
        "*   **France** $\\rightarrow$ COUNTRY (Geopolitical Entity)\n",
        "\n",
        "### 4.2 Applications\n",
        "*   **Efficient search algorithms**.\n",
        "*   **Question answering**.\n",
        "*   **News article classification**.\n",
        "*   **Customer service** (routing tickets based on entities mentioned).\n",
        "\n",
        "### 4.3 NER using spaCy\n",
        "In spaCy, named entities are accessed via the `doc.ents` property. Each entity has a `text` and a `label_`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "string = \"John Doe is a software engineer working at Google. He lives in France.\"\n",
        "\n",
        "# Load model and create Doc object\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(string)\n",
        "\n",
        "# Generate named entities\n",
        "# We iterate over doc.ents, not tokens\n",
        "ne = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "print(ne)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 4.4 NER Annotations in spaCy\n",
        "spaCy supports more than 15 categories of named entities. Here are some common ones:\n",
        "\n",
        "| Type | Description |\n",
        "| :--- | :--- |\n",
        "| **PERSON** | People, including fictional. |\n",
        "| **NORP** | Nationalities or religious or political groups. |\n",
        "| **FAC** | Buildings, airports, highways, bridges, etc. |\n",
        "| **ORG** | Companies, agencies, institutions, etc. |\n",
        "| **GPE** | Countries, cities, states. |\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip: A Word of Caution</b> <br>\n",
        "<ul>\n",
        "    <li><b>Not Perfect:</b> NER models are probabilistic and can make mistakes.</li>\n",
        "    <li><b>Data Dependency:</b> Performance depends heavily on the training data.</li>\n",
        "    <li><b>Specialization:</b> For nuanced cases (e.g., medical texts, legal documents), you often need to train models with specialized data.</li>\n",
        "    <li><b>Language Specific:</b> Models are specific to the language they were trained on.</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Conclusion</span><br>\n",
        "\n",
        "In this notebook, we have covered the foundational steps of **Feature Engineering for NLP** using Python and `spaCy`.\n",
        "\n",
        "**Key Takeaways:**\n",
        "1.  **Tokenization**: Breaking text into meaningful units (words/punctuation) is the first step in understanding text structure.\n",
        "2.  **Lemmatization**: Reducing words to their base forms normalizes the text and reduces vocabulary size.\n",
        "3.  **Text Cleaning**: Removing noise (stopwords, punctuation, special characters) is crucial, but must be done carefully to avoid losing valuable information (like proper nouns with numbers).\n",
        "4.  **POS Tagging**: Identifying grammatical roles helps in disambiguating meaning and analyzing sentiment.\n",
        "5.  **NER**: Extracting real-world entities (People, Orgs, Locations) adds semantic understanding to the text processing pipeline.\n",
        "\n",
        "**Next Steps:**\n",
        "With these features extracted and cleaned, your text data is now ready for vectorization (e.g., Bag of Words, TF-IDF) and subsequent modeling in machine learning algorithms.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}