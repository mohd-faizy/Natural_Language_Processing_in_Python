{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Classifying Fake News Using Supervised Learning with NLP</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Introduction to Natural Language Processing in Python)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "1. [What is Supervised Learning?](#section-1)\n",
        "2. [The IMDB Movie Dataset](#section-2)\n",
        "3. [Supervised Learning Steps](#section-3)\n",
        "4. [Building Word Count Vectors with Scikit-Learn](#section-4)\n",
        "5. [Naive Bayes Classifier](#section-5)\n",
        "6. [Model Evaluation & Confusion Matrix](#section-6)\n",
        "7. [Simple NLP, Complex Problems](#section-7)\n",
        "8. [Conclusion](#section-8)\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. What is Supervised Learning?</span><br>\n",
        "\n",
        "### Understanding the Basics\n",
        "Supervised learning is a fundamental form of machine learning where the problem involves predefined training data. This data comes with a **label** (or outcome) that you want the model to learn.\n",
        "\n",
        "In the context of classification problems, the goal is to make good hypotheses about a specific class or species based on input features.\n",
        "\n",
        "### Example: The Iris Dataset\n",
        "A classic example of supervised learning is classifying flower species based on geometric features. Below is a representation of such a dataset, where `Sepal length`, `Sepal width`, `Petal length`, and `Petal width` are features, and `Species` is the label.\n",
        "\n",
        "| Sepal length | Sepal width | Petal length | Petal width | Species |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| 5.1 | 3.5 | 1.4 | 0.2 | I. setosa |\n",
        "| 7.0 | 3.2 | 4.77 | 1.4 | I. versicolor |\n",
        "| 6.3 | 3.3 | 6.0 | 2.5 | I. virginica |\n",
        "\n",
        "### Supervised Learning with NLP\n",
        "When applying supervised learning to Natural Language Processing (NLP), we face a unique challenge: we must use **language** instead of geometric features.\n",
        "\n",
        "*   **Library**: `scikit-learn` is a powerful open-source library used for this purpose.\n",
        "*   **Data Creation**: To create supervised learning data from text, we typically use:\n",
        "    *   Bag-of-words models\n",
        "    *   tf-idf (Term Frequency-Inverse Document Frequency) as features.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. The IMDB Movie Dataset</span><br>\n",
        "\n",
        "To demonstrate text classification, we will look at the IMDB Movie Dataset. The goal is to predict the movie genre based on the plot summary.\n",
        "\n",
        "### Dataset Structure\n",
        "The dataset consists of movie plots and corresponding categorical labels (Sci-Fi or Action).\n",
        "\n",
        "| Plot | Sci-Fi | Action |\n",
        "| :--- | :--- | :--- |\n",
        "| In a post-apocalyptic world in human decay, a ... | 1 | 0 |\n",
        "| Mohei is a wandering swordsman. He arrives in ... | 0 | 1 |\n",
        "| #137 is a SCI/FI thriller about a girl, Marla,... | 1 | 0 |\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> In this dataset, categorical features are generated using preprocessing. A '1' indicates the movie belongs to that genre, while a '0' indicates it does not.</div>\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. Supervised Learning Steps</span><br>\n",
        "\n",
        "To build a successful NLP model, we follow a structured pipeline:\n",
        "\n",
        "1.  **Collect and preprocess our data**: Gather text and clean it.\n",
        "2.  **Determine a label**: Identify what we are predicting (Example: Movie genre).\n",
        "3.  **Split data**: Divide the dataset into **training** and **test** sets to ensure fair evaluation.\n",
        "4.  **Extract features**: Convert text into numerical vectors to help predict the label.\n",
        "    *   *Note*: The Bag-of-words vector is built into `scikit-learn`.\n",
        "5.  **Evaluate**: Use the trained model on the test set to measure performance.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Building Word Count Vectors with Scikit-Learn</span><br>\n",
        "\n",
        "The first step in our pipeline (after loading data) is converting text to numbers using the `CountVectorizer`.\n",
        "\n",
        "### Original Code (From PDF)\n",
        "The following code demonstrates how to split the data and vectorize it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# df = ... # Load data into DataFrame\n",
        "# y = df['Sci-Fi']\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     df['plot'], y,\n",
        "#     test_size=0.33,\n",
        "#     random_state=53)\n",
        "\n",
        "# count_vectorizer = CountVectorizer(stop_words='english')\n",
        "# count_train = count_vectorizer.fit_transform(X_train.values)\n",
        "# count_test = count_vectorizer.transform(X_test.values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Enhanced Executable Code\n",
        "Below is a fully working example where we create a dummy dataset to simulate the IMDB data, allowing you to run the vectorization process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 26\n",
            "Training Matrix Shape: (5, 26)\n",
            "Test Matrix Shape: (3, 26)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 1. Create Dummy Data (Simulating the IMDB Dataset)\n",
        "data = {\n",
        "    'plot': [\n",
        "        'In a post-apocalyptic world in human decay, a robot finds hope.',\n",
        "        'Mohei is a wandering swordsman. He arrives in a village.',\n",
        "        '#137 is a SCI/FI thriller about a girl, Marla, who travels space.',\n",
        "        'The galactic empire strikes back with laser cannons.',\n",
        "        'A martial artist fights for honor in the ancient arena.',\n",
        "        'Aliens invade earth and a hero must save the planet.',\n",
        "        'A detective solves a crime in the city underworld.',\n",
        "        'Space station alpha is under attack by cyborgs.'\n",
        "    ],\n",
        "    'Sci-Fi': [1, 0, 1, 1, 0, 1, 0, 1] # 1 = Sci-Fi, 0 = Not Sci-Fi (Action/Other)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2. Define Label\n",
        "y = df['Sci-Fi']\n",
        "\n",
        "# 3. Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['plot'], y,\n",
        "    test_size=0.33,\n",
        "    random_state=53\n",
        ")\n",
        "\n",
        "# 4. Initialize CountVectorizer\n",
        "# stop_words='english' removes common words like 'the', 'is', 'in'\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# 5. Fit and Transform Training Data\n",
        "count_train = count_vectorizer.fit_transform(X_train.values)\n",
        "\n",
        "# 6. Transform Test Data (Do NOT fit on test data)\n",
        "count_test = count_vectorizer.transform(X_test.values)\n",
        "\n",
        "print(\"Vocabulary size:\", len(count_vectorizer.get_feature_names_out()))\n",
        "print(\"Training Matrix Shape:\", count_train.shape)\n",
        "print(\"Test Matrix Shape:\", count_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Naive Bayes Classifier</span><br>\n",
        "\n",
        "### Theory\n",
        "The **Naive Bayes Model** is commonly used for testing NLP classification problems. It has its basis in probability: given a particular piece of data, how likely is a particular outcome?\n",
        "\n",
        "**Examples:**\n",
        "*   If the plot has a \"spaceship\", how likely is it to be sci-fi?\n",
        "*   Given a \"spaceship\" **and** an \"alien\", how likely **now** is it sci-fi?\n",
        "\n",
        "Each word from the `CountVectorizer` acts as a feature. Naive Bayes is favored because it is simple and effective.\n",
        "\n",
        "### Original Code (From PDF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "# nb_classifier = MultinomialNB()\n",
        "# nb_classifier.fit(count_train, y_train)\n",
        "# pred = nb_classifier.predict(count_test)\n",
        "# metrics.accuracy_score(y_test, pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Enhanced Executable Code\n",
        "We will now train the model using the vectors created in the previous section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions: [1 1 1]\n",
            "Actual Labels: [0 1 1]\n",
            "Accuracy Score: 0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "# 1. Initialize the Classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# 2. Fit the classifier to the training data\n",
        "nb_classifier.fit(count_train, y_train)\n",
        "\n",
        "# 3. Generate predictions on the test data\n",
        "pred = nb_classifier.predict(count_test)\n",
        "\n",
        "# 4. Calculate Accuracy\n",
        "accuracy = metrics.accuracy_score(y_test, pred)\n",
        "\n",
        "print(f\"Predictions: {pred}\")\n",
        "print(f\"Actual Labels: {y_test.values}\")\n",
        "print(f\"Accuracy Score: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. Model Evaluation & Confusion Matrix</span><br>\n",
        "\n",
        "Accuracy is useful, but a **Confusion Matrix** gives deeper insight into where the model is making mistakes (e.g., confusing Action for Sci-Fi).\n",
        "\n",
        "### Confusion Matrix Structure (From PDF)\n",
        "The PDF presents an example confusion matrix array and table:\n",
        "\n",
        "`array([[6410, 563], [ 864, 2242]])`\n",
        "\n",
        "| | Action | Sci-Fi |\n",
        "| :--- | :--- | :--- |\n",
        "| **Action** | 6410 | 563 |\n",
        "| **Sci-Fi** | 864 | 2242 |\n",
        "\n",
        "*   **6410**: Correctly predicted Action.\n",
        "*   **2242**: Correctly predicted Sci-Fi.\n",
        "*   **563**: Action movies incorrectly predicted as Sci-Fi.\n",
        "*   **864**: Sci-Fi movies incorrectly predicted as Action.\n",
        "\n",
        "### Original Code (From PDF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# metrics.confusion_matrix(y_test, pred, labels=[0,1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Enhanced Executable Code\n",
        "Generating a confusion matrix for our dummy dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix Array:\n",
            "[[0 1]\n",
            " [0 2]]\n",
            "\n",
            "--- Interpretation ---\n",
            "True Negatives (Action predicted as Action): 0\n",
            "False Positives (Action predicted as Sci-Fi): 1\n",
            "False Negatives (Sci-Fi predicted as Action): 0\n",
            "True Positives (Sci-Fi predicted as Sci-Fi): 2\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate confusion matrix\n",
        "# labels=[0, 1] ensures the matrix is ordered by [Not Sci-Fi, Sci-Fi]\n",
        "conf_matrix = confusion_matrix(y_test, pred, labels=[0, 1])\n",
        "\n",
        "print(\"Confusion Matrix Array:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Visualizing the matrix simply\n",
        "print(\"\\n--- Interpretation ---\")\n",
        "print(f\"True Negatives (Action predicted as Action): {conf_matrix[0][0]}\")\n",
        "print(f\"False Positives (Action predicted as Sci-Fi): {conf_matrix[0][1]}\")\n",
        "print(f\"False Negatives (Sci-Fi predicted as Action): {conf_matrix[1][0]}\")\n",
        "print(f\"True Positives (Sci-Fi predicted as Sci-Fi): {conf_matrix[1][1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 7. Simple NLP, Complex Problems</span><br>\n",
        "\n",
        "While the mechanics of NLP (vectorization, classification) can be straightforward, the problems they solve are complex and prone to nuance.\n",
        "\n",
        "### 1. Translation\n",
        "Translation is difficult because context matters.\n",
        "*   **Example**: A tweet \"god bless the german language\" accompanied by a complex German sentence illustrating how difficult it is to translate specific economic terms.\n",
        "*   **Source**: [Twitter Link](https://twitter.com/Lupintweets/status/865533182455685121)\n",
        "\n",
        "### 2. Sentiment Analysis\n",
        "Words have different sentiments depending on the context (community or topic).\n",
        "*   **Example**: The word \"soft\".\n",
        "    *   In `r/sports`: \"big men are very **soft**\" (Negative sentiment).\n",
        "    *   In `r/TwoX`: \"some **soft** pajamas\" (Positive sentiment).\n",
        "*   **Source**: [Stanford NLP Project](https://nlp.stanford.edu/projects/socialsent/)\n",
        "\n",
        "### 3. Language Biases\n",
        "Models trained on human data inherit human biases.\n",
        "*   **Example**: Google Translate (Turkish to English).\n",
        "    *   Turkish: \"O bir profesÃ¶r. O bir bebek bakÄ±cÄ±sÄ±.\" (Gender neutral pronouns).\n",
        "    *   English Translation: \"He's a professor. She's a babysitter.\"\n",
        "    *   The model assumed the gender based on the profession, reflecting societal bias in the training data.\n",
        "*   **Related Talk**: [YouTube Link](https://www.youtube.com/watch?v=j7FwpZB1hWc)\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 8. Conclusion</span><br>\n",
        "\n",
        "In this notebook, we explored the fundamentals of classifying text using supervised learning.\n",
        "\n",
        "**Key Takeaways:**\n",
        "1.  **Supervised Learning**: Requires labeled data. In NLP, we convert text labels (like \"Sci-Fi\") into numerical categories.\n",
        "2.  **Feature Extraction**: We cannot feed raw text into a model. We used `CountVectorizer` (Bag-of-Words) to turn plot summaries into numerical vectors.\n",
        "3.  **Modeling**: We used the **Naive Bayes** classifier, a probabilistic model highly effective for text data.\n",
        "4.  **Evaluation**: We used accuracy scores and the **Confusion Matrix** to understand prediction errors.\n",
        "5.  **Ethics**: We acknowledged that NLP models can struggle with context and perpetuate real-world biases (e.g., gender bias in translation).\n",
        "\n",
        "**Next Steps:**\n",
        "*   Try using `TfidfVectorizer` instead of `CountVectorizer` to see if weighting unique words improves accuracy.\n",
        "*   Experiment with different classifiers like Logistic Regression or Support Vector Machines (SVM).\n",
        "*   Audit your datasets for potential biases before training models for production.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
