{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Natural Language Processing (NLP) Basics</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Natural Language Processing with spaCy)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction to Natural Language Processing (NLP)](#section-1)\n",
        "2. [Introduction to spaCy](#section-2)\n",
        "3. [The spaCy NLP Pipeline & Containers](#section-3)\n",
        "4. [Basic Text Processing Operations](#section-4)\n",
        "5. [Linguistic Features: POS Tagging](#section-5)\n",
        "6. [Named Entity Recognition (NER)](#section-6)\n",
        "7. [Visualization with displaCy](#section-7)\n",
        "8. [Conclusion](#section-8)\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. Introduction to Natural Language Processing (NLP)</span><br>\n",
        "\n",
        "### What is NLP?\n",
        "Natural Language Processing (NLP) is a specialized subfield of Artificial Intelligence (AI). Its primary goal is to help computers understand, interpret, and manipulate human language.\n",
        "\n",
        "**Key Characteristics:**\n",
        "*   **Understanding Language:** Bridges the gap between human communication and computer understanding.\n",
        "*   **Unstructured Data:** Helps extract actionable insights from unstructured text (emails, social media, documents).\n",
        "*   **Interdisciplinary:** Incorporates statistics, machine learning models, and deep learning models.\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> Think of AI as the broad umbrella. Machine Learning is a subset of AI, and NLP intersects with both, often utilizing Deep Learning for complex tasks. </div>\n",
        "\n",
        "### Common NLP Use Cases\n",
        "\n",
        "1.  **Sentiment Analysis:**\n",
        "    *   Determining the underlying subjective tone of a piece of writing.\n",
        "    *   *Example:* Classifying a review as \"Positive\" (\"Great service!\") or \"Negative\" (\"Horrible experience\").\n",
        "\n",
        "2.  **Named Entity Recognition (NER):**\n",
        "    *   Locating and classifying named entities in unstructured text into pre-defined categories.\n",
        "    *   *Named Entities:* Real-world objects such as a person (John McCarthy), location, date (September 4, 1927), or organization.\n",
        "\n",
        "3.  **Text Generation:**\n",
        "    *   Generating human-like responses to text input.\n",
        "    *   *Example:* Large Language Models like ChatGPT.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. Introduction to spaCy</span><br>\n",
        "\n",
        "**spaCy** is a free, open-source library for Advanced NLP in Python. It is specifically designed for production use.\n",
        "\n",
        "**Why use spaCy?**\n",
        "*   **Information Extraction:** Designed to build systems that extract information.\n",
        "*   **Production-Ready:** Provides robust and fast code suitable for real-world applications.\n",
        "*   **Language Support:** Supports 64+ languages.\n",
        "*   **Features:** It is robust, fast, and includes built-in visualization libraries.\n",
        "\n",
        "### Installation and Setup\n",
        "\n",
        "To use spaCy, you must install the library and download a trained language model (e.g., `en_core_web_sm` for English).\n",
        "\n",
        "#### Original Code (Shell Commands)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# $ python3 pip install spacy\n",
        "# python3 -m spacy download en_core_web_sm\n",
        "# import spacy\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Enhanced Code (Executable)\n",
        "The following code installs spaCy (if not present), downloads the model, and initializes the `nlp` object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "  Downloading spacy-3.8.11-cp314-cp314-win_amd64.whl.metadata (28 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.15-cp314-cp314-win_amd64.whl.metadata (2.3 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.13-cp314-cp314-win_amd64.whl.metadata (9.9 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.12-cp314-cp314-win_amd64.whl.metadata (2.6 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.10-cp314-cp314-win_amd64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.5.2-cp314-cp314-win_amd64.whl.metadata (20 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
            "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\mohdf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\mohdf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from spacy) (2.3.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\mohdf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from spacy) (2.32.5)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\mohdf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\mohdf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from spacy) (80.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\mohdf\\appdata\\roaming\\python\\python314\\site-packages (from spacy) (25.0)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading pydantic_core-2.41.5-cp314-cp314-win_amd64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\mohdf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mohdf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohdf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mohdf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohdf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.3.3-cp314-cp314-win_amd64.whl.metadata (7.7 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\mohdf\\appdata\\roaming\\python\\python314\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Collecting click>=8.0.0 (from typer-slim<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
            "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
            "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy)\n",
            "  Downloading wrapt-2.0.1-cp314-cp314-win_amd64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mohdf\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from jinja2->spacy) (3.0.3)\n",
            "Downloading spacy-3.8.11-cp314-cp314-win_amd64.whl (14.4 MB)\n",
            "   ---------------------------------------- 0.0/14.4 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.3/14.4 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.5/14.4 MB 2.7 MB/s eta 0:00:06\n",
            "   -- ------------------------------------- 1.0/14.4 MB 2.3 MB/s eta 0:00:06\n",
            "   --- ------------------------------------ 1.3/14.4 MB 2.1 MB/s eta 0:00:07\n",
            "   ----- ---------------------------------- 2.1/14.4 MB 2.1 MB/s eta 0:00:06\n",
            "   -------- ------------------------------- 2.9/14.4 MB 2.5 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 3.7/14.4 MB 2.7 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 4.5/14.4 MB 2.8 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 5.0/14.4 MB 2.8 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 5.2/14.4 MB 2.7 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 6.0/14.4 MB 2.8 MB/s eta 0:00:04\n",
            "   --------------------- ------------------ 7.6/14.4 MB 3.2 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 9.2/14.4 MB 3.6 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 11.0/14.4 MB 3.9 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 12.1/14.4 MB 4.0 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 12.8/14.4 MB 4.0 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 13.9/14.4 MB 4.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 14.4/14.4 MB 4.0 MB/s  0:00:03\n",
            "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.13-cp314-cp314-win_amd64.whl (40 kB)\n",
            "Downloading murmurhash-1.0.15-cp314-cp314-win_amd64.whl (26 kB)\n",
            "Downloading preshed-3.0.12-cp314-cp314-win_amd64.whl (120 kB)\n",
            "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
            "Downloading pydantic_core-2.41.5-cp314-cp314-win_amd64.whl (2.0 MB)\n",
            "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
            "   --------------- ------------------------ 0.8/2.0 MB 2.3 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 1.3/2.0 MB 2.5 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 1.6/2.0 MB 2.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 1.8/2.0 MB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.0/2.0 MB 1.9 MB/s  0:00:01\n",
            "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.2-cp314-cp314-win_amd64.whl (658 kB)\n",
            "   ---------------------------------------- 0.0/658.9 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/658.9 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/658.9 kB ? eta -:--:--\n",
            "   --------------- ------------------------ 262.1/658.9 kB ? eta -:--:--\n",
            "   ------------------------------ ------- 524.3/658.9 kB 995.2 kB/s eta 0:00:01\n",
            "   ---------------------------------------- 658.9/658.9 kB 1.1 MB/s  0:00:00\n",
            "Downloading thinc-8.3.10-cp314-cp314-win_amd64.whl (1.7 MB)\n",
            "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
            "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
            "   ------------ --------------------------- 0.5/1.7 MB 1.1 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 0.8/1.7 MB 1.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 1.6/1.7 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.7/1.7 MB 1.7 MB/s  0:00:01\n",
            "Downloading blis-1.3.3-cp314-cp314-win_amd64.whl (6.3 MB)\n",
            "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.8/6.3 MB 2.2 MB/s eta 0:00:03\n",
            "   -------- ------------------------------- 1.3/6.3 MB 2.7 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 1.8/6.3 MB 2.5 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 2.1/6.3 MB 2.1 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 2.4/6.3 MB 1.9 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 2.4/6.3 MB 1.9 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 2.6/6.3 MB 1.7 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 2.9/6.3 MB 1.6 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 3.1/6.3 MB 1.5 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 4.2/6.3 MB 1.8 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 4.7/6.3 MB 1.9 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 5.2/6.3 MB 1.9 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 5.2/6.3 MB 1.9 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 5.5/6.3 MB 1.9 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 5.8/6.3 MB 1.7 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 6.0/6.3 MB 1.7 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 6.0/6.3 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 6.3/6.3 MB 1.6 MB/s  0:00:03\n",
            "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
            "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
            "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
            "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
            "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading wrapt-2.0.1-cp314-cp314-win_amd64.whl (60 kB)\n",
            "Installing collected packages: wrapt, wasabi, typing-inspection, spacy-loggers, spacy-legacy, pydantic-core, murmurhash, cymem, cloudpathlib, click, catalogue, blis, annotated-types, typer-slim, srsly, smart-open, pydantic, preshed, confection, weasel, thinc, spacy\n",
            "\n",
            "   - --------------------------------------  1/22 [wasabi]\n",
            "   --- ------------------------------------  2/22 [typing-inspection]\n",
            "   ----- ----------------------------------  3/22 [spacy-loggers]\n",
            "   ------- --------------------------------  4/22 [spacy-legacy]\n",
            "   ------- --------------------------------  4/22 [spacy-legacy]\n",
            "   ---------- -----------------------------  6/22 [murmurhash]\n",
            "   -------------- -------------------------  8/22 [cloudpathlib]\n",
            "   -------------- -------------------------  8/22 [cloudpathlib]\n",
            "   -------------- -------------------------  8/22 [cloudpathlib]\n",
            "   ---------------- -----------------------  9/22 [click]\n",
            "   ---------------- -----------------------  9/22 [click]\n",
            "   -------------------- ------------------- 11/22 [blis]\n",
            "   -------------------- ------------------- 11/22 [blis]\n",
            "   ----------------------- ---------------- 13/22 [typer-slim]\n",
            "   ------------------------- -------------- 14/22 [srsly]\n",
            "   ------------------------- -------------- 14/22 [srsly]\n",
            "   ------------------------- -------------- 14/22 [srsly]\n",
            "   ------------------------- -------------- 14/22 [srsly]\n",
            "   ------------------------- -------------- 14/22 [srsly]\n",
            "   ------------------------- -------------- 14/22 [srsly]\n",
            "   ------------------------- -------------- 14/22 [srsly]\n",
            "   ------------------------- -------------- 14/22 [srsly]\n",
            "   ------------------------- -------------- 14/22 [srsly]\n",
            "   ------------------------- -------------- 14/22 [srsly]\n",
            "   ------------------------- -------------- 14/22 [srsly]\n",
            "   --------------------------- ------------ 15/22 [smart-open]\n",
            "   --------------------------- ------------ 15/22 [smart-open]\n",
            "   ----------------------------- ---------- 16/22 [pydantic]\n",
            "   ----------------------------- ---------- 16/22 [pydantic]\n",
            "   ----------------------------- ---------- 16/22 [pydantic]\n",
            "   ----------------------------- ---------- 16/22 [pydantic]\n",
            "   ----------------------------- ---------- 16/22 [pydantic]\n",
            "   ----------------------------- ---------- 16/22 [pydantic]\n",
            "   ----------------------------- ---------- 16/22 [pydantic]\n",
            "   ----------------------------- ---------- 16/22 [pydantic]\n",
            "   ----------------------------- ---------- 16/22 [pydantic]\n",
            "   ----------------------------- ---------- 16/22 [pydantic]\n",
            "   ----------------------------- ---------- 16/22 [pydantic]\n",
            "   ----------------------------- ---------- 16/22 [pydantic]\n",
            "   ------------------------------ --------- 17/22 [preshed]\n",
            "   ---------------------------------- ----- 19/22 [weasel]\n",
            "   ---------------------------------- ----- 19/22 [weasel]\n",
            "   ---------------------------------- ----- 19/22 [weasel]\n",
            "   ---------------------------------- ----- 19/22 [weasel]\n",
            "   ------------------------------------ --- 20/22 [thinc]\n",
            "   ------------------------------------ --- 20/22 [thinc]\n",
            "   ------------------------------------ --- 20/22 [thinc]\n",
            "   ------------------------------------ --- 20/22 [thinc]\n",
            "   ------------------------------------ --- 20/22 [thinc]\n",
            "   ------------------------------------ --- 20/22 [thinc]\n",
            "   ------------------------------------ --- 20/22 [thinc]\n",
            "   ------------------------------------ --- 20/22 [thinc]\n",
            "   ------------------------------------ --- 20/22 [thinc]\n",
            "   ------------------------------------ --- 20/22 [thinc]\n",
            "   ------------------------------------ --- 20/22 [thinc]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   -------------------------------------- - 21/22 [spacy]\n",
            "   ---------------------------------------- 22/22 [spacy]\n",
            "\n",
            "Successfully installed annotated-types-0.7.0 blis-1.3.3 catalogue-2.0.10 click-8.3.1 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 pydantic-2.12.5 pydantic-core-2.41.5 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 typer-slim-0.21.1 typing-inspection-0.4.2 wasabi-1.1.3 weasel-0.4.3 wrapt-2.0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\001_Github_Repo_all\\Natural_Language_Processing_in_Python\\.venv\\Scripts\\python.exe: No module named pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy version: 3.8.11\n",
            "Model loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Install spaCy and download the small English model\n",
        "# Note: In a Jupyter environment, we use ! for shell commands\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the trained pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print(\"spaCy version:\", spacy.__version__)\n",
        "print(\"Model loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. The spaCy NLP Pipeline & Containers</span><br>\n",
        "\n",
        "### How spaCy Processes Text\n",
        "When you load a model (e.g., `nlp = spacy.load(...)`), you are creating a **Language** object. This object acts as a text processing pipeline.\n",
        "\n",
        "When you apply this `nlp` object to a text string, it returns a **Doc** container.\n",
        "\n",
        "**The Flow:**\n",
        "`Text` (String) -> `nlp(Text)` -> `Doc` Object\n",
        "\n",
        "### Container Objects in spaCy\n",
        "spaCy uses specific data structures to represent text data efficiently.\n",
        "\n",
        "| Name | Description |\n",
        "| :--- | :--- |\n",
        "| **Doc** | A container for accessing linguistic annotations of text. |\n",
        "| **Span** | A slice from a `Doc` object (a sequence of tokens). |\n",
        "| **Token** | An individual token, i.e., a word, punctuation, whitespace, etc. |\n",
        "\n",
        "### Pipeline Components\n",
        "The processing pipeline consists of several components that modify the `Doc` object in sequence.\n",
        "\n",
        "| Component | Name | Description |\n",
        "| :--- | :--- | :--- |\n",
        "| **Tokenizer** | Tokenizer | Segments text into tokens and creates the `Doc` object. |\n",
        "| **Tagger** | Tagger | Assigns part-of-speech tags. |\n",
        "| **Lemmatizer** | Lemmatizer | Reduces words to their root forms. |\n",
        "| **EntityRecognizer** | NER | Detects and labels named entities. |\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> The pipeline is modular. Components like the <b>DependencyParser</b> and <b>Sentencizer</b> are also crucial for understanding grammatical structure and sentence boundaries. </div>\n",
        "\n",
        "#### Code Example: Creating a Doc Object\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of object: <class 'spacy.tokens.doc.Doc'>\n",
            "Text content: Here's my spaCy pipeline.\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process a text string\n",
        "# This runs the entire pipeline (Tokenizer -> Tagger -> Parser -> NER)\n",
        "doc = nlp(\"Here's my spaCy pipeline.\")\n",
        "\n",
        "print(f\"Type of object: {type(doc)}\")\n",
        "print(f\"Text content: {doc.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Basic Text Processing Operations</span><br>\n",
        "\n",
        "### 4.1 Tokenization\n",
        "Tokenization is **always the first operation**. It splits the text into meaningful units called **Tokens**. A token can be a word, a number, or punctuation.\n",
        "\n",
        "#### Original Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['A', 'spaCy', 'pipeline', 'object', 'is', 'created', '.']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"A spaCy pipeline object is created.\"\n",
        "doc = nlp(text)\n",
        "print([token.text for token in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Enhanced Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text: Tokenization splits a sentence into its tokens.\n",
            "Tokens: ['Tokenization', 'splits', 'a', 'sentence', 'into', 'its', 'tokens', '.']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input text\n",
        "text = \"Tokenization splits a sentence into its tokens.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# List comprehension to extract token text\n",
        "tokens = [token.text for token in doc]\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Tokens:\", tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 4.2 Sentence Segmentation\n",
        "Sentence segmentation divides a document into individual sentences. This is more complex than tokenization and relies on the **DependencyParser** component.\n",
        "\n",
        "#### Original Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We are learning NLP.\n",
            "This course introduces spaCy.\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"We are learning NLP. This course introduces spaCy.\"\n",
        "doc = nlp(text)\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Enhanced Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Sentences ---\n",
            "Sentence 1: We are learning NLP.\n",
            "Sentence 2: This course introduces spaCy.\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"We are learning NLP. This course introduces spaCy.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"--- Sentences ---\")\n",
        "# doc.sents is a generator that yields Span objects\n",
        "for i, sent in enumerate(doc.sents, 1):\n",
        "    print(f\"Sentence {i}: {sent.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 4.3 Lemmatization\n",
        "A **lemma** is the base form of a token. For example, the lemma of \"eats\", \"ate\", and \"eating\" is **\"eat\"**. Lemmatization improves the accuracy of language models by normalizing words.\n",
        "\n",
        "#### Original Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('We', 'we'), ('are', 'be'), ('seeing', 'see'), ('her', 'she'), ('after', 'after'), ('one', 'one'), ('year', 'year'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"We are seeing her after one year.\")\n",
        "print([(token.text, token.lemma_) for token in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Enhanced Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token      | Lemma     \n",
            "-------------------------\n",
            "We         | we        \n",
            "are        | be        \n",
            "seeing     | see       \n",
            "her        | she       \n",
            "after      | after     \n",
            "one        | one       \n",
            "year       | year      \n",
            ".          | .         \n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"We are seeing her after one year.\")\n",
        "\n",
        "print(f\"{'Token':<10} | {'Lemma':<10}\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<10} | {token.lemma_:<10}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Linguistic Features: POS Tagging</span><br>\n",
        "\n",
        "### Part-of-Speech (POS) Tagging\n",
        "POS tagging involves categorizing words grammatically based on their function and context within a sentence. spaCy captures these tags in the `pos_` feature of a token.\n",
        "\n",
        "| POS | Description | Example |\n",
        "| :--- | :--- | :--- |\n",
        "| **VERB** | Verb | run, eat, ate, take |\n",
        "| **NOUN** | Noun | man, airplane, tree, flower |\n",
        "| **ADJ** | Adjective | big, old, incompatible |\n",
        "| **ADV** | Adverb | very, down, there, tomorrow |\n",
        "| **CONJ** | Conjunction | and, or, but |\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> Use <code>spacy.explain()</code> to get a human-readable description of any tag (e.g., <code>spacy.explain(\"ADP\")</code>). </div>\n",
        "\n",
        "### Handling Ambiguity\n",
        "Some words can be different parts of speech depending on context (e.g., \"watch\").\n",
        "\n",
        "#### Original Code (Verb vs Noun)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('I', 'PRON', 'pronoun'), ('watch', 'VERB', 'verb'), ('TV', 'NOUN', 'noun'), ('.', 'PUNCT', 'punctuation')]\n",
            "[('I', 'PRON', 'pronoun'), ('left', 'VERB', 'verb'), ('without', 'ADP', 'adposition'), ('my', 'PRON', 'pronoun'), ('watch', 'NOUN', 'noun'), ('.', 'PUNCT', 'punctuation')]\n"
          ]
        }
      ],
      "source": [
        "verb_sent = \"I watch TV.\"\n",
        "print([(token.text, token.pos_, spacy.explain(token.pos_)) for token in nlp(verb_sent)])\n",
        "\n",
        "noun_sent = \"I left without my watch.\"\n",
        "print([(token.text, token.pos_, spacy.explain(token.pos_)) for token in nlp(noun_sent)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Enhanced Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: I watch TV.\n",
            "I          | PRON   | pronoun\n",
            "watch      | VERB   | verb\n",
            "TV         | NOUN   | noun\n",
            ".          | PUNCT  | punctuation\n",
            "\n",
            "========================================\n",
            "\n",
            "Sentence: I left without my watch.\n",
            "I          | PRON   | pronoun\n",
            "left       | VERB   | verb\n",
            "without    | ADP    | adposition\n",
            "my         | PRON   | pronoun\n",
            "watch      | NOUN   | noun\n",
            ".          | PUNCT  | punctuation\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Case 1: 'watch' as a VERB\n",
        "verb_sent = \"I watch TV.\"\n",
        "doc_verb = nlp(verb_sent)\n",
        "\n",
        "print(f\"Sentence: {verb_sent}\")\n",
        "for token in doc_verb:\n",
        "    print(f\"{token.text:<10} | {token.pos_:<6} | {spacy.explain(token.pos_)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "# Case 2: 'watch' as a NOUN\n",
        "noun_sent = \"I left without my watch.\"\n",
        "doc_noun = nlp(noun_sent)\n",
        "\n",
        "print(f\"Sentence: {noun_sent}\")\n",
        "for token in doc_noun:\n",
        "    print(f\"{token.text:<10} | {token.pos_:<6} | {spacy.explain(token.pos_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. Named Entity Recognition (NER)</span><br>\n",
        "\n",
        "### What is NER?\n",
        "Named-Entity Recognition (NER) classifies named entities mentioned in unstructured text into pre-defined categories. A named entity is a real-world object with a name.\n",
        "\n",
        "| Entity Type | Description |\n",
        "| :--- | :--- |\n",
        "| **PERSON** | Named person or family. |\n",
        "| **ORG** | Companies, institutions, etc. |\n",
        "| **GPE** | Geo-political entity (countries, cities). |\n",
        "| **LOC** | Non-GPE locations (mountain ranges, bodies of water). |\n",
        "| **DATE** | Absolute or relative dates or periods. |\n",
        "| **TIME** | Time smaller than a day. |\n",
        "\n",
        "### NER in spaCy\n",
        "*   Entities are extracted using the **NER** pipeline component.\n",
        "*   Access entities via the `doc.ents` property.\n",
        "*   Access the label via `.label_`.\n",
        "*   Access entity type per token via `token.ent_type_`.\n",
        "\n",
        "#### Original Code (Accessing Entities)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Albert Einstein', 0, 15, 'PERSON')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Albert Einstein was genius.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print([(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Enhanced Code (Accessing Entities)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Extracted Entities ---\n",
            "Text: Albert Einstein\n",
            "Start Char: 0\n",
            "End Char: 15\n",
            "Label: PERSON (People, including fictional)\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Albert Einstein was a genius.\" # Corrected grammar slightly for better flow\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"--- Extracted Entities ---\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(f\"Text: {ent.text}\")\n",
        "    print(f\"Start Char: {ent.start_char}\")\n",
        "    print(f\"End Char: {ent.end_char}\")\n",
        "    print(f\"Label: {ent.label_} ({spacy.explain(ent.label_)})\")\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Original Code (Token Entity Types)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Albert', 'PERSON'), ('Einstein', 'PERSON'), ('was', ''), ('genius', ''), ('.', '')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Albert Einstein was genius.\"\n",
        "doc = nlp(text)\n",
        "print([(token.text, token.ent_type_) for token in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Enhanced Code (Token Entity Types)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token           | Entity Type\n",
            "------------------------------\n",
            "Albert          | PERSON\n",
            "Einstein        | PERSON\n",
            "was             | \n",
            "genius          | \n",
            ".               | \n"
          ]
        }
      ],
      "source": [
        "# Checking entity type at the token level\n",
        "# Note: Tokens outside an entity have an empty string for ent_type_\n",
        "print(f\"{'Token':<15} | {'Entity Type':<10}\")\n",
        "print(\"-\" * 30)\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<15} | {token.ent_type_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 7. Visualization with displaCy</span><br>\n",
        "\n",
        "spaCy comes with a built-in visualizer called **displaCy**. It can visualize dependency parses and named entities.\n",
        "\n",
        "#### Original Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\001_Github_Repo_all\\Natural_Language_Processing_in_Python\\.venv\\Lib\\site-packages\\spacy\\displacy\\__init__.py:108: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
            "  warnings.warn(Warnings.W011)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "    <head>\n",
              "        <title>displaCy</title>\n",
              "    </head>\n",
              "\n",
              "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
              "<figure style=\"margin-bottom: 6rem\">\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Albert Einstein\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " was genius.</div>\n",
              "</figure>\n",
              "</body>\n",
              "</html></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using the 'ent' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "text = \"Albert Einstein was genius.\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "displacy.serve(doc, style=\"ent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Enhanced Code (Jupyter Friendly)\n",
        "In a standard script, `displacy.serve` starts a web server. In a Jupyter Notebook, it is often better to use `displacy.render` to show the visualization directly in the cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Albert Einstein was a genius born in Germany in 1879.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# style=\"ent\" visualizes Named Entities\n",
        "# jupyter=True renders it directly in the notebook\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 8. Conclusion</span><br>\n",
        "\n",
        "### Summary\n",
        "In this notebook, we explored the fundamentals of Natural Language Processing using the **spaCy** library.\n",
        "\n",
        "1.  **NLP Basics:** We defined NLP as the intersection of AI and linguistics, used for tasks like Sentiment Analysis and NER.\n",
        "2.  **spaCy Architecture:** We learned about the `nlp` object (Language class) and the `Doc` container.\n",
        "3.  **The Pipeline:** We visualized how text flows through components like the Tokenizer, Tagger, and NER.\n",
        "4.  **Text Processing:** We performed core operations:\n",
        "    *   **Tokenization:** Splitting text into words.\n",
        "    *   **Lemmatization:** Finding the root form of words.\n",
        "    *   **POS Tagging:** Understanding grammatical roles (Noun vs. Verb).\n",
        "    *   **NER:** Identifying real-world entities (People, Dates, Locations).\n",
        "5.  **Visualization:** We used `displaCy` to visually inspect our model's predictions.\n",
        "\n",
        "### Next Steps\n",
        "To deepen your knowledge:\n",
        "*   Experiment with different text inputs to see how the model handles ambiguity.\n",
        "*   Explore the **Dependency Parser** to understand the grammatical structure of sentences.\n",
        "*   Try using larger models (e.g., `en_core_web_md` or `en_core_web_lg`) which include word vectors for semantic similarity.\n",
        "*   Build a custom pipeline component to extract specific information relevant to your domain.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
