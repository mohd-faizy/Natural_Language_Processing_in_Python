{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Creating Transcription Helper Functions & Spoken Language Processing</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Spoken Language Processing in Python)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Exploring Audio Files](#section-1)\n",
        "2. [Preparing for the Proof of Concept](#section-2)\n",
        "3. [Creating Helper Functions](#section-3)\n",
        "4. [Sentiment Analysis on Spoken Language](#section-4)\n",
        "5. [Named Entity Recognition (NER) with spaCy](#section-5)\n",
        "6. [Classifying Transcribed Speech with Sklearn](#section-6)\n",
        "7. [Conclusion](#section-7)\n",
        "\n",
        "***\n",
        "\n",
        "<a id=\"section-1\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. Exploring Audio Files</span><br>\n",
        "\n",
        "Before diving into complex processing, the first step in any audio pipeline is inspecting the data source. We utilize Python's built-in `os` module to navigate the file system and identify the audio files available for processing.\n",
        "\n",
        "### Inspecting the Directory\n",
        "The following code lists the contents of the directory containing the audio files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import os module\n",
        "import os\n",
        "\n",
        "# Check the folder of audio files\n",
        "# Assuming the files are stored in a directory named \"acme_audio_files\"\n",
        "audio_files = os.listdir(\"acme_audio_files\")\n",
        "\n",
        "print(audio_files)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Expected Output:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "['call_1.mp3', 'call_2.mp3', 'call_3.mp3', 'call_4.mp3']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> Note that the files are in <code>.mp3</code> format. Most speech recognition libraries prefer or require <code>.wav</code> format, which we will address in the next section. </div>\n",
        "\n",
        "***\n",
        "\n",
        "<a id=\"section-2\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. Preparing for the Proof of Concept</span><br>\n",
        "\n",
        "To perform speech recognition, we need to convert our compressed audio (`.mp3`) into a lossless format (`.wav`). We use `pydub` for audio manipulation and `speech_recognition` for the actual transcription.\n",
        "\n",
        "### Conversion and Transcription Workflow\n",
        "1.  **Import**: Load the MP3 file using `AudioSegment`.\n",
        "2.  **Export**: Save the file as a WAV.\n",
        "3.  **Recognize**: Use the `Google Web Speech API` via the `speech_recognition` library to transcribe the audio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# 1. Import call 1 and convert to .wav\n",
        "# Load the mp3 file\n",
        "call_1 = AudioSegment.from_file(\"acme_audio_files/call_1.mp3\")\n",
        "\n",
        "# Export as wav\n",
        "call_1.export(\"acme_audio_files/call_1.wav\", format=\"wav\")\n",
        "\n",
        "# 2. Transcribe call 1\n",
        "recognizer = sr.Recognizer()\n",
        "\n",
        "# Load the newly created wav file into the recognizer\n",
        "call_1_file = sr.AudioFile(\"acme_audio_files/call_1.wav\")\n",
        "\n",
        "with call_1_file as source:\n",
        "    # Record the audio data from the file\n",
        "    call_1_audio = recognizer.record(call_1_file)\n",
        "\n",
        "# Recognize the speech using Google's API\n",
        "text = recognizer.recognize_google(call_1_audio)\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<a id=\"section-3\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. Creating Helper Functions</span><br>\n",
        "\n",
        "To make our workflow scalable, we will encapsulate the logic into reusable functions. We will create three specific functions:\n",
        "\n",
        "1.  `convert_to_wav()`: Handles file format conversion.\n",
        "2.  `show_pydub_stats()`: Displays technical audio attributes.\n",
        "3.  `transcribe_audio()`: Handles the transcription process.\n",
        "\n",
        "### Function 1: File Format Conversion\n",
        "This function takes a filename, converts it to `.wav`, and saves it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "def convert_to_wav(filename):\n",
        "    \"\"\"Takes an audio file of non .wav format and converts to .wav\"\"\"\n",
        "    \n",
        "    # Import audio file\n",
        "    audio = AudioSegment.from_file(filename)\n",
        "    \n",
        "    # Create new filename by replacing extension\n",
        "    new_filename = filename.split(\".\")[0] + \".wav\"\n",
        "    \n",
        "    # Export file as .wav\n",
        "    audio.export(new_filename, format=\"wav\")\n",
        "    \n",
        "    print(f\"Converting {filename} to {new_filename}...\")\n",
        "\n",
        "# Usage Example\n",
        "convert_to_wav(\"acme_audio_files/call_1.mp3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Function 2: Audio Statistics\n",
        "This function inspects the `.wav` file and prints metadata like channels, sample rate, and duration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_pydub_stats(filename):\n",
        "    \"\"\"Returns different audio attributes related to an audio file.\"\"\"\n",
        "    \n",
        "    # Create AudioSegment instance\n",
        "    audio_segment = AudioSegment.from_file(filename)\n",
        "    \n",
        "    # Print attributes\n",
        "    print(f\"Channels: {audio_segment.channels}\")\n",
        "    print(f\"Sample width: {audio_segment.sample_width}\")\n",
        "    print(f\"Frame rate (sample rate): {audio_segment.frame_rate}\")\n",
        "    print(f\"Frame width: {audio_segment.frame_width}\")\n",
        "    print(f\"Length (ms): {len(audio_segment)}\")\n",
        "    print(f\"Frame count: {audio_segment.frame_count()}\")\n",
        "\n",
        "# Usage Example\n",
        "show_pydub_stats(\"acme_audio_files/call_1.wav\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Function 3: Transcription\n",
        "This function wraps the `speech_recognition` logic to return text from a `.wav` file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "def transcribe_audio(filename):\n",
        "    \"\"\"Takes a .wav format audio file and transcribes it to text.\"\"\"\n",
        "    \n",
        "    # Setup a recognizer instance\n",
        "    recognizer = sr.Recognizer()\n",
        "    \n",
        "    # Import the audio file and convert to audio data\n",
        "    audio_file = sr.AudioFile(filename)\n",
        "    \n",
        "    with audio_file as source:\n",
        "        audio_data = recognizer.record(audio_file)\n",
        "        \n",
        "    # Return the transcribed text\n",
        "    return recognizer.recognize_google(audio_data)\n",
        "\n",
        "# Usage Example\n",
        "transcription = transcribe_audio(\"acme_audio_files/call_1.wav\")\n",
        "print(transcription)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<a id=\"section-4\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Sentiment Analysis on Spoken Language</span><br>\n",
        "\n",
        "Once we have text, we can analyze the sentiment (positive, negative, or neutral) of the conversation. We use the **NLTK** library and the **VADER** lexicon.\n",
        "\n",
        "### Installation and Setup\n",
        "First, ensure the necessary NLTK data is downloaded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install NLTK if not already installed\n",
        "# !pip install nltk\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK packages\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"vader_lexicon\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Using VADER for Sentiment Analysis\n",
        "We use the `SentimentIntensityAnalyzer` to get polarity scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Create sentiment analysis instance\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Test sentiment analysis on negative text\n",
        "print(sid.polarity_scores(\"This customer service is terrible.\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Output:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "{'neg': 0.437, 'neu': 0.563, 'pos': 0.0, 'compound': -0.4767}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Analyzing Transcribed Text\n",
        "We can apply this to the text we transcribed earlier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming 'call_3_channel_2.wav' exists and was transcribed\n",
        "call_3_channel_2_text = transcribe_audio(\"acme_audio_files/call_3_channel_2.wav\")\n",
        "\n",
        "print(f\"Text: {call_3_channel_2_text}\")\n",
        "\n",
        "# Sentiment analysis on the full text\n",
        "print(sid.polarity_scores(call_3_channel_2_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Sentence-by-Sentence Analysis\n",
        "Analyzing the whole block of text can average out emotions. It is often better to tokenize the text into sentences and analyze them individually.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "call_3_paid_api_text = \"Okay. Yeah. Hi, Diane. This is paid on this call and obviously the status of my orders at three weeks ago, and that service is terrible. Is this any better? Yes...\"\n",
        "\n",
        "# Find sentiment on each sentence\n",
        "for sentence in sent_tokenize(call_3_paid_api_text):\n",
        "    print(sentence)\n",
        "    print(sid.polarity_scores(sentence))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<a id=\"section-5\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Named Entity Recognition (NER) with spaCy</span><br>\n",
        "\n",
        "Named Entity Recognition (NER) helps us identify real-world objects like people, companies, dates, and locations in the text. We use the **spaCy** library for this.\n",
        "\n",
        "### Installation and Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install spaCy\n",
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load spaCy language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Basic Tokenization and NER\n",
        "We process a text string to create a spaCy `Doc` object, which automatically tokenizes the text and identifies entities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a spaCy doc\n",
        "doc = nlp(\"I'd like to talk about a smartphone I ordered on July 31st from your Sydney store, my order number is 40939440. I spoke to Georgia about it last week.\")\n",
        "\n",
        "# Show different tokens and positions\n",
        "print(\"--- Tokens ---\")\n",
        "for token in doc:\n",
        "    print(token.text, token.idx)\n",
        "\n",
        "# Show sentences\n",
        "print(\"\\n--- Sentences ---\")\n",
        "for sentence in doc.sents:\n",
        "    print(sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Built-in Named Entities\n",
        "spaCy comes with several built-in entity types:\n",
        "\n",
        "| Entity Type | Description |\n",
        "| :--- | :--- |\n",
        "| **PERSON** | People, including fictional. |\n",
        "| **ORG** | Companies, agencies, institutions, etc. |\n",
        "| **GPE** | Countries, cities, states. |\n",
        "| **PRODUCT** | Objects, vehicles, foods, etc. (Not services). |\n",
        "| **DATE** | Absolute or relative dates or periods. |\n",
        "| **TIME** | Times smaller than a day. |\n",
        "| **MONEY** | Monetary values, including unit. |\n",
        "| **CARDINAL** | Numerals that do not fall under another type. |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find named entities in doc\n",
        "print(\"\\n--- Entities ---\")\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Creating Custom Named Entities\n",
        "Sometimes the default model misses specific terms (like \"smartphone\" as a PRODUCT). We can add custom rules to the pipeline using `EntityRuler`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spacy.pipeline import EntityRuler\n",
        "\n",
        "# Check current pipeline\n",
        "print(\"Original Pipeline:\", nlp.pipeline)\n",
        "\n",
        "# Create EntityRuler instance\n",
        "ruler = EntityRuler(nlp)\n",
        "\n",
        "# Add token pattern to ruler\n",
        "# We want to label \"smartphone\" as a \"PRODUCT\"\n",
        "ruler.add_patterns([{\"label\": \"PRODUCT\", \"pattern\": \"smartphone\"}])\n",
        "\n",
        "# Add new rule to pipeline before the default 'ner' component\n",
        "# Note: In newer spaCy versions, use nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "# The syntax below follows the PDF's version for compatibility\n",
        "try:\n",
        "    nlp.add_pipe(ruler, before=\"ner\")\n",
        "except TypeError:\n",
        "    # Fallback for newer spaCy versions\n",
        "    nlp.add_pipe(\"entity_ruler\", before=\"ner\", config={\"overwrite_ents\": True}).add_patterns([{\"label\": \"PRODUCT\", \"pattern\": \"smartphone\"}])\n",
        "\n",
        "# Check updated pipeline\n",
        "print(\"Updated Pipeline:\", nlp.pipeline)\n",
        "\n",
        "# Test new entity rule\n",
        "doc = nlp(\"I'd like to talk about a smartphone I ordered on July 31st...\")\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<a id=\"section-6\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. Classifying Transcribed Speech with Sklearn</span><br>\n",
        "\n",
        "In this final section, we build a machine learning pipeline to classify phone calls into categories (e.g., \"post_purchase\" vs \"pre_purchase\") based on their transcribed text.\n",
        "\n",
        "### Data Preparation\n",
        "We assume we have two folders: `post_purchase` and `pre_purchase`, containing audio files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 1. Inspect audio folder\n",
        "post_purchase_audio = os.listdir(\"post_purchase\")\n",
        "print(post_purchase_audio[:5])\n",
        "\n",
        "# 2. Convert all files to wav (using our helper function)\n",
        "for file in post_purchase_audio:\n",
        "    # Note: In a real scenario, ensure full path is passed\n",
        "    # convert_to_wav(os.path.join(\"post_purchase\", file))\n",
        "    print(f\"Converting {file} to .wav...\")\n",
        "\n",
        "# 3. Function to transcribe all files in a folder\n",
        "def create_text_list(folder):\n",
        "    text_list = []\n",
        "    # Loop through folder\n",
        "    for file in folder:\n",
        "        # Check for .wav extension\n",
        "        if file.endswith(\".wav\"):\n",
        "            # Transcribe audio (Mocking the path for this example)\n",
        "            # text = transcribe_audio(os.path.join(folder_path, file))\n",
        "            text = \"mock transcription text\" \n",
        "            text_list.append(text)\n",
        "    return text_list\n",
        "\n",
        "# Generate lists (Mock data for demonstration based on slides)\n",
        "post_purchase_text = [\n",
        "    \"hey man I just bought a product from you guys\",\n",
        "    \"these clothes I just bought are too small\",\n",
        "    \"I recently got these pair of shoes but they're too big\",\n",
        "    \"I bought a pair of pants and they are wrong color\"\n",
        "]\n",
        "\n",
        "pre_purchase_text = [\n",
        "    \"hey I was wondering if you know where my new phone is\",\n",
        "    \"do you have this in stock\",\n",
        "    \"how much does this cost\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Organizing Data with Pandas\n",
        "We combine the text lists into a single DataFrame with labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create post purchase dataframe\n",
        "post_purchase_df = pd.DataFrame({\"label\": \"post_purchase\", \"text\": post_purchase_text})\n",
        "\n",
        "# Create pre purchase dataframe\n",
        "pre_purchase_df = pd.DataFrame({\"label\": \"pre_purchase\", \"text\": pre_purchase_text})\n",
        "\n",
        "# Combine pre purchase and post purchase\n",
        "df = pd.concat([post_purchase_df, pre_purchase_df])\n",
        "\n",
        "# View the combined dataframe\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Building the Text Classifier\n",
        "We use a **Naive Bayes** classifier within a Scikit-Learn **Pipeline**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[\"text\"],\n",
        "    df[\"label\"],\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create text classifier pipeline\n",
        "text_classifier = Pipeline([\n",
        "    (\"vectorizer\", CountVectorizer()),\n",
        "    (\"tfidf\", TfidfTransformer()),\n",
        "    (\"classifier\", MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the classifier pipeline on the training data\n",
        "text_classifier.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Evaluating the Model\n",
        "Finally, we test the accuracy of our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions and compare them to test labels\n",
        "predictions = text_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = 100 * np.mean(predictions == y_test)\n",
        "print(f\"The model is {accuracy:.2f}% accurate.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<a id=\"section-7\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 7. Conclusion</span><br>\n",
        "\n",
        "### Summary of Achievements\n",
        "In this notebook, we have successfully built a comprehensive Spoken Language Processing pipeline:\n",
        "1.  **Audio Conversion**: We used `pydub` to convert compressed `.mp3` files into `.wav` format suitable for analysis.\n",
        "2.  **Transcription**: We utilized `speech_recognition` and the Google Web Speech API to convert audio soundwaves into text.\n",
        "3.  **Audio Manipulation**: We created helper functions to automate conversion and inspect audio statistics.\n",
        "4.  **NLP Pipeline**: We applied `NLTK` for sentiment analysis, `spaCy` for Named Entity Recognition (including custom entities), and `scikit-learn` to classify the intent of the calls.\n",
        "\n",
        "### Next Steps\n",
        "To further advance your skills:\n",
        "*   **Project**: Apply these techniques to your own dataset of audio files.\n",
        "*   **Live Audio**: Explore the `speech_recognition.Microphone()` class to transcribe audio in real-time.\n",
        "*   **Advanced Models**: Experiment with different classifiers (e.g., Support Vector Machines) or deep learning models for improved accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One last transcription to celebrate!\n",
        "# one_last_transcription = transcribe_audio(\"congratulations.wav\")\n",
        "print(\"Congratulations on finishing the Spoken Language Processing with Python course!\")\n",
        "print(\"You should be proud.\")\n",
        "print(\"Now get out there and recognize some speech!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}