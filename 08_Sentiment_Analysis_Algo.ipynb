{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>SENTIMENT ANALYSIS IN PYTHON</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Let's predict the sentiment!)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Classification Problems](#section-1)\n",
        "2. [Linear and Logistic Regressions](#section-2)\n",
        "3. [The Logistic Function](#section-3)\n",
        "4. [Logistic Regression in Python](#section-4)\n",
        "5. [Measuring Model Performance](#section-5)\n",
        "6. [Using Accuracy Score](#section-6)\n",
        "7. [Train/Test Split](#section-7)\n",
        "8. [Logistic Regression with Train/Test Split](#section-8)\n",
        "9. [Confusion Matrix](#section-9)\n",
        "10. [Complex Models and Regularization](#section-10)\n",
        "11. [Predicting Probability vs. Class](#section-11)\n",
        "12. [The Sentiment Analysis Problem](#section-12)\n",
        "13. [Exploration of Reviews](#section-13)\n",
        "14. [Numeric Transformations (Vectorization)](#section-14)\n",
        "15. [Arguments of the Vectorizers](#section-15)\n",
        "16. [The Automated Sentiment Analysis System](#section-16)\n",
        "17. [Conclusion](#section-17)\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. Classification Problems</span><br>\n",
        "\n",
        "Sentiment analysis is fundamentally a classification problem where we attempt to categorize text data into specific sentiments.\n",
        "\n",
        "### Types of Classification\n",
        "1.  **Binary Classification**:\n",
        "    *   Used for product and movie reviews.\n",
        "    *   Outcome: **Positive** or **Negative**.\n",
        "2.  **Multi-class Classification**:\n",
        "    *   Used for more nuanced scenarios, such as tweets about airline companies.\n",
        "    *   Outcome: **Positive**, **Neutral**, or **Negative**.\n",
        "\n",
        "To demonstrate the concepts in this notebook, we will first generate some synthetic data to simulate a classification problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features shape: (1000, 10)\n",
            "Labels shape: (1000,)\n",
            "Sample labels: [0 1 1 0 1 0 0 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "# Synthetic Data Generation for Demonstration\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic features (X) and labels (y)\n",
        "# X represents numeric features extracted from text\n",
        "# y represents sentiment (0 = Negative, 1 = Positive)\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Labels shape: {y.shape}\")\n",
        "print(f\"Sample labels: {y[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. Linear and Logistic Regressions</span><br>\n",
        "\n",
        "When approaching classification, it is important to distinguish between Linear and Logistic regression.\n",
        "\n",
        "*   **Linear Regression**: Fits a straight line through the data. It is designed to predict a continuous numeric outcome (e.g., price, temperature). It is unbounded, meaning predictions can go from $-\\infty$ to $+\\infty$.\n",
        "*   **Logistic Regression**: Fits an S-shaped curve (sigmoid function). It is designed for classification. It outputs values between 0 and 1, which can be interpreted as probabilities.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. The Logistic Function</span><br>\n",
        "\n",
        "While linear regression predicts a numeric outcome, logistic regression predicts a **probability**.\n",
        "\n",
        "In the context of sentiment analysis, we are calculating the probability that a given review is positive:\n",
        "\n",
        "$$ Probability(sentiment = positive \\mid review) $$\n",
        "\n",
        "*   **Y-axis**: Probability (0 to 1).\n",
        "*   **X-axis**: Input features.\n",
        "*   **Threshold**: Typically, if the probability is $> 0.5$, we classify it as Positive (1); otherwise, Negative (0).\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Logistic Regression in Python</span><br>\n",
        "\n",
        "We use the `scikit-learn` library to implement Logistic Regression. The standard workflow involves importing the class, instantiating it, and fitting it to the data.\n",
        "\n",
        "### Original Code Snippet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Assuming X and y are defined\n",
        "# log_reg = LogisticRegression().fit(X, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Enhanced Executable Code\n",
        "Below, we train a model using the synthetic data generated in Section 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model trained successfully.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Instantiate the model\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "log_reg.fit(X, y)\n",
        "\n",
        "print(\"Model trained successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Measuring Model Performance</span><br>\n",
        "\n",
        "**Accuracy** is the fraction of predictions our model got right.\n",
        "*   The higher and closer the accuracy is to 1, the better.\n",
        "\n",
        "We can calculate accuracy directly using the `.score()` method of the trained model.\n",
        "\n",
        "### Code Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.859\n"
          ]
        }
      ],
      "source": [
        "# Accuracy using the .score() method\n",
        "score = log_reg.score(X, y)\n",
        "\n",
        "print(f\"Accuracy score: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> In the original presentation, the example accuracy was 0.9009. Your result above depends on the synthetic data generation. </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. Using Accuracy Score</span><br>\n",
        "\n",
        "Alternatively, we can predict the labels first and then use the `accuracy_score` function from `sklearn.metrics`. This is useful when you want to compare predicted vectors against actual vectors explicitly.\n",
        "\n",
        "### Code Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy using accuracy_score: 0.859\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Predict labels for the data\n",
        "y_predicted = log_reg.predict(X)\n",
        "\n",
        "# 2. Calculate accuracy by comparing actual (y) vs predicted (y_predicted)\n",
        "accuracy = accuracy_score(y, y_predicted)\n",
        "\n",
        "print(f\"Accuracy using accuracy_score: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 7. Train/Test Split</span><br>\n",
        "\n",
        "To properly evaluate a model, we cannot test it on the same data used for training (this leads to overfitting). We must split the data.\n",
        "\n",
        "*   **Training set**: Used to train the model (typically 70-80% of data).\n",
        "*   **Testing set**: Used to evaluate performance on unseen data.\n",
        "\n",
        "### Key Parameters for `train_test_split`\n",
        "*   `X`: Features.\n",
        "*   `y`: Labels.\n",
        "*   `test_size`: Proportion of data used for testing (e.g., 0.2 for 20%).\n",
        "*   `random_state`: Seed generator for reproducibility.\n",
        "*   `stratify`: Ensures the proportion of classes (positive/negative) in the split matches the original data.\n",
        "\n",
        "### Code Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape: (800, 10)\n",
            "Testing set shape: (200, 10)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, \n",
        "    y, \n",
        "    test_size=0.2, \n",
        "    random_state=123, \n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 8. Logistic Regression with Train/Test Split</span><br>\n",
        "\n",
        "Now we retrain the model using **only** the training data and evaluate it on both sets to check for overfitting.\n",
        "\n",
        "### Code Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on training data: 0.86625\n",
            "Accuracy on testing data: 0.85\n"
          ]
        }
      ],
      "source": [
        "# Train on training data\n",
        "log_reg = LogisticRegression().fit(X_train, y_train)\n",
        "\n",
        "# Check accuracy on training data\n",
        "train_acc = log_reg.score(X_train, y_train)\n",
        "print(f'Accuracy on training data: {train_acc}')\n",
        "\n",
        "# Check accuracy on testing data\n",
        "test_acc = log_reg.score(X_test, y_test)\n",
        "print(f'Accuracy on testing data: {test_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> If Training Accuracy is significantly higher than Testing Accuracy, the model is likely <b>overfitting</b>. </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 9. Confusion Matrix</span><br>\n",
        "\n",
        "Accuracy can be misleading. A confusion matrix gives a detailed breakdown of correct and incorrect predictions.\n",
        "\n",
        "### The Matrix Structure\n",
        "\n",
        "| | **Actual = 1** (Positive) | **Actual = 0** (Negative) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Predicted = 1** | **True Positive** (Correct) | **False Positive** (Type I Error) |\n",
        "| **Predicted = 0** | **False Negative** (Type II Error) | **True Negative** (Correct) |\n",
        "\n",
        "### Code Implementation\n",
        "\n",
        "We can normalize the matrix (divide by length of test set) to see proportions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix (Counts):\n",
            "[[85 15]\n",
            " [15 85]]\n",
            "\n",
            "Confusion Matrix (Proportions):\n",
            "[[0.425 0.075]\n",
            " [0.075 0.425]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Predict on test set\n",
        "y_predicted = log_reg.predict(X_test)\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_predicted)\n",
        "\n",
        "# Print raw counts\n",
        "print(\"Confusion Matrix (Counts):\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Print proportions (as shown in the PDF)\n",
        "print(\"\\nConfusion Matrix (Proportions):\")\n",
        "print(conf_matrix / len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 10. Complex Models and Regularization</span><br>\n",
        "\n",
        "### The Problem: Overfitting\n",
        "A complex model might capture \"noise\" in the data rather than the underlying pattern. This often happens when there are a large number of features.\n",
        "\n",
        "### The Solution: Regularization\n",
        "Regularization simplifies the model to prevent overfitting.\n",
        "\n",
        "*   **L2 Regularization**: Shrinks all coefficients towards zero (but not exactly zero).\n",
        "*   **C Parameter**: Inverse of regularization strength.\n",
        "    *   **High C**: Low penalization (Model fits training data very closely, risk of overfitting).\n",
        "    *   **Low C**: High penalization (Model is less flexible, simpler).\n",
        "\n",
        "### Code Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regularized model trained.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\001_Github_Repo_all\\Natural_Language_Processing_in_Python\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression with L2 penalty and specific C value\n",
        "# Note: 'l2' is often the default, but we specify it explicitly here.\n",
        "log_reg_regularized = LogisticRegression(penalty='l2', C=1.0)\n",
        "\n",
        "log_reg_regularized.fit(X_train, y_train)\n",
        "print(\"Regularized model trained.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 11. Predicting Probability vs. Class</span><br>\n",
        "\n",
        "Sometimes we want the raw probability score rather than just the class label (0 or 1).\n",
        "\n",
        "*   `predict()`: Returns the class (0 or 1).\n",
        "*   `predict_proba()`: Returns the probability for each class.\n",
        "\n",
        "### Code Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilities (first 5 samples):\n",
            " [[0.99733605 0.00266395]\n",
            " [0.95713098 0.04286902]\n",
            " [0.34869794 0.65130206]\n",
            " [0.55542056 0.44457944]\n",
            " [0.19333358 0.80666642]]\n",
            "\n",
            "Probabilities for Class 1 (first 5 samples):\n",
            " [0.00266395 0.04286902 0.65130206 0.44457944 0.80666642]\n"
          ]
        }
      ],
      "source": [
        "# Predict labels\n",
        "y_predicted = log_reg.predict(X_test)\n",
        "\n",
        "# Predict probabilities\n",
        "# Returns array of shape (n_samples, 2) -> [prob_class_0, prob_class_1]\n",
        "y_probab = log_reg.predict_proba(X_test)\n",
        "\n",
        "print(\"Probabilities (first 5 samples):\\n\", y_probab[:5])\n",
        "\n",
        "# Select probabilities for Class 1 (Positive Sentiment)\n",
        "y_probab_class1 = y_probab[:, 1]\n",
        "print(\"\\nProbabilities for Class 1 (first 5 samples):\\n\", y_probab_class1[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Note on Metrics**: Standard metrics like `accuracy_score` and `confusion_matrix` require **classes**, not probabilities. If you use probabilities, you will get a `ValueError`.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 12. The Sentiment Analysis Problem</span><br>\n",
        "\n",
        "Sentiment analysis is the process of understanding the opinion of an author about a subject. Common data sources include:\n",
        "*   Movie reviews (IMDb).\n",
        "*   Amazon product reviews.\n",
        "*   Twitter airline sentiment.\n",
        "*   Emotionally charged literary examples.\n",
        "\n",
        "The goal is to map these text inputs to sentiment labels.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 13. Exploration of Reviews</span><br>\n",
        "\n",
        "Before modeling, we explore the text data. Common techniques include:\n",
        "\n",
        "1.  **Basic Info**: Size of reviews.\n",
        "2.  **Word Clouds**: Visualizing frequent words.\n",
        "3.  **Length Features**:\n",
        "    *   Number of words.\n",
        "    *   Number of sentences.\n",
        "4.  **Language Detection**: Identifying the language of the review.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 14. Numeric Transformations (Vectorization)</span><br>\n",
        "\n",
        "Machine learning models cannot understand raw text. We must convert text into numbers.\n",
        "\n",
        "### Methods\n",
        "1.  **Bag-of-words**: Counts word occurrences.\n",
        "2.  **TfIdf Vectorization**: Weighs words by importance (Term Frequency - Inverse Document Frequency).\n",
        "\n",
        "### Code Implementation\n",
        "We will create a small dummy text dataset to demonstrate vectorization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: ['an' 'and' 'bad' 'great' 'hated' 'it' 'loved' 'movie' 'not' 'okay'\n",
            " 'terrible' 'the' 'was']\n",
            "\n",
            "Transformed Shape: (3, 13)\n",
            "\n",
            "Array Representation:\n",
            " [[0 1 0 1 0 1 1 1 0 0 0 1 1]\n",
            " [0 1 0 0 1 1 0 1 0 0 1 1 1]\n",
            " [1 0 1 1 0 1 0 1 2 1 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Dummy text data\n",
        "data_text = [\n",
        "    \"The movie was great and I loved it\",\n",
        "    \"The movie was terrible and I hated it\",\n",
        "    \"It was an okay movie, not great not bad\"\n",
        "]\n",
        "\n",
        "# Initialize Vectorizer\n",
        "vect = CountVectorizer()\n",
        "\n",
        "# Fit and Transform\n",
        "# fit: learns the vocabulary\n",
        "# transform: converts text to matrix\n",
        "vect.fit(data_text)\n",
        "X_text = vect.transform(data_text)\n",
        "\n",
        "print(\"Vocabulary:\", vect.get_feature_names_out())\n",
        "print(\"\\nTransformed Shape:\", X_text.shape)\n",
        "print(\"\\nArray Representation:\\n\", X_text.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 15. Arguments of the Vectorizers</span><br>\n",
        "\n",
        "We can tune the vectorizers to improve performance and reduce noise.\n",
        "\n",
        "### Key Arguments\n",
        "*   `stop_words`: Removes non-informative, frequent words (e.g., \"the\", \"is\", \"and\").\n",
        "*   `ngram_range`: Captures phrases instead of just single words (e.g., \"not good\").\n",
        "    *   `(1, 1)`: Unigrams only.\n",
        "    *   `(1, 2)`: Unigrams and Bigrams.\n",
        "*   `max_features`: Limits the vocabulary to the top N most frequent words.\n",
        "*   `max_df` / `min_df`: Ignores terms that appear in too many or too few documents.\n",
        "*   `token_pattern`: Regex to capture specific patterns (e.g., remove digits).\n",
        "\n",
        "**Note**: Lemmas and stems are important NLP concepts but are **NOT** direct arguments to the standard sklearn vectorizers (they usually require a custom tokenizer).\n",
        "\n",
        "### Code Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuned Vocabulary: ['bad' 'great' 'great bad' 'great loved' 'hated' 'loved' 'movie'\n",
            " 'movie great' 'movie terrible' 'okay' 'okay movie' 'terrible'\n",
            " 'terrible hated']\n"
          ]
        }
      ],
      "source": [
        "# Vectorizer with arguments\n",
        "vect_tuned = CountVectorizer(\n",
        "    stop_words='english',  # Remove English stop words\n",
        "    ngram_range=(1, 2),    # Use unigrams and bigrams\n",
        "    max_features=100       # Limit vocabulary size\n",
        ")\n",
        "\n",
        "vect_tuned.fit(data_text)\n",
        "X_tuned = vect_tuned.transform(data_text)\n",
        "\n",
        "print(\"Tuned Vocabulary:\", vect_tuned.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 16. The Automated Sentiment Analysis System</span><br>\n",
        "\n",
        "The complete workflow for an automated sentiment analysis system combines all the steps we have discussed:\n",
        "\n",
        "1.  **Exploration & Feature Engineering**: Understanding the data and creating new features (like review length).\n",
        "2.  **Numeric Transformation**: Converting text to numbers using `CountVectorizer` or `TfidfVectorizer`.\n",
        "3.  **Classification**: Using a supervised learning model (like `LogisticRegression`) to predict the sentiment.\n",
        "\n",
        "This pipeline allows us to take raw text input and output a sentiment prediction (Positive/Negative).\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 17. Conclusion</span><br>\n",
        "\n",
        "In this notebook, we have covered the essential building blocks of **Sentiment Analysis in Python**:\n",
        "\n",
        "1.  **Problem Definition**: We identified sentiment analysis as a classification problem (Binary or Multi-class).\n",
        "2.  **Modeling**: We explored **Logistic Regression** as a probabilistic classifier suitable for this task.\n",
        "3.  **Evaluation**: We learned to measure success using **Accuracy** and the **Confusion Matrix**, and emphasized the importance of the **Train/Test Split**.\n",
        "4.  **Text Processing**: We demonstrated how to convert raw text into numerical features using **Vectorization** (Bag-of-words/TF-IDF) and how to tune these vectorizers.\n",
        "\n",
        "**Next Steps for the Learner:**\n",
        "*   Apply these techniques to a real-world dataset (e.g., the IMDb movie review dataset).\n",
        "*   Experiment with different `C` values in Logistic Regression to observe the effect of regularization.\n",
        "*   Try `TfidfVectorizer` instead of `CountVectorizer` to see if it improves performance.\n",
        "*   Explore more complex models like Naive Bayes or Support Vector Machines (SVM).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
