{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Building a Bag of Words Model</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Feature Engineering for NLP in Python)</span></div>\n",
        "\n",
        "# Table of Contents\n",
        "\n",
        "1. [Recap of Data Format for ML Algorithms](#section-1)\n",
        "2. [The Bag of Words (BoW) Model](#section-2)\n",
        "3. [BoW Example: From Corpus to Vectors](#section-3)\n",
        "4. [Text Preprocessing](#section-4)\n",
        "5. [Implementing BoW using Scikit-Learn](#section-5)\n",
        "6. [Building a BoW Naive Bayes Classifier](#section-6)\n",
        "7. [Text Preprocessing with CountVectorizer Arguments](#section-7)\n",
        "8. [Full Workflow: Building the Spam Filter](#section-8)\n",
        "9. [BoW Shortcomings & Introduction to n-grams](#section-9)\n",
        "10. [Building n-gram Models](#section-10)\n",
        "11. [Shortcomings of n-gram Models](#section-11)\n",
        "12. [Conclusion](#section-12)\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. Recap of Data Format for ML Algorithms</span><br>\n",
        "\n",
        "Before diving into Natural Language Processing (NLP), it is crucial to understand the data requirements for standard Machine Learning (ML) algorithms.\n",
        "\n",
        "For almost any ML algorithm to function correctly, the data must adhere to two specific rules:\n",
        "1.  **Tabular Form**: The data must be structured in rows (observations) and columns (features).\n",
        "2.  **Numerical Features**: The training features must be numbers. Algorithms cannot natively understand raw text strings like \"The lion is the king\".\n",
        "\n",
        "This necessitates **Feature Engineering**: converting raw text into a numerical, tabular representation.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. The Bag of Words (BoW) Model</span><br>\n",
        "\n",
        "The **Bag of Words (BoW)** model is a fundamental technique to convert text into numerical vectors. It simplifies text by disregarding grammar and word order, focusing only on word multiplicity.\n",
        "\n",
        "### Core Steps:\n",
        "1.  **Extract word tokens**: Break down the text into individual words.\n",
        "2.  **Compute frequency**: Count how many times each word appears in a document.\n",
        "3.  **Construct a word vector**: Create a vector for each document based on these frequencies and the entire vocabulary of the corpus.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. BoW Example: From Corpus to Vectors</span><br>\n",
        "\n",
        "Let's look at a concrete example of how a corpus is transformed into vectors.\n",
        "\n",
        "### The Corpus\n",
        "Consider the following three documents (sentences):\n",
        "\n",
        "1.  \"The lion is the king of the jungle\"\n",
        "2.  \"Lions have lifespans of a decade\"\n",
        "3.  \"The lion is an endangered species\"\n",
        "\n",
        "### The Vocabulary\n",
        "First, we extract the unique words (vocabulary) from the entire corpus. Note that in this raw example, case sensitivity matters (\"The\" vs \"the\") and plurals are distinct (\"lion\" vs \"Lions\").\n",
        "\n",
        "**Vocabulary**: `a`, `an`, `decade`, `endangered`, `have`, `is`, `jungle`, `king`, `lifespans`, `lion`, `Lions`, `of`, `species`, `the`, `The`\n",
        "\n",
        "### The Vectors\n",
        "Each sentence is converted into a vector of counts corresponding to the vocabulary order above.\n",
        "\n",
        "| Document | Vector Representation |\n",
        "| :--- | :--- |\n",
        "| \"The lion is the king of the jungle\" | `[0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1]` |\n",
        "| \"Lions have lifespans of a decade\" | `[1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0]` |\n",
        "| \"The lion is an endangered species\" | `[0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1]` |\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> Notice that in the first sentence, the word \"the\" appears twice (once as \"The\" and once as \"the\" in the raw text, but if we count \"the\" specifically in the vocabulary slot, it has a count of 2). In a strict raw implementation without preprocessing, \"The\" and \"the\" might be separate features.</div>\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Text Preprocessing</span><br>\n",
        "\n",
        "To improve the efficiency and accuracy of the BoW model, we perform text preprocessing. This reduces the size of the vocabulary (dimensionality) and groups similar words.\n",
        "\n",
        "### Common Preprocessing Steps:\n",
        "1.  **Lowercasing**: Converting \"Lions\" and \"lion\" $\\rightarrow$ \"lion\", \"The\" and \"the\" $\\rightarrow$ \"the\".\n",
        "2.  **Removing Punctuation**: Stripping symbols like `. , ! ?`.\n",
        "3.  **Removing Stopwords**: Removing common words that carry little meaning (e.g., \"is\", \"of\", \"the\").\n",
        "\n",
        "### Benefits:\n",
        "*   **Smaller Vocabularies**: Fewer columns in our data table.\n",
        "*   **Improved Performance**: Reducing the number of dimensions (features) often helps ML algorithms generalize better.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Implementing BoW using Scikit-Learn</span><br>\n",
        "\n",
        "We can implement the Bag of Words model easily using Python's `pandas` and `scikit-learn`.\n",
        "\n",
        "### Step 1: Define the Corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the corpus as a pandas Series\n",
        "corpus = pd.Series([\n",
        "    'The lion is the king of the jungle',\n",
        "    'Lions have lifespans of a decade',\n",
        "    'The lion is an endangered species'\n",
        "])\n",
        "\n",
        "print(\"Corpus:\")\n",
        "print(corpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Step 2: Generate the BoW Matrix\n",
        "We use `CountVectorizer` from `sklearn.feature_extraction.text`. This class handles tokenization, counting, and basic preprocessing automatically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "# fit_transform learns the vocabulary and transforms the text\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert to array to visualize the matrix\n",
        "print(\"BoW Matrix (Dense Array):\")\n",
        "print(bow_matrix.toarray())\n",
        "\n",
        "# Optional: View the vocabulary mapping\n",
        "print(\"\\nVocabulary Mapping:\")\n",
        "print(vectorizer.vocabulary_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Explanation of Output**:\n",
        "The output array corresponds to the counts of each word in the vocabulary for each document. `CountVectorizer` by default lowercases text, which is why the output might differ slightly from the manual example if case sensitivity was strictly enforced there.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. Building a BoW Naive Bayes Classifier</span><br>\n",
        "\n",
        "One of the most common applications of BoW is **Spam Filtering**. We will build a classifier to detect whether a message is \"spam\" or \"ham\" (legitimate).\n",
        "\n",
        "### The Dataset\n",
        "Imagine we have a dataset with two columns: `message` and `label`.\n",
        "\n",
        "| message | label |\n",
        "| :--- | :--- |\n",
        "| WINNER!! As a valued network customer you have been selected to receive a $900 prize reward! To claim call 09061701461 | spam |\n",
        "| Ah, work. I vaguely remember that. What does it feel like? | ham |\n",
        "\n",
        "### The Workflow\n",
        "1.  **Text Preprocessing**: Clean the data.\n",
        "2.  **Building BoW Model**: Convert text to numerical vectors.\n",
        "3.  **Machine Learning**: Train a classifier (Naive Bayes) on the vectors.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 7. Text Preprocessing with CountVectorizer Arguments</span><br>\n",
        "\n",
        "`CountVectorizer` is powerful because it can handle preprocessing steps via arguments during initialization.\n",
        "\n",
        "### Key Arguments:\n",
        "*   `lowercase`: `True` (default) or `False`. Converts all characters to lowercase.\n",
        "*   `strip_accents`: `'unicode'`, `'ascii'`, or `None`. Removes accents/special characters.\n",
        "*   `stop_words`: `'english'`, a custom `list`, or `None`. Removes common stop words.\n",
        "*   `token_pattern`: regex. Defines what constitutes a \"token\" (word).\n",
        "*   `tokenizer`: function. Allows you to pass a custom tokenizer function.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 8. Full Workflow: Building the Spam Filter</span><br>\n",
        "\n",
        "Let's implement the full pipeline. First, we will create a dummy dataset to simulate the spam data.\n",
        "\n",
        "### Step 1: Setup Data and Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Creating a dummy dataset for demonstration\n",
        "data = {\n",
        "    'message': [\n",
        "        'WINNER!! As a valued network customer you have been selected to receive a $900 prize reward!',\n",
        "        'Ah, work. I vaguely remember that. What does it feel like?',\n",
        "        'Congratulations! You won a free ticket to the Bahamas. Call now!',\n",
        "        'I am going to the grocery store later, do you need anything?',\n",
        "        'URGENT! Your mobile number has been awarded with a Â£2000 prize.',\n",
        "        'Hey, are we still meeting for lunch tomorrow?'\n",
        "    ],\n",
        "    'label': ['spam', 'ham', 'spam', 'ham', 'spam', 'ham']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split into training and test sets\n",
        "# We use 25% of data for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['message'], \n",
        "    df['label'], \n",
        "    test_size=0.25, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Step 2: Build the BoW Model\n",
        "We configure the vectorizer to strip accents, remove English stop words, and keep the original casing (just for demonstration, usually lowercase is better).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create CountVectorizer object with specific preprocessing\n",
        "vectorizer = CountVectorizer(\n",
        "    strip_accents='ascii', \n",
        "    stop_words='english', \n",
        "    lowercase=False\n",
        ")\n",
        "\n",
        "# Generate training BoW vectors\n",
        "# We fit AND transform on training data\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Generate test BoW vectors\n",
        "# We ONLY transform test data (using the vocabulary learned from training)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "print(\"Shape of Training Matrix:\", X_train_bow.shape)\n",
        "print(\"Shape of Test Matrix:\", X_test_bow.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Step 3: Train the Naive Bayes Classifier\n",
        "We use `MultinomialNB`, which is standard for classification with discrete features (like word counts).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Create MultinomialNB object\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# Train clf\n",
        "clf.fit(X_train_bow, y_train)\n",
        "\n",
        "# Compute accuracy on test set\n",
        "accuracy = clf.score(X_test_bow, y_test)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 9. BoW Shortcomings & Introduction to n-grams</span><br>\n",
        "\n",
        "While BoW is effective, it has significant limitations regarding **context**.\n",
        "\n",
        "### The Context Problem\n",
        "Consider these two reviews:\n",
        "1.  \"The movie was good and not boring\" (Positive)\n",
        "2.  \"The movie was not good and boring\" (Negative)\n",
        "\n",
        "**Problem**: Both sentences contain the exact same words. A standard BoW model produces the **exact same vector representation** for both, losing the meaning entirely. The sentiment depends heavily on the position of the word \"not\".\n",
        "\n",
        "### Solution: n-grams\n",
        "An **n-gram** is a contiguous sequence of $n$ elements (words) in a given document.\n",
        "*   **n = 1**: Unigram (Bag of Words)\n",
        "*   **n = 2**: Bigram\n",
        "*   **n = 3**: Trigram\n",
        "\n",
        "**Example**: \"for you a thousand times over\"\n",
        "\n",
        "**n=2 (Bigrams)**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[\n",
        "    'for you',\n",
        "    'you a',\n",
        "    'a thousand',\n",
        "    'thousand times',\n",
        "    'times over'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**n=3 (Trigrams)**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[\n",
        "    'for you a',\n",
        "    'you a thousand',\n",
        "    'a thousand times',\n",
        "    'thousand times over'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By capturing sequences, n-grams retain more context (e.g., \"not good\" vs \"good\").\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 10. Building n-gram Models</span><br>\n",
        "\n",
        "We can build n-gram models using `CountVectorizer` by adjusting the `ngram_range` argument.\n",
        "\n",
        "### Applications\n",
        "*   Sentence completion\n",
        "*   Spelling correction\n",
        "*   Machine translation correction\n",
        "\n",
        "### Implementation\n",
        "The `ngram_range` argument takes a tuple `(min_n, max_n)`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text_data = ['for you a thousand times over']\n",
        "\n",
        "# Generates ONLY bigrams (n=2)\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "bigrams = bigram_vectorizer.fit_transform(text_data)\n",
        "\n",
        "print(\"Bigrams Vocabulary:\")\n",
        "print(bigram_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Generates unigrams, bigrams, AND trigrams (n=1 to n=3)\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
        "ngrams = ngram_vectorizer.fit_transform(text_data)\n",
        "\n",
        "print(\"\\nUnigrams, Bigrams, and Trigrams Vocabulary:\")\n",
        "print(ngram_vectorizer.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 11. Shortcomings of n-gram Models</span><br>\n",
        "\n",
        "While n-grams capture context, they introduce new challenges:\n",
        "\n",
        "1.  **Curse of Dimensionality**: As $n$ increases, the size of the vocabulary explodes. If you have $V$ unique words, you could theoretically have $V^n$ n-grams. This creates massive, sparse matrices that are computationally expensive to process.\n",
        "2.  **Rarity**: Higher-order n-grams (e.g., 5-grams) are very rare in corpora, leading to overfitting or lack of generalizability.\n",
        "3.  **Guidance**: It is generally recommended to keep $n$ small (usually 1, 2, or 3).\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 12. Conclusion</span><br>\n",
        "\n",
        "In this notebook, we explored the foundational techniques of Feature Engineering for NLP:\n",
        "\n",
        "*   **Bag of Words (BoW)**: We learned how to convert raw text into numerical vectors by counting word frequencies.\n",
        "*   **Preprocessing**: We saw how cleaning text (lowercasing, removing stopwords) improves model efficiency.\n",
        "*   **Scikit-Learn Implementation**: We utilized `CountVectorizer` to automate tokenization and vectorization.\n",
        "*   **Classification**: We successfully built a pipeline to classify spam messages using BoW and Naive Bayes.\n",
        "*   **n-grams**: We addressed the context limitations of BoW by introducing n-grams, which capture sequences of words, while noting the trade-off with dimensionality.\n",
        "\n",
        "**Next Steps**:\n",
        "To further improve NLP models, one might explore **TF-IDF** (Term Frequency-Inverse Document Frequency) to weigh the importance of words, or **Word Embeddings** (like Word2Vec or GloVe) for capturing semantic meaning beyond simple counts.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}