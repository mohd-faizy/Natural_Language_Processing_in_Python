{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS3VUyUC6aij"
      },
      "source": [
        "---\n",
        "<h1 align='center'><strong>4️⃣\"Fake News\" Classifier</strong></h1>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoSpaoNj6WeF",
        "outputId": "f7652e2f-8086-43cf-cc2f-9cd2224b904e"
      },
      "source": [
        "!git clone https://github.com/mohd-faizy/Natural_Language_Processing_in_Python.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Natural_Language_Processing_in_Python'...\n",
            "remote: Enumerating objects: 148, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 148 (delta 14), reused 23 (delta 7), pack-reused 116\u001b[K\n",
            "Receiving objects: 100% (148/148), 32.91 MiB | 21.81 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw05dwIb9A9U"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# plt.style.use('fivethirtyeight')\n",
        "# plt.style.use('ggplot')\n",
        "# sns.set_theme()\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LZeI_v3_aDvw"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download a specific NLTK dataset, e.g., the 'punkt' tokenizer models.\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Download the NLTK stopwords dataset, which contains common stopwords for various languages.\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Download the WordNet lexical database, which is used for various NLP tasks like synonym and antonym lookup.\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# Download the NLTK averaged perceptron tagger, which is used for part-of-speech tagging.\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "# Download the NLTK names dataset, which contains a list of common first names and last names.\n",
        "# nltk.download('names', quit=True)\n",
        "\n",
        "# Download the NLTK movie_reviews dataset, which contains movie reviews categorized as positive and negative.\n",
        "# nltk.download('movie_reviews', quit=True)\n",
        "\n",
        "# Download the NLTK reuters dataset, which is a collection of news documents categorized into topics.\n",
        "# nltk.download('reuters', quit=True)\n",
        "\n",
        "# Download the NLTK brown corpus, which is a collection of text from various genres of written American English.\n",
        "# nltk.download('brown', quit=True)\n",
        "\n",
        "# Download the 'maxent_ne_chunker' dataset, which is used for Named Entity Recognition.\n",
        "nltk.download('maxent_ne_chunker', quiet=True)\n",
        "\n",
        "# Download the 'words' dataset, which contains a list of common English words.\n",
        "nltk.download('words', quiet=True)"
      ],
      "metadata": {
        "id": "Wgf8tcBR4ic9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4239f2d5-65c9-465f-f421-2b21df0e24d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_QJ6k0e9bI2",
        "outputId": "cbe71c72-2f8d-4287-9507-5833f99949fc"
      },
      "source": [
        "os.chdir('/content/Natural_Language_Processing_in_Python/01-Introduction-to-Natural-Language-Processing-in-Python/_datasets')\n",
        "cwd = os.getcwd()\n",
        "print('Curent working directory is ', cwd)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Curent working directory is  /content/Natural_Language_Processing_in_Python/01-Introduction-to-Natural-Language-Processing-in-Python/_datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H7VtZ_99qrY",
        "outputId": "0376f29c-80e9-4c70-bdad-2e6b794ea29b"
      },
      "source": [
        "ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " articles.csv            grail.txt            \u001b[0m\u001b[01;34mwikipedia_articles\u001b[0m/\n",
            " english_stopwords.txt   \u001b[01;34mnews_articles\u001b[0m/      'Wikipedia articles.zip'\n",
            " fake_or_real_news.csv  'News articles.zip'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQEPrKg8GDIF"
      },
      "source": [
        "## **Supervised Learning and NLP for Fake News Classification**\n",
        "\n",
        "### Introduction\n",
        "\n",
        "- Utilizing supervised learning with NLP for fake news classification.\n",
        "- Focus on linguistic features over geometric ones, using `Bag-of-Words` or `TF-IDF` models.\n",
        "\n",
        "### **Supervised Learning Steps**\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   - Gather and preprocess news article data.\n",
        "   - Ensure data quality and consistency.\n",
        "\n",
        "2. **Label Assignment**:\n",
        "   - Categorize articles as \"`fake`\" or \"`authentic`\" based on analysis and expertise.\n",
        "\n",
        "3. **Data Splitting**:\n",
        "   - Divide the dataset into `training` and `test` sets.\n",
        "\n",
        "4. **Feature Extraction**:\n",
        "   - Convert text into numerical representations (`BoW` or `TF-IDF`) to aid in prediction.\n",
        "\n",
        "5. **Model Evaluation**:\n",
        "   - Assess model performance using metrics like `accuracy`, `precision`, `recall`, and `F1-score`.\n",
        "\n",
        "By following these steps, we can effectively classify fake news, contributing to misinformation mitigation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llPis5UXGDIF"
      },
      "source": [
        "## **Building word count vectors with scikit-learn**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "0uzBbuBPGDIG",
        "outputId": "52ac1fe3-633a-477b-a3a5-9f80aaa8ad2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6335, 4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                              title  \\\n",
              "0        8476                       You Can Smell Hillary’s Fear   \n",
              "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
              "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
              "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
              "4         875   The Battle of New York: Why This Primary Matters   \n",
              "\n",
              "                                                text label  \n",
              "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
              "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
              "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
              "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
              "4  It's primary day in New York and front-runners...  REAL  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-01446991-7bb2-4270-acf9-d14967af2eed\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8476</td>\n",
              "      <td>You Can Smell Hillary’s Fear</td>\n",
              "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10294</td>\n",
              "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
              "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3608</td>\n",
              "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
              "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10142</td>\n",
              "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
              "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>875</td>\n",
              "      <td>The Battle of New York: Why This Primary Matters</td>\n",
              "      <td>It's primary day in New York and front-runners...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01446991-7bb2-4270-acf9-d14967af2eed')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-01446991-7bb2-4270-acf9-d14967af2eed button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-01446991-7bb2-4270-acf9-d14967af2eed');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cb346ad0-ff46-485d-8f12-76c3be973ca2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cb346ad0-ff46-485d-8f12-76c3be973ca2')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cb346ad0-ff46-485d-8f12-76c3be973ca2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "url = \"/content/Natural_Language_Processing_in_Python/01-Introduction-to-Natural-Language-Processing-in-Python/_datasets/fake_or_real_news.csv\"\n",
        "df = pd.read_csv(url)\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVxtgkc9GDIH",
        "outputId": "e7a0b527-d97f-43f4-99b7-0660cbf831dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n",
            " '000km']\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare the data for text classification\n",
        "X = df['text']\n",
        "y = df.label\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# - test_size is set to 0.33, meaning 33% of the data will be used for testing\n",
        "# - random_state is set to 53 for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.33,\n",
        "                                                    random_state=53)\n",
        "\n",
        "# Create an instance of the CountVectorizer with specified settings\n",
        "# - 'stop_words' is set to 'english' to remove common English stopwords (e.g., \"the\", \"and\")\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Use the CountVectorizer to transform the training data (X_train)\n",
        "# This converts the text data into a 'bag-of-words' representation\n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Use the same CountVectorizer to transform the test data (X_test)\n",
        "# Note that we use 'transform' on the test data to ensure consistent vocabulary\n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "# Print the first 10 feature names (words) in the bag-of-words representation\n",
        "print(count_vectorizer.get_feature_names_out()[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">The reason we use `fit_transform` on `X_train` and transform on `X_test` is to avoid data leakage. Data leakage occurs when information from the test set is used to train the model, which can lead to overfitting and unrealistic performance estimates. To prevent this, we need to ensure that the model only learns from the training set and does not see the test set until the evaluation stage.\n",
        "\n",
        "- **The `fit_transform` method does two things:** it fits the `CountVectorizer` to the training data, which means it **learns the vocabulary** and **the frequency of each word** in the training data, and it transforms the training data into a sparse matrix of word counts.\n",
        "\n",
        "- **The `transform` method only does the second thing:** it transforms the test data into a sparse matrix of word counts using the vocabulary and frequency learned from the training data. This way, the test data is not used to influence the model training, and we can get a fair evaluation of the model’s performance on unseen data."
      ],
      "metadata": {
        "id": "77rpuo0KSnDP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K7JocR9GDII"
      },
      "source": [
        "### **TfidfVectorizer for text classification**\n",
        "\n",
        "Similar to the sparse `CountVectorizer` created in the previous exercise, you'll work on creating tf-idf vectors for your documents. You'll set up a `TfidfVectorizer` and investigate some of its features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "x_3jkDv0GDII",
        "outputId": "0ddd1abb-788a-456e-d13d-299765b60d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n",
            " '000km']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Import the TfidfVectorizer class from the scikit-learn library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create an instance of the TfidfVectorizer with specified settings\n",
        "# - 'stop_words' is set to 'english' to remove common English stopwords (e.g., \"the\", \"and\")\n",
        "# - 'max_df' is set to 0.7 to ignore terms that have a document frequency higher than 70%\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "\n",
        "# Use the TfidfVectorizer to transform the training data (X_train)\n",
        "# This converts the text data into a TF-IDF matrix\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Use the same TfidfVectorizer to transform the test data (X_test)\n",
        "# Note that we use 'transform' on the test data to ensure consistent vocabulary and scaling\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Print the first 10 feature names (terms) in the TF-IDF matrix\n",
        "print(tfidf_vectorizer.get_feature_names_out()[:10])\n",
        "\n",
        "# Print the TF-IDF values of the first 5 training examples as a dense array\n",
        "display(tfidf_train.A[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `CountVectorizer` vs `TfidfVectorizer`\n",
        "\n",
        "- `CountVectorizer` simply counts the occurrences of words.\n",
        "- `TfidfVectorizer` considers both the frequency of a word in a document and its rarity across the entire corpus, giving more weight to words that are important within a document but less common in the corpus. This makes it a better choice when you want to capture the relative importance of words in text data.\n",
        "\n",
        "- **`count_train.A`** represents the output of the `CountVectorizer` for the training data. The `.A` attribute is used to convert the **`Sparse Matrix`** generated by the **CountVectorizer** into a **`Dense Matrix`**. **This dense matrix contains the counts of each term (word) in each document in your training dataset.**\n",
        "\n",
        "- **`tfidf_train.A`**  is used to access the **Dense Matrix** representation of the `TF-IDF` (Term Frequency-Inverse Document Frequency) matrices for the training and test data."
      ],
      "metadata": {
        "id": "zvvEVR-UoE6p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH2adjp_GDII"
      },
      "source": [
        "### **Inspecting the vectors**\n",
        "\n",
        "To get a better idea of how the `vectors` work, we will investigate them by converting them into **pandas DataFrames**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create DataFrames `count_df` and `tfidf_df` using `pd.DataFrame()`.\n",
        "- Specify values as the first argument and columns (features) as the second argument.\n",
        "- Access values using the `.A` attribute of `count_train` and `tfidf_train`.\n",
        "- Access columns using the `.get_feature_names_out()` methods of `count_vectorizer` and `tfidf_vectorizer`.\n",
        "- Print the head of each DataFrame to inspect their structure.\n",
        "- Test if column names are the same by creating an object called `difference`.\n",
        "- Find the difference between columns in `count_df` and `tfidf_df` using `.columns`.\n",
        "- Check if the two DataFrames are equivalent using the `.equals()` method with `count_df` and `tfidf_df` as arguments.\n"
      ],
      "metadata": {
        "id": "boGHD-lQzP3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the CountVectorizer DataFrame: count_df\n",
        "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the head of count_df\n",
        "print(count_df.head())\n",
        "\n",
        "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the head of tfidf_df\n",
        "print(tfidf_df.head())\n",
        "\n",
        "\n",
        "# Calculate the difference in columns: difference\n",
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "print(difference)\n",
        "\n",
        "# Check whether the DataFrame are equal\n",
        "print(count_df.equals(tfidf_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccxAejISv6lM",
        "outputId": "d3bac24e-e5a9-445d-8ecf-82dedbe5ade4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
            "0   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "1   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "2   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "3   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "4   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "\n",
            "   حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  ยงade  \n",
            "0    0     0   0   0   0        0   0    0        0      0  \n",
            "1    0     0   0   0   0        0   0    0        0      0  \n",
            "2    0     0   0   0   0        0   0    0        0      0  \n",
            "3    0     0   0   0   0        0   0    0        0      0  \n",
            "4    0     0   0   0   0        0   0    0        0      0  \n",
            "\n",
            "[5 rows x 56922 columns]\n",
            "    00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
            "0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "\n",
            "   حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  ยงade  \n",
            "0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "\n",
            "[5 rows x 56922 columns]\n",
            "set()\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-lTi5L9GDIJ"
      },
      "source": [
        "## **Training and testing a classification model with scikit-learn**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEWmoM8HGDIJ"
      },
      "source": [
        "### **Training and testing the \"fake news\" model with CountVectorizer**\n",
        "\n",
        "\n",
        "> **Which of the below is the most reasonable model to use when training a new supervised model using text vector data?** $\\Rightarrow$ `Naive Bayes`\n",
        "\n",
        "- `Feature Independence`: Naive Bayes assumes all features are independent, which simplifies calculations but may not hold in reality.\n",
        "- `Handling High Dimensionality`: It efficiently handles high-dimensional text data by treating features independently.\n",
        "- `Probabilistic Model`: Provides intuitive, probabilistic predictions based on - conditional probabilities.\n",
        "- `Multinomial and Bernoulli Variants`: Multinomial for discrete features, - Bernoulli for binary features.\n",
        "- `Scalability`: Highly scalable and works well with large datasets.\n",
        "- `Performance`: Surprisingly accurate, can outperform sophisticated methods.\n",
        "- `Model Selection`: Choose wisely based on data and task requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing a **Naive Bayes** model using the **CountVectorizer** data."
      ],
      "metadata": {
        "id": "nbzVmZCGz7R-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "a6yxeRNRGDIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c22153e3-ab47-4dfd-dc47-97e7021a1b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.893352462936394\n",
            "[[ 865  143]\n",
            " [  80 1003]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "nb_classifier.fit(count_train, y_train)\n",
        "\n",
        "# Create the predicted tags: pred\n",
        "pred = nb_classifier.predict(count_test)\n",
        "\n",
        "# Calculate the accuracy score: score\n",
        "score = accuracy_score(y_test, pred)\n",
        "print(score)\n",
        "\n",
        "# Calculate the confusion matrix: cm\n",
        "cm =confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcbebDqUGDIK"
      },
      "source": [
        "### Training and testing the \"fake news\" model with TfidfVectorizer\n",
        "Now that you have evaluated the model using the `CountVectorizer`, you'll do the same using the `TfidfVectorizer` with a Naive Bayes model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uFEY8qzoGDIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae1cedf-d791-4a35-ec88-577a01f6f820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8565279770444764\n",
            "[[ 739  269]\n",
            " [  31 1052]]\n"
          ]
        }
      ],
      "source": [
        "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "nb_classifier.fit(tfidf_train, y_train)\n",
        "\n",
        "# Create the predicted tags: pred\n",
        "pred = nb_classifier.predict(tfidf_test)\n",
        "\n",
        "# Calculate the accuracy score: score\n",
        "score = accuracy_score(y_test, pred)\n",
        "print(score)\n",
        "\n",
        "# Calculate the confusion matrix: cm\n",
        "cm = confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKcx1NgYGDIK"
      },
      "source": [
        "## Simple NLP, complex problems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8YATSDSGDIK"
      },
      "source": [
        "### Improving your model\n",
        "\n",
        "\n",
        "What are possible next steps you could take to improve the model?\n",
        "\n",
        "- *Tweaking alpha levels.*\n",
        "- *Trying a new classification model.*\n",
        "- *Training on a larger dataset.*\n",
        "- *Improving text preprocessing.*\n",
        "\n",
        "For the larger models:\n",
        "\n",
        "**Tweaking Alpha Levels:**\n",
        "\n",
        "- Alpha levels control the trade-off between model-generated responses and user-provided instructions. Fine-tuning these levels can help make the model's output more aligned with the desired context or style.\n",
        "- Increasing alpha makes the output more focused on the given instruction, while decreasing it makes the output more creative and free-form.\n",
        "- Experimenting with different alpha values can help you strike the right balance for your specific use case.\n",
        "\n",
        "**Trying a New Classification Model:**\n",
        "\n",
        "- GPT-3.5 is a language model, and while it can perform some classification tasks, its primary strength lies in generating text. If your task is primarily classification-oriented, you might consider using a specialized classification model.\n",
        "- Models like BERT, RoBERTa, or more recent transformer-based architectures designed for classification tasks might be more suitable depending on your specific requirements.\n",
        "\n",
        "**Training on a Larger Dataset:**\n",
        "\n",
        "- Training on a larger and more diverse dataset can potentially improve the model's performance, as it can learn from a broader range of language patterns and contexts.\n",
        "- However, increasing the dataset size also comes with computational and infrastructure challenges, including higher costs and longer training times.\n",
        "\n",
        "**Improving Text Preprocessing:**\n",
        "\n",
        "- Proper text preprocessing is crucial for improving the model's understanding and performance. This can include tasks such as tokenization, text cleaning, and data augmentation.\n",
        "- Fine-tuning the preprocessing steps to handle domain-specific language or text types can lead to better model performance.\n",
        "\n",
        "**Additional Steps to Consider:**\n",
        "\n",
        "- **Fine-Tuning:**\n",
        "  - Fine-tuning the model on domain-specific or task-specific data can significantly improve its performance for specific applications.\n",
        "  - It allows you to adapt the model's behavior to better suit your needs.\n",
        "\n",
        "- **Diversity in Data:**\n",
        "  - Ensuring diversity in the training data, including different languages, cultures, and perspectives, can help reduce biases and improve the model's ability to understand and generate content for a global audience.\n",
        "\n",
        "- **Ethical and Safety Measures:**\n",
        "  - Implementing ethical guidelines and safety measures to prevent the model from generating harmful or inappropriate content is essential, especially in applications where user-generated text is involved.\n",
        "\n",
        "- **Feedback Loops:**\n",
        "  - Collecting user feedback and iteratively improving the model based on real-world usage can be invaluable for refining its capabilities.\n",
        "\n",
        "- **Research and Development:**\n",
        "  - Stay updated with the latest research in the field of natural language processing and AI. Incorporate state-of-the-art techniques and insights into your model development process.\n",
        "\n",
        "The choice of which steps to take depends on your specific objectives, available resources, and the nature of your application. Often, a combination of these approaches is necessary to achieve the best results. Regular testing, evaluation, and adaptation based on real-world usage are essential for model improvement.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "B0Yq0OC6GDIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae5adebb-5dfb-4dd5-daab-c1177116ba81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha:  0.0\n",
            "Score:  0.8813964610234337\n",
            "\n",
            "Alpha:  0.1\n",
            "Score:  0.8976566236250598\n",
            "\n",
            "Alpha:  0.2\n",
            "Score:  0.8938307030129125\n",
            "\n",
            "Alpha:  0.30000000000000004\n",
            "Score:  0.8900047824007652\n",
            "\n",
            "Alpha:  0.4\n",
            "Score:  0.8857006217120995\n",
            "\n",
            "Alpha:  0.5\n",
            "Score:  0.8842659014825442\n",
            "\n",
            "Alpha:  0.6000000000000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py:629: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py:635: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score:  0.874701099952176\n",
            "\n",
            "Alpha:  0.7000000000000001\n",
            "Score:  0.8703969392635102\n",
            "\n",
            "Alpha:  0.8\n",
            "Score:  0.8660927785748446\n",
            "\n",
            "Alpha:  0.9\n",
            "Score:  0.8589191774270684\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create the list of alphas: alphas\n",
        "alphas = np.arange(0, 1, 0.1)\n",
        "\n",
        "# Define train_and_predict()\n",
        "def train_and_predict(alpha):\n",
        "    # Instantiate the classifier: nb_classifier\n",
        "    nb_classifier = MultinomialNB(alpha=alpha)\n",
        "\n",
        "    # Fit to the training data\n",
        "    nb_classifier.fit(tfidf_train, y_train)\n",
        "\n",
        "    # Predict the labels: pred\n",
        "    pred = nb_classifier.predict(tfidf_test)\n",
        "\n",
        "    # Compute accuracy: score\n",
        "    score = accuracy_score(y_test, pred)\n",
        "    return score\n",
        "\n",
        "# Iterate over the alphas and print the corresponding score\n",
        "for alpha in alphas:\n",
        "    print('Alpha: ', alpha)\n",
        "    print('Score: ', train_and_predict(alpha))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMV30diEGDIL"
      },
      "source": [
        "### **Inspecting Model**\n",
        "\n",
        "Now that we have built a \"fake news\" classifier, we will investigate what it has learned. we can map the important vector weights back to actual words using some simple inspection techniques."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the class labels: class_labels\n",
        "class_labels = nb_classifier.classes_\n",
        "\n",
        "# Extract the features: feature_names\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Zip the feature names together with the coefficient array\n",
        "# and sort by weights: feat_with_weights\n",
        "feat_with_weights = sorted(zip(nb_classifier.feature_log_prob_[0], feature_names))\n",
        "\n",
        "# Print the first class label and the top 20 feat_with_weights entries\n",
        "print(class_labels[0], feat_with_weights[:20])\n",
        "\n",
        "# Print the second class label and the bottom 20 feat_with_weights entries\n",
        "print(class_labels[1], feat_with_weights[-20:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY3eq5Tw4FJZ",
        "outputId": "5ab7f077-6b15-41ee-cc3c-cc68dd53937c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAKE [(-11.280753302177917, '00000031'), (-11.280753302177917, '00006'), (-11.280753302177917, '000ft'), (-11.280753302177917, '001'), (-11.280753302177917, '002'), (-11.280753302177917, '003'), (-11.280753302177917, '006'), (-11.280753302177917, '008'), (-11.280753302177917, '010'), (-11.280753302177917, '013'), (-11.280753302177917, '025'), (-11.280753302177917, '027'), (-11.280753302177917, '035'), (-11.280753302177917, '037'), (-11.280753302177917, '040'), (-11.280753302177917, '044'), (-11.280753302177917, '048'), (-11.280753302177917, '066'), (-11.280753302177917, '068'), (-11.280753302177917, '075')]\n",
            "REAL [(-8.036772745824807, 'president'), (-8.022187159522364, 'american'), (-8.013319806154513, 'media'), (-8.007761560290644, 'donald'), (-8.006632122322646, 'october'), (-7.989623223030759, 'government'), (-7.929695447721539, 'like'), (-7.922750601304927, 'war'), (-7.915731838943572, 'new'), (-7.908889774759155, 'world'), (-7.885018054191407, 'just'), (-7.758145325115569, 'said'), (-7.7498037548099585, 'russia'), (-7.697669509488481, 'fbi'), (-7.604825769578616, '2016'), (-7.554879292243166, 'election'), (-7.541640806988918, 'people'), (-7.235945549755579, 'hillary'), (-6.923220068888362, 'clinton'), (-6.867377223688766, 'trump')]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}